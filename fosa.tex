% To Do: paragrah widow penalty   
%        
%      Hallo?  
\documentclass[8pt]{extarticle}
\usepackage[english, ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage[a4paper,left=2.3cm,right=1.2cm,top=2cm,bottom=2cm]{geometry} 
\usepackage{blindtext}
\usepackage{setspace}
\usepackage{float}
\usepackage{titletoc}
\usepackage{titlesec}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{amsmath} 
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{verbatimbox}
\usepackage{enumitem}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{bigints}
\onehalfspacing
\usepackage[hidelinks]{hyperref}
\usepackage[all]{nowidow} %funktioniert nicht....
\setlength\parindent{0pt}

%Hier sind die unterschiedlichen Ausführlichkeitsgrade definiert
\includecomment{Extensiv} 
\includecomment{Beweis} 
\includecomment{Annahmen}
\includecomment{Mathspez}
\includecomment{Mathfolg}
\includecomment{Rechreg}
\mdfdefinestyle{MyFrame}{%
    linecolor=black!20!,
    outerlinewidth=0.2pt,
    roundcorner=5pt,
    innertopmargin=0.5\baselineskip,
    innerbottommargin=0.5\baselineskip,
    innerrightmargin=10pt,
    innerleftmargin=10pt,
    backgroundcolor=white}
\specialcomment{Beweis}{\begin{mdframed}[style=MyFrame] Beweis: \ \\}{\end{mdframed}}
\specialcomment{Rechreg}{\noindent \textit{Rechenregeln:} \begin{itemize}[nosep,label=$\star$] }{\end{itemize}}
\renewcommand\ThisComment[1]{% Fix for Umlauts in comments
  \immediate\write\CommentStream{\unexpanded{#1}}%
}

% Hier die Ausführlichkeit bestimmen:
%\excludecomment{Extensiv} 
%\excludecomment{Beweis} 
%\excludecomment{Annahmen}
%\excludecomment{Mathspez}
%\excludecomment{Mathfolg}

% Inhaltsverzeichnis mit zwei Spalten
\usepackage[toc]{multitoc}
\renewcommand*{\multicolumntoc}{2}




%Überschriftengrößen anpassen, so dass Paragraph kleiner ist als Subsubsection
\titleformat{\section}
  {\normalfont\fontsize{16}{15}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\fontsize{14}{15}\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\fontsize{12}{15}\bfseries}{\thesubsubsection}{1em}{}


\begin{document}
\title{\Huge Statistik Formelsammlung}
\author{\LARGE Katharina Ring}
\date{\LARGE \today}
\Huge \maketitle \normalsize

\clearpage

\tableofcontents

\clearpage


% weitere Anpassungen im Hauptteil des Dokuments
\raggedright %linksbündig
\setlength{\parindent}{15pt} %Einzuglänge festsetzen
\setlength{\columnseprule}{0.3pt} %Liniendicke zwischen zwei Multicols





%-------------------------------------------------------------------------------

% SECTION: DESKRIPTIVE STATISTIK

%-------------------------------------------------------------------------------

\section{Deskriptive Statistik}


\subsection{Kenngrößen (Parameter): Stichprobe}

\begin{multicols}{2}[\subsubsection{Lagemaße}][4cm] 

\paragraph{Modus}

 Häufigster Wert von $x_i$. Auch zwei oder mehr Modi sind möglich (bimodal).

\paragraph{Median}

$$\tilde{x}_{0.5}=\begin{cases} x_{((n+1)/2)} & \text{falls n ungerade} \\ \frac{1}{2}(x_{(n/2)}+x_{(n/2+1)} & \text{falls n gerade} \end{cases}$$

\paragraph{Quantile}

$$\tilde{x}_\alpha=\begin{cases} x_{(k)} & \text{falls n}\alpha \notin \mathbb{N}\\ \frac{1}{2}(x_{(n\alpha)}+ x_{(n\alpha+1)}) & \text{falls n}\alpha \text{ ganzzahlig} \end{cases}$$

mit

\begin{tabular}{l l}
 k=min x $\in \mathbb{N}$, &  x $>$ n$\alpha$ \\
\end{tabular}

\paragraph{Minimum/Maximum}


$$x_{\min}=\min_{i \in \{ 1,...,N\}} (x_i) \hspace{0.8cm}   x_{\max}=\max_{i \in \{ 1,...,N\}} (x_i)$$
 


\paragraph{Arithmetisches Mittel}

 $$\bar{x}=\frac{1}{n}\sum\limits_{i=1}^n x_i$$

\noindent Schätzer für den Erwartungswert 
$\mu = E[X]$ (erstes~Verteilungsmoment)

\begin{Rechreg}
\item $E(a+b\cdot X)=a+b\cdot E(X)$
\item $E(X\pm Y)=E(X)\pm E(Y)$
\end{Rechreg}

\paragraph{Geometrisches Mittel}

$$\bar{x}_G=\sqrt[n]{\sum\limits_{i=1}^n x_i} $$

\noindent Für Wachstumsfaktoren: $\bar{x}_G=\sqrt[n]{\frac{B_n}{B_0}}$

\paragraph{Harmonisches Mittel}

$$\bar{x}_H=\frac{\sum\limits_{i=1}^n w_i}{\sum\limits_{i=1}^n \frac{w_i}{x_i}}$$


\end{multicols}


\begin{multicols}{2}[\subsubsection{Streuungsmaße}][4cm] 

\paragraph{Spannweite}

$$R=x_{(n)}-x_{(1)}$$

\paragraph{Quartilsabstand}

$$d_Q=\tilde{x}_{0.75}-\tilde{x}_{0.25}$$

\paragraph{(Empirische) Varianz}

$$s^2=\frac{1}{n}\sum\limits_{i=1}^n(x_i-\bar{x})^2=\frac{1}{n}\sum\limits_{i=1}^nx_i^2-\bar{x}^2$$

\noindent Schätzer für das zweite zentrierte Moment, inkl. Varianzverschiebungssatz

\begin{Rechreg}
\item $Var(aX+b)=a^2\cdot Var(X)$
\item $Var(X\pm Y)= Var(X)+Var(Y) + 2Cov(X,Y)$
\end{Rechreg}

\paragraph{(Empirische) Standardabweichung}

$$s=\sqrt{s^2}$$

\paragraph{Variationskoeffizient}

$$ \nu=\frac{s}{\bar{x}}$$

\paragraph{Mittlere absolute Abweichung}


$$ \mathit{e} = \frac{1}{n}\sum_{i=1}^n \left|x_i - \bar{x}\right|$$

\noindent Schätzer für das erste absolute zentrierte Moment 

\end{multicols}



\begin{multicols}{2}[\subsubsection{Konzentrationsmaße}][4cm] 

\paragraph{Gini-Koeffizient}

\begin{equation*} 
\begin{split}
G & = \frac{2\sum\limits_{i=1}^n ix_{(i)}-(n+1)\sum\limits_{i=1}^n x_{(i)}}{n\sum\limits_{i=1}^n x_{(i)}}  = 1-\frac{1}{n}\sum\limits_{i=1}^n(v_{i-1}+v_i)
\end{split}
\end{equation*}

mit

$$  u_i=\frac{i}{n}, \hspace{0.3cm} v_i= \frac{\sum\limits_{j=1}^i x_{(j)}}{\sum\limits_{j=1}^i x_{(j)}} \hspace{0.7cm} (u_0=0, \hspace{0.2cm} v_0=0 )$$


\noindent Dies sind auch die Werte für die Lorenzkurve.

\ \\

\indent Wertebereich: $ 0 \le G \le \frac{n-1}{n}$




\paragraph{Lorenz-Münzner-Koeffizient ($G$ normiert)}

$$G^+=\frac{n}{n-1}G$$

\indent Wertebereich: $ 0 \le G^+ \le 1$






\end{multicols}




\begin{multicols}{2}[\subsubsection{Gestaltmaße}][4cm] 

\paragraph{(Empirische) Schiefe}
$$\nu = \frac{n}{(n-1)(n-2)} \sum_{i=1}^n \left(\frac{x_i-\bar{x}}{s}\right)^3$$

\noindent Schätzer für das dritte zentrierte Moment, normiert durch $(\sigma^2)^{\frac{2}{3}}$

\paragraph{(Empirische) Wölbung/Kurtosis}

$$k=\left[n(n+1) \cdot \sum_{i=1}^n \left(\frac{x_i-\bar{x}}{s}\right)^4 - 3(n-1)\right] \cdot \frac{n-1}{(n-2)(n-3)}+3$$

\noindent Schätzer für das vierte zentrierte Moment, normiert durch $(\sigma^2)^2$

\paragraph{Exzess}

$$\gamma=k-3$$

\end{multicols}



\begin{multicols}{2}[\subsubsection{Zusammenhangsmaße}][4cm]

\subsubsection*{\textit{Für zwei nominale Variablen}}

\paragraph{$\chi^2$-Statistik}

\begin{equation*}
\begin{split}
\chi^2 & =\sum\limits_{i=1}^k \sum\limits_{j=1}^l \frac{(n_{ij}-\frac{n_{i+}n_{+j}}{n})^2}{\frac{n_{i+}n_{+j}}{n}}  =n\left(\sum\limits_{i=1}^k \sum\limits_{j=1}^l \frac{n_{ij}^2}{n_{i+}n_{+j}}-1\right)
\end{split}
\end{equation*}

Wertebereich: $ 0 \le \chi^2 \le n(\min(k,l)-1)$

\paragraph{Phi-Koeffizient}

$$\Phi=\sqrt{\frac{\chi^2}{n}}$$

Wertebereich: $ 0 \le \Phi \le \sqrt{\min(k,l)-1}$

\paragraph{Cram\'ers $V$}

$$ V= \sqrt{\frac{\chi^2}{\min(k,l)-1}}$$

Wertebereich: $ 0 \le V \le 1$

\paragraph{Kontingenzkoeffizient $C$}

$$C=\sqrt{\frac{\chi^2}{\chi^2 + n}}$$

Wertebereich: $ 0 \le C \le \sqrt{\frac{\min(k,l)-1}{\min(k,l)}} $

\paragraph{Korrigierter Kontingenzkoeffizient $C_{korr}$}

$$C_{korr}= \sqrt{\frac{\min(k,l)}{\min(k,l)-1}} \cdot \sqrt{\frac{\chi^2}{\chi^2 + n}} $$

Wertebereich: $ 0 \le C_{korr} \le 1 $

\paragraph{Odds-Ratio}

$$OR=\frac{ad}{bc} = \frac{n_{ii}n_{jj}}{n_{ij}n_{ji}}$$

Wertebereich: $0 \le OR < \infty$

\subsubsection*{\textit{Für zwei ordinale Variablen}}

\paragraph{Gamma nach Goodman und Kruskal}

$$\gamma=\frac{K-D}{K+D}$$


\begin{tabular}{l l }
$ K=\sum_{i<m}\sum_{j<n}n_{ij}n_{mn}$ & Anzahl konkordanter Paare \\
$ D=\sum_{i<m}\sum_{j>n}n_{ij}n_{mn}$ & Anzahl diskordanter Paare \\
\end{tabular}

\ \\

Wertebereich: $-1 \le \gamma \le 1$

\paragraph{Kendalls $\tau_b$}

$$ \tau_b=\frac{K-D}{\sqrt{(K+D+T_X)(K+D+T_Y)}}$$

mit

\begin{tabular}{l l } 
$ T_X=\sum_{i=m}\sum_{j<n}n_{ij}n_{mn}$ & Anzahl Bindungen bzgl. $X$ \\
$ T_Y=\sum_{i<m}\sum_{j=n}n_{ij}n_{mn}$ & Anzahl Bindungen bzgl. $Y$ \\
\end{tabular}

\ \\

Wertebereich: $-1 \le \tau_b \le 1$

\paragraph{Kendalls/Stuarts $\tau_c$}

$$\tau_c=\frac{2\min(k,l)(K-D)}{n^2(\min(k,l)-1)}$$

Wertebereich: $-1 \le \tau_c \le 1$

\paragraph{Spearmans Rangkorrelationskoeffizient}

$$\rho=\frac{n(n^2-1)-\frac{1}{2}\sum\limits_{j=1}^J b_j(b_j^2-1)-\frac{1}{2}\sum\limits_{k=1}^K c_k(c_k^2-1)-6\sum\limits_{i=1}^n d_i^2}{\sqrt{n(n^2-1)-\sum\limits_{j=1}^J b_j(b_j^2-1)}\sqrt{n(n^2-1)-\sum\limits_{k=1}^Kc_k(c_k^2-1)}}$$

oder

$$\rho=\frac{s_{rg_xrg_y}}{\sqrt{s_{rg_xrg_x}s_{rg_yrg_y}}}$$

 Entspricht ohne Bindungen:

$$\rho=1-\frac{6\sum\limits_{i=1}^nd_i^2}{n(n^2-1)}$$

mit

\begin{tabular}{l l } 
 $d_i=R(x_i)-R(y_i)$ & Rangdifferenz \\ 
\end{tabular}

\ \\

Wertebereich: $-1 \le \rho \le 1$

\subsubsection*{\textit{Für zwei metrische Variablen}}

\paragraph{Korrelationskoeffizient nach Bravais-Pearson}

$$r=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}=\frac{s_{xy}}{\sqrt{s_{xx}s_{yy}}}$$

mit

\begin{tabular}{l l } 
$S_{xy}=\sum\limits_{i=1}^n(x_i-\bar{x})^2(y_i-\bar{y})^2$ & bzw. $s_{xy}=\frac{S_{xy}}{n}$ \\
$S_{xx}=\sum\limits_{i=1}^n(x_i-\bar{x})^2$ & bzw. $s_{xx}=\frac{S_{xx}}{n}$ \\ 
$S_{yy}=\sum\limits_{i=1}^n(y_i-\bar{y})^2$ & bzw. $s_{yy}=\frac{S_{yy}}{n}$ \\
\end{tabular}

\ \\

Wertebereich: $-1 \le r \le 1$


%\subsubsection*{\textit{Für zwei unterschiedliche Variablen}}


\end{multicols}

\subsection{Tabellen}

\subsection{Diagramme}

\begin{multicols}{2}[\subsubsection{Histogramm}][4cm]



\begin{tikzpicture}

\draw[-latex] (0.5,0) -- (0.5,3) node[below left]{Häufigkeit};
\draw[-latex] (0.5,0) -- (5,0) node[below]{Variable};

\draw[fill=black!50!] (1,0) rectangle (2,1.5);
\draw[fill=black!50!] (2,0) rectangle (3,2.5);
\draw[fill=black!50!] (3,0) rectangle (4,1);

\draw[dashed] (1,1.5) -- (0.4,1.5) node[left]{$v_0$};
\draw[dashed] (1,0) -- (1,-0.5) node[below]{$t_0$};
\draw[dashed] (2,0) -- (2,-0.5) node[below]{$t_1$};
\draw[dashed] (4,0) -- (4,-0.5) node[below]{$t_m$};
\draw[<->] (1,-0.2) -- (2,-0.2) node[midway, below]{$h$};

\end{tikzpicture}

\begin{minipage}{\columnwidth}
Stichprobe: $X=\{ x_1,x_2,...;x_n\}$ \\
$k$-te Klasse: $ B_k=\left[t_k,t_{k+1}\right), k=\{0,1,...,m-1\} $ \\
Anzahl Beobachtungen in der $k$-ten Klasse: $v_k$ \\
Klassenbreite: $h=t_{k+1}-t_k, \forall k$ \\
\end{minipage}

\paragraph{Scotts Regel}

$$h^* \approx 3.5\sigma n^{-\frac{1}{3}}$$

\noindent Für annähernd normalverteilte Daten (min MSE)


\end{multicols}

\subsubsection{QQ-Plot}

\subsubsection{Plot der Realisationen}

\subsubsection{Scatterplot}

%-------------------------------------------------------------------------------

% SECTION: WAHRSCHEINLICHKEITSRECHNUNG

%-------------------------------------------------------------------------------

\section{Wahrscheinlichkeit}

\subsection{Kombinatorik}

% first column
\begin{minipage}[t]{0.7\textwidth}
\addvbuffer[12pt 8pt]{\begin{tabular}{l l || c | c}
& & ohne Wiederholung & mit Wiederholung \\
\midrule
Permutationen & & $n!$ & $\frac{n!}{n_1!\cdot\cdot\cdot n_s!}$ \\
\midrule
Kombinationen: & ohne Reihenfolge & $\binom{n}{m}$ & $\binom{n+m-1}{m}$ \\
& mit Reihenfolge & $\binom{n}{m}m!$ & $n^m$ \\
\end{tabular}}
\end{minipage}
%second column
\begin{Mathspez}
\begin{minipage}[b]{0.2\textwidth}
Dabei gilt: 


 $n!=n\cdot (n-1)\cdot ... \cdot 1$

 $\binom{n}{m} = \frac{n!}{m!(n-m)!}$

\end{minipage}
\end{Mathspez}


\begin{multicols}{2}[\subsection{Wahrscheinlichkeitsrechnung}][4cm]


\paragraph{Laplace-Wahrscheinlichkeit}

$$P(A)=\frac{|A|}{|\Omega|}$$


\paragraph{Axiome von Kolmogorov} \ \\

\noindent mathematische Definition von Wahrscheinlichkeit

\addvbuffer[12pt 8pt]{\begin{tabular}{c l}

(1) & $0 \le P(A) \le 1 \hspace{0.5 cm} \forall A \in \mathcal{A}$ \\

(2) & $P(\Omega) = 1$ \\

(3) & $P(\bigcup_{i=1}^{\infty}{A_i}) = \sum_{i=1}^{\infty}{P(A_i)}$ \\ 
    & $\forall A_i \in \mathcal{A}, i=1,...,\infty \text{ mit } A_i \cap A_j = \emptyset \text{ f"ur } i \neq j$ \\

\end{tabular}}


\begin{Mathfolg}

Folgerungen:
\begin{itemize}
\item $P(\bar{A})=1-P(A)$
\item $P(\emptyset)=0$
\item $P(A\cup B)=P(A)+P(B)-P(A\cap B)$
\item $A \subseteq B \Rightarrow P(A) \le P(B)$
\item $P(B)=\sum\limits_{i=1}^n P(B\cap A_i), \textnormal{ f"ur } A_i,...,A_n$  vollst\"andige Zerlegung von  $\Omega$  in paarweise disjunkte Ereignisse
\end{itemize}

\end{Mathfolg}

\paragraph{Mises' Wahrscheinlichtkeitsbegriff} \ \\

\noindent frequentistische Definition von Wahrscheinlichkeit

$$P(A) = \lim\limits_{n \to \infty}\frac{n_A(n))}{n}$$

mit $n$ Anzahl der Wiederholungen eines Zufallsexperiments und $n_A(n)$ Anzahl an Ereignissen $A$

\paragraph{Bedingte Wahrscheinlichkeit}

$$P(A|B)=\frac{P(A \cap B)}{P(B)} \hspace{0.5cm} \text{für } P(B) > 0$$

\paragraph{Multiplikationssatz}

$$P(A \cap B)=P(B|A)P(A)=P(A|B)P(B)$$

\paragraph{Satz von der totalen Wahrscheinlichkeit}

$$P(B)=\sum\limits_{i=1}^nP(B|A_i)P(A_i)$$

\paragraph{Satz von Bayes}

$$P(A|B)=\frac{P(B|A)P(A)}{P(B)} \hspace{0.5cm} \text{für } P(A), P(B) > 0$$

\paragraph{Stochastische Unabhängigkeit}

\begin{alignat*}{2}
 \text{A, B unabhängig}  \Leftrightarrow  && P(A\cap B) &= P(A)+P(B) \\
 \text{X, Y unabhängig}  \Leftrightarrow && f_{XY}(x,y) &= f_X(x)\cdot f_Y(y) \hspace{0.5cm} \forall x,y
\end{alignat*}

\end{multicols}


\begin{multicols}{2}[\subsection{Zufallsvariablen}][4cm]

\paragraph{Definition}

$$Y: \Omega \to \mathbb{R}$$

\noindent Die Untermenge möglicher Werte von $\mathbb{R}$ heißt Träger

\noindent Notation: Realisationen von $Y$ werden als Kleinbuchstaben dargestellt. $Y=y$ bedeutet, dass $Y$ die Realisation $y$ angenommen hat.

\paragraph{Stetige und diskrete Zufallsvariablen} \ \\

\noindent Ist der Träger überabzählbar unendlich, so heißt die Zufallsvariable \textit{stetig}, sonst heißt sie \textit{diskret}.

\begin{itemize}
\item \textbf{Dichte \boldmath$f(\cdot)$:} 

Für stetige Variablen:
$P(Y \in \left[a, b\right]) = \int_{a}^{b} f_Y(y) dy$

Für diskrete Variablen lässt sich die Dichte (und andere Funktionen) wie die gleichen Funktionen für den stetigen Fall aufschreiben, wenn man 
$\int_{-\infty}^{y} f_Y(\tilde{y})d\tilde{y} := \sum_{k:k \leq y} P(Y=k)$ definiert. Diese Notation wird hier verwendet.

\item \textbf{Verteilungsfunktion \boldmath$F(\cdot)$:} 
$F_Y(y) =P(Y\leq y)$
\end{itemize}

Zusammenhang:

$$F_Y(y) = \int_{-\infty}^{y} f_Y(\tilde{y})d\tilde{y}$$

\noindent Ist der Träger endlich oder abzählbar unendlich, so heißt die Zufallsvariable \textit{diskret}.

\paragraph{Momente}

\begin{itemize}
\item \textbf{Erwartungswert (1.\ Moment)}: $\mu = E(Y) = \int y f_Y(y)dy$
\item \textbf{Varianz (2.\ zentriertes Moment)}: $\sigma^2 = Var(Y) = E(\{Y-E(Y)\}^2) = \int (y - E(Y))^2 f(y) dy$ \\
Varianzverschiebungssatz: $E(\{Y-\mu\}^2) = E(Y^2) - \mu^2$
\begin{Beweis}
$E(\{Y-\mu\}^2) = E(Y^2 - 2Y\mu + \mu^2) = E(Y^2) - 2\mu^2 + \mu^2 = E(Y^2) - \mu^2$
\end{Beweis}
\item \textbf{k.\ Moment}: $E(Y^k) = \int y^k f_Y(y) dy$,\\ \textbf{k.\ zentrales Moment}: $E(\{Y-E(Y)\}^k)$
\end{itemize}

\paragraph{Momenterzeugende Funktion}

$$M_Y(t) = \mathrm{E}(e^{tY})$$

mit $\frac{\partial^kM_Y(t)}{\partial t^k} \bigg|_{t = 0} = \mathrm{E}(Y^k)$ 

kumulanterzeugende Funktion $K_Y(t) = \log M_Y(t)$

Eine Zufallsvariable ist durch ihre momenterzeugende Funktion eindeutig definiert und andersherum (solange die Momente und Kumulanten endlich sind).
  
\end{multicols}

\begin{multicols}{2}[\subsection{Zufallsvektoren}][4cm]

\paragraph{Dichte und Verteilungsfunktion}

$$F(y_1, ..., y_q) = P(Y_1 \leq y_1, ..., Y_q \leq y_q)$$
\begin{align*}
& P(a_1 \leq Y_1 \leq b_1, ..., a_q \leq Y_q \leq b_q) \\
& = \int_{a_1}^{b_1} ...\int_{a_q}^{b_q} f(y_1, .., y_q)dy_1...dy_q
\end{align*}

\paragraph{Marginale Dichte}

$$f_{Y_1}(y_1) = \int_{-\infty}^{\infty}...\int_{-\infty}^{\infty} f(y_1,...,y_k)dy_2...dy_k$$

\paragraph{Bedingte Dichte}

$$f_{Y_1|Y_2}(y_1|y_2) = \frac{f(y_1, ..., y_2)}{f(y_2)} \text{ f"ur } f(y_2) > 0$$

\paragraph{Iterierter Erwartungswert}

$$\mathrm{E}(Y)=\mathrm{E}_X(\mathrm{E}(Y|X))$$
\begin{Beweis}
$$\mathrm{E}(Y) = \int yf(y)dy = \int\int y f(y|x)dy f_X(x)dx = \mathrm{E}_X(\mathrm{E}(Y|X))$$
\end{Beweis}
$$\mathrm{Var}(Y) = \mathrm{E}_X(\mathrm{Var}(Y|X)) + \mathrm{Var}_X(\mathrm{E}(Y|X))$$
\begin{Beweis}
\begin{align*}
\mathrm{Var}(Y) =&  \int (y- \mu_Y)^2 f(y)dy\\
=& \int (y- \mu_Y)^2 f(y|x)f(x)dydx\\
=& \int (y- \mu_{Y|x} + \mu_{Y|x} - \mu_Y)^2 f(y|x)f(x)dydx\\
=& \int (y- \mu_{Y|x})^2 f(y|x)f(x)dydx + \\
 & \int (\mu_{Y|x} - \mu_Y)^2 f(y|x)f(x)dydx + \\
 & 2 \int (y- \mu_{Y|x})(\mu_{Y|x} - \mu_Y) f(y|x)f(x)dydx \\
=& \int \mathrm{Var}(Y|x)f(x)dx + \int (\mu_{Y|x} - \mu_Y)^2f(x)dx\\
=& \mathrm{E}_X(\mathrm{Var}(Y|X)) + \mathrm{Var}_X(\mathrm{E}(Y|X))
\end{align*}
\end{Beweis}

\end{multicols}

\subsection{Verteilungen}

\begin{multicols}{2}[\subsubsection{Diskrete Verteilungen}][4cm]

  \paragraph{Diskrete Gleichverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{U}(\{y_1, ..., y_k\}),\: y \in \{y_1, ..., y_k\} \\
    & P(Y=y_i) =\frac{1}{k},\: i = 1, ...,k \\
    & \mathrm{E}(Y) = \frac{k+1}{2} ,\: \mathrm{Var}(Y) = \frac{k^2 - 1}{12}
  \end{align*}
  
    \paragraph{Binomialverteilung}
  Erfolge in unabhängigen Versuchen 

  \begin{align*}
    & Y \sim \mathrm{Bin}(n, \pi) \text{ mit } n \in \mathbb{N}, \pi \in \left[0,1\right] ,\: y \in \{0, ..., n\} \\
    & P(Y=y|\lambda) = \binom{n}{y}\pi^k(1-\pi)^{n-y} \\
    & \mathrm{E}(Y|\pi,n) = n\pi ,\: \mathrm{Var}(Y|\pi,n) = n\pi(1-\pi)
  \end{align*}

  \paragraph{Poissonverteilung}
  Zählmodelle für seltene Ereignisse

\noindent Immer nur ein Ereignis pro Zeitpunkt, Eintreten der Ereignisse ist unabhängig von bisheriger Geschichte, mittlere Anzahl der Ereignisse pro Zeit ist konstant und proportional zur Länge des betrachteten Zeitintervalls.  

  \begin{align*}
    & Y \sim \mathrm{Po}(\lambda) \text{ mit } \lambda \in \left[ 0, + \infty \right] ,\: y \in \mathbb{N}_0 \\
    & P(Y=y|\lambda) =\frac{\lambda^y exp^{-\lambda}}{y!} \\
    & \mathrm{E}(Y|p) = \lambda ,\: \mathrm{Var}(Y|p) = \lambda
  \end{align*}

\noindent Häufig wird die Varianz durchdas Poisson-Modell unterschätzt, es liegt Überdispersion vor.
  
\noindent  \textit{Approximation} der Binomialverteilung für kleine p

  
    \paragraph{Geometrische Verteilung}

  \begin{align*}
    & Y \sim \mathrm{Geom}(\pi) \text{ mit } \pi \in \left[0,1\right] ,\: y \in \mathbb{N}_0 \\
    & P(Y=y|\pi) = \pi(1-\pi)^{y-1} \\
    & \mathrm{E}(Y|\pi) = \frac{1}{\pi} ,\: \mathrm{Var}(Y|\pi) = \frac{1-\pi}{\pi^2}
  \end{align*}
  
    \paragraph{Negative Binomialverteilung}

  \begin{align*}
    & Y \sim \mathrm{NegBin}(\alpha, \beta) \text{ mit } \alpha, \beta \geq 0 ,\: y \in \mathbb{N}_0 \\
    & P(Y=y|\alpha, \beta) = \binom{\alpha + y - 1}{\alpha - 1} \left(\frac{\beta}{\beta - 1}\right)^{\alpha} \left(\frac{1}{\beta + 1}\right)^y \\
    & \mathrm{E}(Y|\alpha,\beta) = \frac{\alpha}{\beta} ,\: \mathrm{Var}(Y|\alpha,\beta) = \frac{\alpha}{\beta^2}(\beta+1)
  \end{align*}

\end{multicols}

\begin{multicols}{2}[\subsubsection{Stetige Verteilungen}][4cm]

	\paragraph{Stetige Gleichverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{U}(a,b) \text{ mit } \alpha, \beta \in \mathbb{R}, a \le b,\: y \in \left[a,b\right] \\
    & p(y|a,b) =\frac{1}{b-a} \\
    & \mathrm{E}(Y|a,b) = \frac{a+b}{2} ,\: \mathrm{Var}(Y|a,b) = \frac{(b-a)^2}{12}
  \end{align*}
  
    \paragraph{Univariate Normalverteilung} symmetrisch mit $\mu$ und $\sigma^2$
  
    \begin{align*}
    & Y \sim \mathrm{N}(\mu, \sigma^2) \text{ mit } \mu \in \mathbb{R}, \sigma^2 > 0,\: y \in \mathbb{R} \\
    & p(y|\mu, \sigma^2) =\frac{1}{\sqrt{2\pi\sigma^2}} \exp \left(-\frac{(y-\mu)^2}{2 \sigma^2} \right) \\
    & \mathrm{E}(Y|\mu, \sigma^2) = \mu ,\: \mathrm{Var}(Y|\mu, \sigma^2) = \sigma^2
  \end{align*}
  
    \paragraph{Multivariate Normalverteilung} symmetrisch mit $\mu$ und $\Sigma$
  
    \begin{align*}
    & Y \sim \mathrm{N}(\mu, \Sigma) \text{ mit } \mu \in \mathbb{R}^d, \Sigma \in \mathbb{R}^{d\times d} s.p.d.,\: y \in \mathbb{R}^d \\
    & p(y|\mu, \Sigma) = (2\pi)^{-\frac{d}{2}} \det (\Sigma)^{-\frac{1}{2}} \exp \left( -\frac{1}{2}(y-\mu)^{T} \Sigma^{-1}(y-\mu)\right) \\
    & \mathrm{E}(Y|\mu, \Sigma) = \mu ,\: \mathrm{Var}(Y|\mu, \Sigma) = \Sigma
  \end{align*}
  
    \paragraph{Log-Normalverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{LogN}(\mu, \sigma^2) \text{ mit } \mu \in \mathbb{R}, \sigma^2 > 0,\: y > 0 \\
    & p(y|\mu, \sigma^2) =\frac{1}{\sqrt{2\pi\sigma^2 y}} \exp \left(-\frac{(\log y-\mu)^2}{2 \sigma^2} \right) \\
    & \mathrm{E}(Y|\mu, \sigma^2) = \exp (\mu + \frac{\sigma^2}{2}) ,\\
    & \mathrm{Var}(Y|\mu, \sigma^2) = \exp (2\mu + \sigma^2)(\exp (\sigma^2) - 1)
  \end{align*}
  
\noindent Zusammenhang: $\log (Y) \sim \mathrm{N}(\mu, \sigma^2) \Rightarrow Y \sim \mathrm{LogN}(\mu, \sigma^2)$
  
    \paragraph{Nichtzentrale Studentverteilung} statistische Tests für $\mu$ mit unbekannter (geschätzter) Varianz und $\nu$ Freiheitsgraden
  
    \begin{align*}
    & Y \sim \mathrm{t}_\nu(\mu, \sigma) \text{ mit } \mu \in \mathbb{R}, \sigma^2, \nu > 0,\: y \in \mathbb{R}\\
    & p(y|\mu, \sigma^2, \nu) =\frac{\Gamma \left( \frac{\nu + 1}{2}\right) }{\Gamma (\frac{\nu}{2}) \Gamma (\sqrt{\nu\pi}\sigma)} \left(1+ \frac{(y-\mu)^2}{\nu \sigma^2} \right)^{-\frac{\nu + 1}{2}} \\
    & \mathrm{E}(Y|\mu, \sigma^2, \nu) = \mu \text{ f"ur }  \nu > 1,\\
    & \mathrm{Var}(Y|\mu, \sigma^2, \nu) = \sigma^2 \frac{\nu}{\nu-2} \text{ f"ur }  \nu > 2
  \end{align*}
  
 \noindent Zusammenhang: $Y|\theta \sim \mathrm{N}(\mu, \frac{\sigma^2}{\theta}), \: \theta \sim  \mathrm{Ga}(\frac{\nu}{2}, \frac{\nu}{2}) \Rightarrow Y \sim \mathrm{t}_\nu(\mu, \sigma)$ 
  
	\paragraph{Betaverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{Be}(a, b) \text{ mit } a,b > 0,\: y \in \left[0,1\right]\\
    & p(y|a, b) =\frac{\Gamma \left( a+b\right) }{\Gamma (a) \Gamma (b)} y^{a-1} (1-y)^{b-1} \\
    & \mathrm{E}(Y|a, b) = \frac{a}{a+b},\\
    & \mathrm{Var}(Y|a, b) = \frac{ab}{\left(a+b\right)^2(a+b+1)}, \\
    & \mathrm{mod}(Y|a, b) = \frac{a-1}{a+b-2} \text{ f"ur } a,b > 1
  \end{align*}
  
  
  	\paragraph{Gammaverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{Ga}(a, b) \text{ mit } a,b > 0,\: y > 0\\
    & p(y|a, b) = \frac{ b^a }{\Gamma (a)} y^{a-1} \exp (-by) \\
    & \mathrm{E}(Y|a, b) = \frac{a}{b},\\
    & \mathrm{Var}(Y|a, b) = \frac{a}{b^a}, \\
    & \mathrm{mod}(Y|a, b) = \frac{a-1}{b} \text{ f"ur } a \ge 1
  \end{align*}

  	\paragraph{Invers-Gammaverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{IG}(a, b) \text{ mit } a,b > 0,\: y > 0\\
    & p(y|a, b) = \frac{ b^a }{\Gamma (a)} y^{-a-1} \exp (-\frac{b}{y}) \\
    & \mathrm{E}(Y|a, b) = \frac{b}{a-1} \text{ f"ur } a > 1,\\
    & \mathrm{Var}(Y|a, b) = \frac{b^2}{(a-1)^2(a-2)} \text{ f"ur } a \ge 2, \\
    & \mathrm{mod}(Y|a, b) = \frac{b}{a+1}
  \end{align*}
  
\noindent Zusammenhang: $Y^{-1} \sim \mathrm{Ga}(a, b) \Leftrightarrow Y \sim \mathrm{IG}(a, b)$
  
  
  	\paragraph{Exponentialverteilung} Zeit zwischen Poisson-Ereignissen
  
    \begin{align*}
    & Y \sim \mathrm{Exp}(\lambda) \text{ mit } \lambda > 0,\: y \geq 0\\
    & p(y|\lambda) = \lambda\exp (-\lambda y) \\
    & \mathrm{E}(Y|\lambda) = \frac{1}{\lambda}, \:
	\mathrm{Var}(Y|\lambda) = \frac{1}{\lambda^2}
  \end{align*}
  
  
  	\paragraph{Chi-Quadrat-Verteilung} quadrierte standardnormalverteilte Zufallsvariablen mit $\nu$ Freiheitsgraden
  
    \begin{align*}
    & Y \sim \chi^2(\nu) \text{ mit } \nu > 0,,\: y \in \mathbb{R}\\
    & p(y|\nu) = \frac{y^{\frac{\nu}{2}-1}e^{-\frac{y}{2}}}{2^{\frac{\nu}{2}}\Gamma \left(\frac{\nu}{2}\right)} \\
    & \mathrm{E}(Y|\nu) = \nu, \:
	\mathrm{Var}(Y|\nu) = 2\nu
  \end{align*}

\end{multicols}

\begin{multicols}{2}[\subsubsection{Exponentialfamilie}][4cm]

  \paragraph{Definition} \ \\
  \noindent Zur Exponentialfamilie gehören alle Verteilungen, deren Dichte wie folgt geschrieben werden kann:
  
  $$f_Y(y,\theta) = \exp^{t^T(y)\theta - \kappa (\theta)}h(y)$$
  
  mit $h(y) \geq 0$, $t(y)$ Vektor der kanonischen Statistiken, $\theta$ Parametervektor und $\kappa (\theta)$ Normalisationskonstante.
  
  \paragraph{Normalisierungskonstante}
  
  \begin{align*}
  1 &= \int \exp^{t^T(y)\theta}h(y)dy \exp^{ - \kappa (\theta)} \\
  \Leftrightarrow \kappa (\theta) &= \log \int \exp^{t^T(y)\theta}h(y)dy
  \end{align*}
  
  \noindent $\kappa (\theta)$ ist die kumulanterzeugende Funktion, somit $\frac{\partial\kappa(\theta)}{\partial\theta} = \mathrm{E}(t(Y))$ und $\frac{\partial^2\kappa(\theta)}{\partial\theta^2} = \mathrm{Var}(t(Y))$
  
  
  
  \paragraph{Mitglieder}
  
  \begin{itemize}
  \item \textbf{Poissonverteilung}
  \item \textbf{Geometrische Verteilung}
  \item \textbf{Exponentialverteilung}
  \item \textbf{Normalverteilung}
   $t(y) = \left(-\frac{y^2}{2},y \right)^T$,  
   $\theta = \left(\frac{1}{\sigma^2}, \frac{\mu}{\sigma^2}\right)^T$,
   $h(y) = \frac{1}{\sqrt{2\pi}}$,
   $\kappa ( \theta ) = \frac{1}{2} \left( -\log \frac{1}{\sigma^2} + \frac{\mu^2}{\sigma^2} \right)$
  \item \textbf{Gammaverteilung}
  \item \textbf{Chi-Quadrat-Verteilung}
  \item \textbf{Betaverteilung}
  \end{itemize}
  


\end{multicols}

\begin{multicols}{2}[\subsection{Grenzwertsätze}][4cm]

  \paragraph{Gesetz der großen Zahlen}
  
  \paragraph{Zentraler Grenzwertsatz}
  
  $$Z_n \overset{d}{\longrightarrow} \mathrm{N}(0, \sigma^2)$$
  mit  $Z_n = \sum_{i=1}^{n} \frac{Y_i}{\sqrt{n}}$ und $Y_i$ i.i.d. mit $\mu=0$ und Varianz $\sigma^2$
\begin{Beweis}
Für eine normalverteilte Zufallsvariable $Z \sim \mathrm{N}(\mu, \sigma^2)$ gilt $K_Z(t)=\mu t + \frac{1}{2}\sigma^2t^2$. Die ersten beiden Ableitungen $\frac{\partial^kK_Z(t)}{\partial t^k} \bigg|_{t = 0}$ entsprechen $\mu$ und $\sigma$. Alle anderen Momente sind null. 

\noindent Für $Z_n = (Y_1 + Y_2 + ... +Y_n)/\sqrt{n}$ gilt:
\begin{align*}
M_{Z_n}(t) &= \mathrm{E}\left(e^{t(Y_1 + Y_2 + ... +Y_n)/\sqrt{n}}\right)\\
&= \mathrm{E}\left(e^{tY_1/\sqrt{n}} \cdot e^{tY_2/\sqrt{n}}\cdot ... \cdot e^{tY_n/\sqrt{n}}\right) \\
&= \mathrm{E}\left(e^{tY_1/\sqrt{n}}\right) \mathrm{E}\left(e^{tY_2/\sqrt{n}}\right) ... \mathrm{E}\left(e^{tY_n/\sqrt{n}}\right) \\
&= M_Y^n(t/\sqrt{n})
\end{align*}
Analog gilt: $K_{Z_n}(t) = nK_Y(t/\sqrt{n})$.
\begin{align*}
 \frac{\partial K_{Z_n}(t)}{\partial t} \bigg|_{t = 0} &= \frac{n}{\sqrt{n}} \frac{\partial K_Y(t)}{\partial t} \bigg|_{t = 0} = \sqrt{n}\mu \\
 \frac{\partial^2K_{Z_n}(t)}{\partial t^2} \bigg|_{t = 0} &= \frac{n}{n} \frac{\partial^2 K_Y(t)}{\partial t^2} \bigg|_{t = 0} = \sigma^2
\end{align*}
Mithilfe der Taylorreihe können wir $K_{Z_n}(t) = 0 + \sqrt{n}\mu t + \frac{1}{2}\sigma^2t^2 + ...$ schreiben, wobei die Terme in $...$ alle für $n \rightarrow \infty$ gegen 0 gehen.

\noindent Damit gilt $K_{Z_n}(t) \overset{n\rightarrow\infty}{\longrightarrow} K_{Z}(t)$ mit $Z \sim \mathrm{N}(\sqrt{n}\mu,\sigma^2)$.
\end{Beweis}

\end{multicols}

  
%-------------------------------------------------------------------------------

% SECTION: HYPOTHESENTESTS

%-------------------------------------------------------------------------------

\section{Hypothesentests}

\subsection{Tests für Einstichprobenprobleme}

\begin{multicols}{2}[\subsubsection{Normalverteilung}][4cm]

  \paragraph{ $\mu$ gesucht, $\sigma^2 $ bekannt (Einfacher Gauß-Test)}
  


\end{multicols}


%-------------------------------------------------------------------------------

% SECTION: REGRESSION

%-------------------------------------------------------------------------------

\section{Regression}

\subsection{Annahmen}

\subsection{Verfahren}
\begin{multicols}{2}[\subsubsection{Kleinste Quadrate (OLS)}][4cm]

\paragraph{KQ-Schätzer (Einfachregression)}

$$\hat{\beta}_1=\frac{Cov(x,y)}{Var(x)}=\frac{S_{xy}}{S_{xx}}= \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}} \cdot \sqrt{\frac{S_{yy}}{S_{xx}}}=r\sqrt{\frac{S_{yy}}{S_{xx}}}$$

\begin{Beweis}
$Cov(x,y)=Cov(x,\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x})=\hat{\beta}_1Var(x)$

\raggedleft
$ \iff \hat{\beta}_1= \frac{Cov(x,y)}{Var(x)}$
\end{Beweis}

$$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$

\begin{Beweis}
$E\left[y\right] = E\left[\hat{\beta}_0+\hat{\beta}_1 x+\hat{e}\right] \iff \hat{\beta}_0 = E\left[y\right] - \hat{\beta}_1E\left[x\right]$
\end{Beweis}

\end{multicols}

\subsection{Modell}

\begin{multicols}{2}[\subsubsection{lineare Einfachregression}][4cm]

\paragraph{Theoretisches Modell}

$$y_i=\beta_0+\beta_1x_i+u_i$$

\paragraph{Empirisches Modell}

$$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_i+e_i$$

\paragraph{Eigenschaften der Regressionsgeraden}
\begin{equation*}
\begin{split}
\hat{y}_i & = \hat{\beta}_0+\hat{\beta}_1x_i  =\bar{y}+ \hat{\beta}_1(x_i-\bar{x}) \\
\hat{e}_i  & =  y_i-\hat{y}_i = y_i-(\hat{\beta}_0+\hat{\beta}_1x_i) \\
 & =y_i-(\bar{y}+ \hat{\beta}_1(x_i-\bar{x})) \\
\sum\limits_{i=1}^n\hat{e}_i & = \sum\limits_{i=1}^ny_i-\sum\limits_{i=1}^n\bar{y}-\hat{\beta}_1\sum\limits_{i=1}^n(x_i-\bar{x}) \\
 & = n\bar{y}-n\bar{y}-\hat{\beta}_1(n\bar{x}-n\bar{x})=0 \\
\bar{\hat{y}} & = \frac{1}{n}\sum\limits_{i=1}^n\hat{y}_i=\frac{1}{n}(n\bar{y}+\hat{\beta}_1(n\bar{x} - n\bar{x})) = \bar{y}
\end{split}
\end{equation*}



\end{multicols}

\subsubsection{Multivariate lineare Regression}

%\subsubsection{Spezialfall: Zeitreihen}

\begin{multicols}{2}[\subsection{ANOVA (Streuungszerlegung)}][4cm]

$$SS_{Total}=SS_{Explained}+SS_{Residual}$$

mit
\begin{align*}
SS_{Total}  &=  \sum\limits_{i=1}^n(y_i-\bar{y})^2 \\
SS_{Explained} &= \sum\limits_{i=1}^n(\hat{y}_i-\bar{y})^2 \\
SS_{Residual} &= \sum\limits_{i=1}^n(y_i-\hat{y}_i)^2=\sum\limits_{i=1}^n e_i^2=S_{yy}-\hat{\beta}^2S_{xx} 
\end{align*}

\end{multicols}

\subsection{Gütemaße}

\begin{multicols}{2}[\subsubsection{Bestimmtheitsmaß}][4cm]

$$R^2=\frac{SS_{Explained}}{SS_{Total}}=1-\frac{SS_{Residual}}{SS_{Total}}=r^2$$

Wertebereich: $0 \le R^2 \le 1$

\end{multicols}

%-------------------------------------------------------------------------------

% SECTION: INFERENZ

%-------------------------------------------------------------------------------


\section{Inferenz}


\subsection{Methode der Momente}

Die theoretischen Momente werden durch die empirischen geschätzt: \ \\
\vspace{0.5em}
$\mathrm{E}_{\hat{\theta}_{MM}}(Y^k) = m_k(y_1,...,y_n)$ \ \\
\vspace{0.5em}
\noindent Für die Exponentialfamilie gilt: $\hat{\theta}_{MM} = \hat{\theta}_{ML}$


\begin{multicols}{2}[\subsection{Verlustfunktionen}][4cm]

\paragraph{Verlust}

$$\mathcal{L}: \mathcal{T} \times \Theta \rightarrow \mathbb{R}^+$$

\noindent mit Parameterraum $\Theta \subset \mathbb{R}$, $t\in \mathcal{T}$ mit $t:\mathbb{R}^n \rightarrow \mathbb{R}$ eine Statistik, die den Parameter $\theta$ schätzt. 

Es gilt: $\mathcal{L}(\theta, \theta) = 0$

\begin{itemize}
\item \textbf{absoluter Verlust (L1)}: $\mathcal{L}(t, \theta) = (t-\theta)^2$
\item \textbf{quadratischer Verlust (L2)}: $\mathcal{L}(t, \theta) = \left|t-\theta \right|$
\end{itemize}

\noindent Da $\theta$ unbekannt ist, ist der Verlust eine theoretische Größe. Zudem ist er die Realisation einer Zufallsvariable, da er von einer konkreten Stichprobe abhängt.

\paragraph{Risiko}

\begin{align*}
R(t(.), \theta) &= \mathrm{E}_\theta \left(\mathcal{L}(t(Y_1,...,Y_n),\theta)\right) \\
&= \int_{-\infty}^\infty \mathcal{L}(t(Y_1,...,Y_n),\theta) \prod_{i=1}^n f(y_i;\theta)dy_i
\end{align*}

hier auch Kullback Leibler Distanz?

\end{multicols}


\begin{multicols}{2}[\subsection{Maximum Likelihood (ML)}][4cm]
  \paragraph{Voraussetzungen}
  
  \begin{itemize}
  \item $Y_i \sim f(y;\theta)\:\: i.i.d.$
  \item $\theta \in \mathbb{R}^p$
  \item $f(.;\theta)$ Fisher-regulär:
  \begin{itemize}
  \item $\{ y: f (y; \theta > 0) \}$ unabhängig von $\theta$
  \item Möglicher Parameterraum $\Theta$ ist offen
  \item $f(y;\theta)$ zweimal differenzierbar
  \item $\int \frac{\partial}{\partial \theta} f(y;\theta)dy = \frac{\partial}{\partial \theta} \int f(y;\theta)dy$
  \end{itemize}
  \end{itemize}
  
  \paragraph{Zentrale Funktionen} \ \\
  \begin{itemize}
  \item \textbf{Likelihood} $L(\theta;y_1,...,y_n)$: $\prod_{i=1}^n f(y_i;\theta)$
  \item \textbf{log-Likelihood} $l(\theta;y_1,..y_n)$: 
$\log L(\theta;y_1,...,y_n) = \sum_{i=1}^n \log f(y_i;\theta)$
  \item \textbf{Score} $s(\theta;y_1,...,y_n)$: $\frac{\partial l(\theta;y_1,..y_n)}{\partial \theta}$
  \item \textbf{Fisher-Information} $I(\theta)$: $-\mathrm{E}_\theta\left(\frac{\partial s(\theta;Y_1,...,Y_n)}{\partial\theta}\right)$
  \end{itemize}
  
  
  \paragraph{Eigenschaften der Score-Funktion} \ \\
  
  erste Bartlett Gleichung:
  
  $$\mathrm{E}\left(s(\theta;Y)\right) = 0$$
  
\begin{Beweis}
\vspace{-1.5em}
\begin{align*}
1 &= \int f(y;\theta) dy \\
0 = \frac{\partial 1}{\partial\theta} &= \int \frac{\partial f(y;\theta)}{\partial \theta}dy = \int \frac{\partial f(y;\theta) / \partial\theta}{f(y;\theta)} f(y;\theta) dy \\ &= \int \frac{\partial}{\partial\theta} \log f(y;\theta) f(y;\theta) dy = \int s(\theta;y) f(y;\theta) dy
\end{align*}
\end{Beweis}
  
  zweite Bartlett Gleichung:
  
  $$\mathrm{Var}_\theta\left(s(Y;\theta)\right) = \mathrm{E}_\theta\left(-\frac{\partial^2 log f(Y;\theta)}{\partial\theta^2}\right) = I(\theta)$$
  
\begin{Beweis}
\vspace{-1.5em}
\begin{align*}
0 = \frac{\partial 0}{\partial \theta} =& \frac{\partial}{\partial\theta}\int \frac{\partial}{\partial\theta} \log f(y;\theta) f(y;\theta) dy \hspace{2em}\text{      siehe oben}\\
=& \int \left( \frac{\partial^2}{\partial \theta^2} \log f(y;\theta)\right) f(y;\theta) dy  \\
&+ \int\frac{\partial \log f(y;\theta)}{\partial \theta}\frac{\partial f(y;\theta)}{\partial \theta}dy \\
=& \: \mathrm{E}_\theta \left( \frac{\partial^2}{\partial \theta^2} \log f(Y;\theta)\right)  \\
&+ \int\frac{\partial \log f(y;\theta)}{\partial \theta}\frac{\partial \log f(y;\theta)}{\partial \theta} f(y;\theta) dy
\end{align*}
$\Leftrightarrow \mathrm{E}_\theta \left(s(\theta;Y) s(\theta;Y)\right) = \mathrm{E}_\theta \left(- \frac{\partial^2}{\partial \theta^2} \log f(Y;\theta)\right)$

\noindent Bartletts zweite Gleichung gilt dann, weil $\mathrm{E}\left(s(\theta;Y)\right) = 0$
\end{Beweis}
  
  \paragraph{ML-Schätzer}
   $$\hat{\theta}_{ML} = \text{arg max } l(\theta; y_1,...y_n)$$
   
   für Fisher-reguläre Verteilungen:
   $s\left(\hat{\theta}_{ML};y_1,...,y_n\right) = 0$
   
   Der ML-Schätzer ist invariant.
  

\end{multicols}


\begin{multicols}{2}[\subsection{Suffizienz, Konstistenz und Effizienz}][4cm]

\end{multicols}


\begin{multicols}{2}[\subsection{Konfidenzintervalle}][4cm]

\end{multicols}

%-------------------------------------------------------------------------------

% SECTION: KLASSIFIKATION

%-------------------------------------------------------------------------------

\section{Klassifikation}

\subsection{Diskriminanzanalyse (Bayes)}


%-------------------------------------------------------------------------------

% SECTION: CLUSTERANALYSE

%-------------------------------------------------------------------------------

\section{Clusteranalyse}

%-------------------------------------------------------------------------------

% SECTION: BAYESSCHE STATISTIK

%-------------------------------------------------------------------------------

\section{Bayessche Statistik}

\begin{multicols}{2}[\subsection{Grundlagen}][4cm] 

\paragraph{Bayes-Formel}
$$P(A|B)=\frac{P(B|A)P(A)}{P(B)} \hspace{0.5cm} \text{für } P(A), P(B) > 0$$

\begin{center}oder allgemeiner:\end{center}

\vspace{-1 em}

\begin{align*}
  f(\theta | X) &= \frac{f(X | \theta ) \cdot f(\theta)}{\int f(X|\tilde{\theta}) f(\tilde{\theta})  d \tilde{\theta}}\\
  &= C \cdot f(X | \theta ) \cdot f(\theta) \hspace{1 em} \text{wähle C so, dass $\int f(\theta | X)=1$} \\
  &\propto f(X | \theta ) \cdot f(\theta)
\end{align*}

\paragraph{Punktschätzer}

\paragraph{Kredibilitätsintervall}

\paragraph{Sensitivitätsanalyse}

\paragraph{Prädiktive Posteriori}

$$f(x_Z|\mathbf{x}) =\int f(x_Z, \lambda|\mathbf{x})d\lambda = \int f(x_Z|\lambda)p(\lambda|\mathbf{x})$$

\paragraph{Uninformative Priori} ~\\

$f(\theta)=const. \text{ für } \theta > 0$
,  damit:
 $f(\theta | X) = C \cdot f(X | \theta )$

\noindent (Da $\int f(\theta) =1$ so nicht möglich, ist das eigentlich keine Dichte)

\paragraph{Konjugierte Priori}

\begin{Extensiv}
~\\
  \noindent Wenn die Priori- und die Posteriori-Verteilung denselben Typ hat für eine gegebene Likelihoodfunktion, so nennt man sie konjugiert.
  
\end{Extensiv}

\vspace{1 em}
\noindent Binomial-Beta-Modell: \begin{itemize}
  \vspace{-0.7 em}
\setlength\itemsep{-0.7 em}
\item Priori $ \sim Be(\alpha,\beta)$
\item $X$ $ \sim Binom(n,p,k)$
\item Posteriori $\sim Be(\alpha + k, \beta + n - k)$
  
\end{itemize}

\end{multicols}


\begin{multicols}{2}[\subsection{Markov Chain / Monte Carlo}][4cm]



\end{multicols}

\end{document}
