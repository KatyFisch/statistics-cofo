% To Do: paragrah widow penalty   
%        
%      
\documentclass[8pt]{extarticle}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage[a4paper,left=2.3cm,right=1.2cm,top=2cm,bottom=2cm]{geometry} 
\usepackage{blindtext}
\usepackage{setspace}
\usepackage{float}
\usepackage{titletoc}
\usepackage{titlesec}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{amsmath} 
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{verbatimbox}
\usepackage{enumitem}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{bigints}
\onehalfspacing
\usepackage[hidelinks]{hyperref}
\usepackage[all]{nowidow} %funktioniert nicht....
\clubpenalty=9996
\widowpenalty=9999
\brokenpenalty=4991
\predisplaypenalty=10000
\postdisplaypenalty=1549
\displaywidowpenalty=1602

\setlength\parindent{0pt}



\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}



%Hier sind die unterschiedlichen Ausführlichkeitsgrade definiert
\includecomment{Extensiv} 
\includecomment{Proof} 
\includecomment{Annahmen}
\includecomment{Mathspez}
\includecomment{Mathfolg}
\includecomment{Rechreg}
\mdfdefinestyle{MyFrame}{%
    linecolor=black!20!,
    outerlinewidth=0.2pt,
    roundcorner=5pt,
    innertopmargin=0.5\baselineskip,
    innerbottommargin=0.5\baselineskip,
    innerrightmargin=10pt,
    innerleftmargin=10pt,
    backgroundcolor=white}
\specialcomment{Proof}{\begin{mdframed}[style=MyFrame,nobreak=true] Proof: \ \\}{\end{mdframed}}
\specialcomment{Rechreg}{\noindent \textit{Calculation Rules:} \begin{itemize}[nosep,label=$\star$] }{\end{itemize}}
\renewcommand\ThisComment[1]{% Fix for Umlauts in comments
  \immediate\write\CommentStream{\unexpanded{#1}}%
}

% Hier die Ausführlichkeit bestimmen:
%\excludecomment{Extensiv} 
%\excludecomment{Proof} 
%\excludecomment{Annahmen}
%\excludecomment{Mathspez}
%\excludecomment{Mathfolg}

% Inhaltsverzeichnis mit zwei Spalten
\usepackage[toc]{multitoc}
\renewcommand*{\multicolumntoc}{2}




%Überschriftengrößen anpassen, so dass Paragraph kleiner ist als Subsubsection
\titleformat{\section}
  {\normalfont\fontsize{16}{15}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\fontsize{14}{15}\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\fontsize{12}{15}\bfseries}{\thesubsubsection}{1em}{}


\begin{document}
\hrule
\begin{center}
{\fontsize{30}{60}\selectfont \textbf{Statistics}} \\ \

{\fontsize{20}{60}\selectfont Collection of Formulas}
\end{center}
\hrule
\vfill
\tableofcontents

\clearpage


% weitere Anpassungen im Hauptteil des Dokuments
\raggedright %linksbündig
\setlength{\parindent}{15pt} %Einzuglänge festsetzen
\setlength{\columnseprule}{0.3pt} %Liniendicke zwischen zwei Multicols





%-------------------------------------------------------------------------------

% SECTION: DESKRIPTIVE STATISTIK

%-------------------------------------------------------------------------------

\section{Deskriptive Statistics}


\subsection{Summary Statistics: Sample}

\begin{multicols}{2}[\subsubsection{Location}][4cm] 

\paragraph{Mode}

 Most frequent value of $x_i$. Two or more modes are possible (bimodal).

\paragraph{Median}

$$\tilde{x}_{0.5}=\begin{cases} x_{((n+1)/2)} & \text{falls }n\text{ ungerade} \\ \frac{1}{2}(x_{(n/2)}+x_{(n/2+1)} & \text{falls }n\text{ gerade} \end{cases}$$

\paragraph{Quantile}

$$\tilde{x}_\alpha=\begin{cases} x_{(k)} & \text{falls } n\alpha \notin \mathbb{N}\\ \frac{1}{2}(x_{(n\alpha)}+ x_{(n\alpha+1)}) & \text{falls } n\alpha \text{ ganzzahlig} \end{cases}$$

with

\begin{tabular}{l l}
 $k=\min x$ $\in \mathbb{N}$, &  $x$ $>$ $n\alpha$ \\
\end{tabular}

\paragraph{Minimum/Maximum}


$$x_{\min}=\min_{i \in \{ 1,...,N\}} (x_i) \hspace{0.8cm}   x_{\max}=\max_{i \in \{ 1,...,N\}} (x_i)$$
 


\paragraph{Arithmetic Mean}

 $$\bar{x}=\frac{1}{n}\sum\limits_{i=1}^n x_i$$

\noindent Estimates the expectation
$\mu = E[X]$ (first~moment).

\begin{Rechreg}
\item $E(a+b\cdot X)=a+b\cdot E(X)$
\item $E(X\pm Y)=E(X)\pm E(Y)$
\end{Rechreg}

\paragraph{Geometric Mean}

$$\bar{x}_G=\sqrt[n]{\sum\limits_{i=1}^n x_i} $$

\noindent For growth factors: $\bar{x}_G=\sqrt[n]{\frac{B_n}{B_0}}$

\paragraph{Harmonic Mean}

$$\bar{x}_H=\frac{\sum\limits_{i=1}^n w_i}{\sum\limits_{i=1}^n \frac{w_i}{x_i}}$$


\end{multicols}


\begin{multicols}{2}[\subsubsection{Dispersion}][4cm] 

\paragraph{Range}

$$R=x_{(n)}-x_{(1)}$$

\paragraph{Interquartile Range}

$$d_Q=\tilde{x}_{0.75}-\tilde{x}_{0.25}$$

\paragraph{(Empirical) Variance}

$$s^2=\frac{1}{n}\sum\limits_{i=1}^n(x_i-\bar{x})^2=\frac{1}{n}\sum\limits_{i=1}^nx_i^2-\bar{x}^2$$

\noindent Estimates the second centralized moment.

\begin{Rechreg}
\item $Var(aX+b)=a^2\cdot Var(X)$
\item $Var(X\pm Y)= Var(X)+Var(Y) + 2Cov(X,Y)$
\end{Rechreg}

\paragraph{(Empirical) Standard Deviation}

$$s=\sqrt{s^2}$$

\paragraph{Coefficient of Variation}

$$ \nu=\frac{s}{\bar{x}}$$

\paragraph{Average Absolute Deviation}


$$ \mathit{e} = \frac{1}{n}\sum_{i=1}^n \left|x_i - \bar{x}\right|$$

\noindent Estimates the first absolute centralized moment.

\end{multicols}



\begin{multicols}{2}[\subsubsection{Concentration}][4cm] 

\paragraph{Gini Coefficient}

\begin{equation*} 
\begin{split}
G & = \frac{2\sum\limits_{i=1}^n ix_{(i)}-(n+1)\sum\limits_{i=1}^n x_{(i)}}{n\sum\limits_{i=1}^n x_{(i)}}  = 1-\frac{1}{n}\sum\limits_{i=1}^n(v_{i-1}+v_i)
\end{split}
\end{equation*}

with

$$  u_i=\frac{i}{n}, \hspace{0.3cm} v_i= \frac{\sum\limits_{j=1}^i x_{(j)}}{\sum\limits_{j=1}^i x_{(j)}} \hspace{0.7cm} (u_0=0, \hspace{0.2cm} v_0=0 )$$


\noindent These are also the values for the Lorenz curve.

\ \\

\indent Range: $ 0 \le G \le \frac{n-1}{n}$




\paragraph{Lorenz-Münzner Coefficient (normed $G$)}

$$G^+=\frac{n}{n-1}G$$

\indent Range: $ 0 \le G^+ \le 1$






\end{multicols}




\begin{multicols}{2}[\subsubsection{Shape}][4cm] 

\paragraph{(Empirical) Skewness}
$$\nu = \frac{n}{(n-1)(n-2)} \sum_{i=1}^n \left(\frac{x_i-\bar{x}}{s}\right)^3$$

\noindent Estimates the third centralized moment, scaled with $(\sigma^2)^{\frac{2}{3}}$

\paragraph{(Empirical) Kurtosis}

$$k=\left[n(n+1) \cdot \sum_{i=1}^n \left(\frac{x_i-\bar{x}}{s}\right)^4 - 3(n-1)\right] \cdot \frac{n-1}{(n-2)(n-3)}+3$$

\noindent Estimates the fourth centralized moment, scaled with $(\sigma^2)^2$

\paragraph{Excess}

$$\gamma=k-3$$

\end{multicols}



\begin{multicols}{2}[\subsubsection{Dependence}][4cm]

\subsubsection*{\textit{for two nominal variables}}

\paragraph{$\chi^2$-Statistic}

\begin{equation*}
\begin{split}
\chi^2 & =\sum\limits_{i=1}^k \sum\limits_{j=1}^l \frac{(n_{ij}-\frac{n_{i+}n_{+j}}{n})^2}{\frac{n_{i+}n_{+j}}{n}}  =n\left(\sum\limits_{i=1}^k \sum\limits_{j=1}^l \frac{n_{ij}^2}{n_{i+}n_{+j}}-1\right)
\end{split}
\end{equation*}

Range: $ 0 \le \chi^2 \le n(\min(k,l)-1)$

\paragraph{Phi-Coefficient}

$$\Phi=\sqrt{\frac{\chi^2}{n}}$$

Range: $ 0 \le \Phi \le \sqrt{\min(k,l)-1}$

\paragraph{Cram\'er's $V$}

$$ V= \sqrt{\frac{\chi^2}{\min(k,l)-1}}$$

Range: $ 0 \le V \le 1$

\paragraph{Contingency Coefficient $C$}

$$C=\sqrt{\frac{\chi^2}{\chi^2 + n}}$$

Range: $ 0 \le C \le \sqrt{\frac{\min(k,l)-1}{\min(k,l)}} $

\paragraph{Corrected Contingency Coefficient $C_{corr}$}

$$C_{corr}= \sqrt{\frac{\min(k,l)}{\min(k,l)-1}} \cdot \sqrt{\frac{\chi^2}{\chi^2 + n}} $$

Range $ 0 \le C_{corr} \le 1 $

\paragraph{Odds-Ratio}

$$OR=\frac{ad}{bc} = \frac{n_{ii}n_{jj}}{n_{ij}n_{ji}}$$

Range: $0 \le OR < \infty$

\subsubsection*{\textit{for two ordinal variables}}

\paragraph{Gamma (Goodman and Kruskal)}

$$\gamma=\frac{K-D}{K+D}$$


\begin{tabular}{l l }
$ K=\sum_{i<m}\sum_{j<n}n_{ij}n_{mn}$ & Number of concordant pairs \\
$ D=\sum_{i<m}\sum_{j>n}n_{ij}n_{mn}$ & Number of reversed pairs \\
\end{tabular}

\ \\

Range: $-1 \le \gamma \le 1$

\paragraph{Kendall's $\tau_b$}

$$ \tau_b=\frac{K-D}{\sqrt{(K+D+T_X)(K+D+T_Y)}}$$

with

\begin{tabular}{l l } 
$ T_X=\sum_{i=m}\sum_{j<n}n_{ij}n_{mn}$ & Number of ties w.r.t. $X$ \\
$ T_Y=\sum_{i<m}\sum_{j=n}n_{ij}n_{mn}$ & Number of ties w.r.t. $Y$ \\
\end{tabular}

\ \\

Range: $-1 \le \tau_b \le 1$

\paragraph{Kendall's/Stuart's $\tau_c$}

$$\tau_c=\frac{2\min(k,l)(K-D)}{n^2(\min(k,l)-1)}$$

Range: $-1 \le \tau_c \le 1$

\paragraph{Spearman's Rank Correlation Coefficient}

$$\rho=\frac{n(n^2-1)-\frac{1}{2}\sum\limits_{j=1}^J b_j(b_j^2-1)-\frac{1}{2}\sum\limits_{k=1}^K c_k(c_k^2-1)-6\sum\limits_{i=1}^n d_i^2}{\sqrt{n(n^2-1)-\sum\limits_{j=1}^J b_j(b_j^2-1)}\sqrt{n(n^2-1)-\sum\limits_{k=1}^Kc_k(c_k^2-1)}}$$

or

$$\rho=\frac{s_{rg_xrg_y}}{\sqrt{s_{rg_xrg_x}s_{rg_yrg_y}}}$$

 Without ties:

$$\rho=1-\frac{6\sum\limits_{i=1}^nd_i^2}{n(n^2-1)}$$

with

\begin{tabular}{l l } 
 $d_i=R(x_i)-R(y_i)$ & rank difference \\ 
\end{tabular}

\ \\

Range: $-1 \le \rho \le 1$

\subsubsection*{\textit{for two metric variables}}

\paragraph{Correlation Coefficient (Bravais-Pearson)}

$$r=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}=\frac{s_{xy}}{\sqrt{s_{xx}s_{yy}}}$$

with

\begin{tabular}{l l } 
$S_{xy}=\sum\limits_{i=1}^n(x_i-\bar{x})^2(y_i-\bar{y})^2$ & or $s_{xy}=\frac{S_{xy}}{n}$ \\
$S_{xx}=\sum\limits_{i=1}^n(x_i-\bar{x})^2$ & or $s_{xx}=\frac{S_{xx}}{n}$ \\ 
$S_{yy}=\sum\limits_{i=1}^n(y_i-\bar{y})^2$ & or $s_{yy}=\frac{S_{yy}}{n}$ \\
\end{tabular}

\ \\

Range: $-1 \le r \le 1$


%\subsubsection*{\textit{Für zwei unterschiedliche Variablen}}


\end{multicols}

\subsection{Tables}

\subsection{Diagrams}

\begin{multicols}{2}[\subsubsection{Histogram}][4cm]



\begin{tikzpicture}

\draw[-latex] (0.5,0) -- (0.5,3) node[below left]{Frequency};
\draw[-latex] (0.5,0) -- (5,0) node[below]{Variable};

\draw[fill=black!50!] (1,0) rectangle (2,1.5);
\draw[fill=black!50!] (2,0) rectangle (3,2.5);
\draw[fill=black!50!] (3,0) rectangle (4,1);

\draw[dashed] (1,1.5) -- (0.4,1.5) node[left]{$v_0$};
\draw[dashed] (1,0) -- (1,-0.5) node[below]{$t_0$};
\draw[dashed] (2,0) -- (2,-0.5) node[below]{$t_1$};
\draw[dashed] (4,0) -- (4,-0.5) node[below]{$t_m$};
\draw[<->] (1,-0.2) -- (2,-0.2) node[midway, below]{$h$};

\end{tikzpicture}

\begin{minipage}{\columnwidth}
sample: $X=\{ x_1,x_2,...;x_n\}$ \\
$k$-th bin: $ B_k=\left[t_k,t_{k+1}\right), k=\{0,1,...,m-1\} $ \\
Number of observations in the $k$-th bin: $v_k$ \\
bin width: $h=t_{k+1}-t_k, \forall k$ \\
\end{minipage}

\paragraph{Scott's Rule}

$$h^* \approx 3.5\sigma n^{-\frac{1}{3}}$$

\noindent For approximately normal distributed data (min.\ MSE)


\end{multicols}

\subsubsection{QQ-Plot}

\subsubsection{Scatterplot}

%-------------------------------------------------------------------------------

% SECTION: PROBABILITY
%-------------------------------------------------------------------------------

\section{Probability}

\subsection{Combinatorics}

% first column
\begin{minipage}[t]{0.7\textwidth}
\addvbuffer[12pt 8pt]{\begin{tabular}{l r || c | c}
& & without replacement & with replacement \\
\midrule
Permutations & & $n!$ & $\frac{n!}{n_1!\cdot\cdot\cdot n_s!}$ \\
\midrule
Combinations: & without order & $\binom{n}{m}$ & $\binom{n+m-1}{m}$ \\
& with order & $\binom{n}{m}m!$ & $n^m$ \\
\end{tabular}}
\end{minipage}
%second column
\begin{Mathspez}
\begin{minipage}[b]{0.2\textwidth}
with: 


 $n!=n\cdot (n-1)\cdot ... \cdot 1$

 $\binom{n}{m} = \frac{n!}{m!(n-m)!}$

\end{minipage}
\end{Mathspez}


\begin{multicols}{2}[\subsection{Probability Theory}][4cm]


\paragraph{Laplace}

$$P(A)=\frac{|A|}{|\Omega|}$$


\paragraph{Kolmogorov Axioms} mathematical definition of probability

\addvbuffer[12pt 8pt]{\begin{tabular}{c l}

(1) & $0 \le P(A) \le 1 \hspace{0.5 cm} \forall A \in \mathcal{A}$ \\

(2) & $P(\Omega) = 1$ \\

(3) & $P(\bigcup_{i=1}^{\infty}{A_i}) = \sum_{i=1}^{\infty}{P(A_i)}$ \\ 
    & $\forall A_i \in \mathcal{A}, i=1,...,\infty \text{ with } A_i \cap A_j = \emptyset \text{ for } i \neq j$ \\

\end{tabular}}


\begin{Mathfolg}

Implications:
\begin{itemize}
\item $P(\bar{A})=1-P(A)$
\item $P(\emptyset)=0$
\item $P(A\cup B)=P(A)+P(B)-P(A\cap B)$
\item $A \subseteq B \Rightarrow P(A) \le P(B)$
\item $P(B)=\sum\limits_{i=1}^n P(B\cap A_i), \textnormal{ f"ur } A_i,...,A_n$  complete decomposition of $\Omega$  into pairwise disjoint events
\end{itemize}

\end{Mathfolg}

\paragraph{Probability (Mises)} frequentist definition of probability

$$P(A) = \lim\limits_{n \to \infty}\frac{n_A(n))}{n}$$

\noindent with $n$ repetitions of a random experiment and $n_A(n)$ events $A$

\paragraph{Conditional Probability}

$$P(A|B)=\frac{P(A \cap B)}{P(B)} \hspace{0.5cm} \text{für } P(B) > 0$$


$ \Rightarrow P(A \cap B)=P(B|A)P(A)=P(A|B)P(B)$

\paragraph{Law of Total Probability}

$$P(B)=\sum\limits_{i=1}^nP(B|A_i)P(A_i)$$

\paragraph{Bayes' Theorem}

$$P(A|B)=\frac{P(B|A)P(A)}{P(B)} \hspace{0.5cm} \text{for } P(A), P(B) > 0$$

\paragraph{Stochastic Independence}

\begin{alignat*}{2}
 \text{A, B independent}  \Leftrightarrow  && P(A\cap B) &= P(A)+P(B) \\
 \text{X, Y independent}  \Leftrightarrow && f_{XY}(x,y) &= f_X(x)\cdot f_Y(y) \hspace{0.5cm} \forall x,y
\end{alignat*}

\end{multicols}


\begin{multicols}{2}[\subsection{Random Variables/Vectors}][4cm]

\subsubsection*{\textit{Random Variables $\in \mathbb{R}$}}

\paragraph{Definition}

$$Y: \Omega \to \mathbb{R}$$

\noindent The Subset of possible values for $\mathbb{R}$ is called support.

\noindent Notation: Realisations of $Y$ are depicted with lower case letters. $Y=y$ means, that $y$ is the realisation of $Y$.

\paragraph{Discrete and Continuous Random Variables} \ \\

\noindent If the support is uncountably infinite, the random variable is called \textit{continuous}, otherwise it is called \textit{discrete}.

\begin{itemize}
\item \textbf{Density \boldmath$f(\cdot)$:} 

For continuous variables:
$P(Y \in \left[a, b\right]) = \int_{a}^{b} f_Y(y) dy$

For discrete variables the density (and other functions) can be depicted like the corresponding function for continuous variables, if the notation is extended as follows: 
$\int_{-\infty}^{y} f_Y(\tilde{y})d\tilde{y} := \sum_{k:k \leq y} P(Y=k)$. This notation is used.

\item \textbf{Cumulative Distribution Function \boldmath$F(\cdot)$:} 
$F_Y(y) =P(Y\leq y)$
\end{itemize}

Relationship:

$$F_Y(y) = \int_{-\infty}^{y} f_Y(\tilde{y})d\tilde{y}$$



\paragraph{Moments}

\begin{itemize}
\item \textbf{Expectation (1.\ Moment)}: $\mu = E(Y) = \int y f_Y(y)dy$
\item \textbf{Variance (2.\ centralized Moment)}: $\sigma^2 = Var(Y) = E(\{Y-E(Y)\}^2) = \int (y - E(Y))^2 f(y) dy$ \\
Note: $E(\{Y-\mu\}^2) = E(Y^2) - \mu^2$
\begin{Proof}
$E(\{Y-\mu\}^2) = E(Y^2 - 2Y\mu + \mu^2) = E(Y^2) - 2\mu^2 + \mu^2 = E(Y^2) - \mu^2$
\end{Proof}
\item \textbf{$k$th Moment}: $E(Y^k) = \int y^k f_Y(y) dy$,\\ \textbf{k.\ centralized Moment}: $E(\{Y-E(Y)\}^k)$
\end{itemize}

\paragraph{Moment Generating Function}

$$M_Y(t) = \mathrm{E}(e^{tY})$$

with $\frac{\partial^kM_Y(t)}{\partial t^k} \bigg|_{t = 0} = \mathrm{E}(Y^k)$ 

Cumulant Generating Function $K_Y(t) = \log M_Y(t)$

\noindent A random variable is uniquely defined by its moment generating function and vice versa (as long as moments and cumulants are finite).
  
\subsubsection*{\textit{Random Vectors $\in \mathbb{R}^q$}}

\paragraph{Density and Cumulative Distribution Function}

$$F(y_1, ..., y_q) = P(Y_1 \leq y_1, ..., Y_q \leq y_q)$$
\begin{align*}
& P(a_1 \leq Y_1 \leq b_1, ..., a_q \leq Y_q \leq b_q) \\
& = \int_{a_1}^{b_1} ...\int_{a_q}^{b_q} f(y_1, .., y_q)dy_1...dy_q
\end{align*}

\paragraph{Marginal Density}

$$f_{Y_1}(y_1) = \int_{-\infty}^{\infty}...\int_{-\infty}^{\infty} f(y_1,...,y_k)dy_2...dy_k$$

\paragraph{Conditional Density}

$$f_{Y_1|Y_2}(y_1|y_2) = \frac{f(y_1, ..., y_2)}{f(y_2)} \text{ f"ur } f(y_2) > 0$$

\paragraph{Iterated Expectation}

$$\mathrm{E}(Y)=\mathrm{E}_X(\mathrm{E}(Y|X))$$
\begin{Proof}
$$\mathrm{E}(Y) = \int yf(y)dy = \int\int y f(y|x)dy f_X(x)dx = \mathrm{E}_X(\mathrm{E}(Y|X))$$
\end{Proof}
$$\mathrm{Var}(Y) = \mathrm{E}_X(\mathrm{Var}(Y|X)) + \mathrm{Var}_X(\mathrm{E}(Y|X))$$
\begin{Proof}
\begin{align*}
\mathrm{Var}(Y) =&  \int (y- \mu_Y)^2 f(y)dy\\
=& \int (y- \mu_Y)^2 f(y|x)f(x)dydx\\
=& \int (y- \mu_{Y|x} + \mu_{Y|x} - \mu_Y)^2 f(y|x)f(x)dydx\\
=& \int (y- \mu_{Y|x})^2 f(y|x)f(x)dydx + \\
 & \int (\mu_{Y|x} - \mu_Y)^2 f(y|x)f(x)dydx + \\
 & 2 \int (y- \mu_{Y|x})(\mu_{Y|x} - \mu_Y) f(y|x)f(x)dydx \\
=& \int \mathrm{Var}(Y|x)f(x)dx + \int (\mu_{Y|x} - \mu_Y)^2f(x)dx\\
=& \mathrm{E}_X(\mathrm{Var}(Y|X)) + \mathrm{Var}_X(\mathrm{E}(Y|X))
\end{align*}
\end{Proof}

\end{multicols}

\subsection{Probability Distributions}

\begin{multicols}{2}[\subsubsection{Discrete Distributions}][4cm]

  \paragraph{Diskrete Gleichverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{U}(\{y_1, ..., y_k\}),\: y \in \{y_1, ..., y_k\} \\
    & P(Y=y_i) =\frac{1}{k},\: i = 1, ...,k \\
    & \mathrm{E}(Y) = \frac{k+1}{2} ,\: \mathrm{Var}(Y) = \frac{k^2 - 1}{12}
  \end{align*}
  
    \paragraph{Binomialverteilung}
  Erfolge in unabhängigen Versuchen 

  \begin{align*}
    & Y \sim \mathrm{Bin}(n, \pi) \text{ mit } n \in \mathbb{N}, \pi \in \left[0,1\right] ,\: y \in \{0, ..., n\} \\
    & P(Y=y|\lambda) = \binom{n}{y}\pi^k(1-\pi)^{n-y} \\
    & \mathrm{E}(Y|\pi,n) = n\pi ,\: \mathrm{Var}(Y|\pi,n) = n\pi(1-\pi)
  \end{align*}

  \paragraph{Poissonverteilung}
  Zählmodelle für seltene Ereignisse

\noindent Immer nur ein Ereignis pro Zeitpunkt, Eintreten der Ereignisse ist unabhängig von bisheriger Geschichte, mittlere Anzahl der Ereignisse pro Zeit ist konstant und proportional zur Länge des betrachteten Zeitintervalls.  

  \begin{align*}
    & Y \sim \mathrm{Po}(\lambda) \text{ mit } \lambda \in \left[ 0, + \infty \right] ,\: y \in \mathbb{N}_0 \\
    & P(Y=y|\lambda) =\frac{\lambda^y exp^{-\lambda}}{y!} \\
    & \mathrm{E}(Y|p) = \lambda ,\: \mathrm{Var}(Y|p) = \lambda
  \end{align*}

\noindent Häufig wird die Varianz durchdas Poisson-Modell unterschätzt, es liegt Überdispersion vor.
  
\noindent  \textit{Approximation} der Binomialverteilung für kleine p

  
    \paragraph{Geometrische Verteilung}

  \begin{align*}
    & Y \sim \mathrm{Geom}(\pi) \text{ mit } \pi \in \left[0,1\right] ,\: y \in \mathbb{N}_0 \\
    & P(Y=y|\pi) = \pi(1-\pi)^{y-1} \\
    & \mathrm{E}(Y|\pi) = \frac{1}{\pi} ,\: \mathrm{Var}(Y|\pi) = \frac{1-\pi}{\pi^2}
  \end{align*}
  
    \paragraph{Negative Binomialverteilung}

  \begin{align*}
    & Y \sim \mathrm{NegBin}(\alpha, \beta) \text{ mit } \alpha, \beta \geq 0 ,\: y \in \mathbb{N}_0 \\
    & P(Y=y|\alpha, \beta) = \binom{\alpha + y - 1}{\alpha - 1} \left(\frac{\beta}{\beta - 1}\right)^{\alpha} \left(\frac{1}{\beta + 1}\right)^y \\
    & \mathrm{E}(Y|\alpha,\beta) = \frac{\alpha}{\beta} ,\: \mathrm{Var}(Y|\alpha,\beta) = \frac{\alpha}{\beta^2}(\beta+1)
  \end{align*}

\end{multicols}

\begin{multicols}{2}[\subsubsection{Continuous Distributions}][4cm]

	\paragraph{Stetige Gleichverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{U}(a,b) \text{ mit } \alpha, \beta \in \mathbb{R}, a \le b,\: y \in \left[a,b\right] \\
    & p(y|a,b) =\frac{1}{b-a} \\
    & \mathrm{E}(Y|a,b) = \frac{a+b}{2} ,\: \mathrm{Var}(Y|a,b) = \frac{(b-a)^2}{12}
  \end{align*}
  
    \paragraph{Univariate Normalverteilung} symmetrisch mit $\mu$ und $\sigma^2$
  
    \begin{align*}
    & Y \sim \mathrm{N}(\mu, \sigma^2) \text{ mit } \mu \in \mathbb{R}, \sigma^2 > 0,\: y \in \mathbb{R} \\
    & p(y|\mu, \sigma^2) =\frac{1}{\sqrt{2\pi\sigma^2}} \exp \left(-\frac{(y-\mu)^2}{2 \sigma^2} \right) \\
    & \mathrm{E}(Y|\mu, \sigma^2) = \mu ,\: \mathrm{Var}(Y|\mu, \sigma^2) = \sigma^2
  \end{align*}
  
    \paragraph{Multivariate Normalverteilung} symmetrisch mit $\mu$ und $\Sigma$
  
    \begin{align*}
    & Y \sim \mathrm{N}(\mu, \Sigma) \text{ mit } \mu \in \mathbb{R}^d, \Sigma \in \mathbb{R}^{d\times d} s.p.d.,\: y \in \mathbb{R}^d \\
    & p(y|\mu, \Sigma) = (2\pi)^{-\frac{d}{2}} \det (\Sigma)^{-\frac{1}{2}} \exp \left( -\frac{1}{2}(y-\mu)^{T} \Sigma^{-1}(y-\mu)\right) \\
    & \mathrm{E}(Y|\mu, \Sigma) = \mu ,\: \mathrm{Var}(Y|\mu, \Sigma) = \Sigma
  \end{align*}
  
    \paragraph{Log-Normalverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{LogN}(\mu, \sigma^2) \text{ mit } \mu \in \mathbb{R}, \sigma^2 > 0,\: y > 0 \\
    & p(y|\mu, \sigma^2) =\frac{1}{\sqrt{2\pi\sigma^2 y}} \exp \left(-\frac{(\log y-\mu)^2}{2 \sigma^2} \right) \\
    & \mathrm{E}(Y|\mu, \sigma^2) = \exp (\mu + \frac{\sigma^2}{2}) ,\\
    & \mathrm{Var}(Y|\mu, \sigma^2) = \exp (2\mu + \sigma^2)(\exp (\sigma^2) - 1)
  \end{align*}
  
\noindent Zusammenhang: $\log (Y) \sim \mathrm{N}(\mu, \sigma^2) \Rightarrow Y \sim \mathrm{LogN}(\mu, \sigma^2)$
  
    \paragraph{Nichtzentrale Studentverteilung} statistische Tests für $\mu$ mit unbekannter (geschätzter) Varianz und $\nu$ Freiheitsgraden
  
    \begin{align*}
    & Y \sim \mathrm{t}_\nu(\mu, \sigma) \text{ mit } \mu \in \mathbb{R}, \sigma^2, \nu > 0,\: y \in \mathbb{R}\\
    & p(y|\mu, \sigma^2, \nu) =\frac{\Gamma \left( \frac{\nu + 1}{2}\right) }{\Gamma (\frac{\nu}{2}) \Gamma (\sqrt{\nu\pi}\sigma)} \left(1+ \frac{(y-\mu)^2}{\nu \sigma^2} \right)^{-\frac{\nu + 1}{2}} \\
    & \mathrm{E}(Y|\mu, \sigma^2, \nu) = \mu \text{ f"ur }  \nu > 1,\\
    & \mathrm{Var}(Y|\mu, \sigma^2, \nu) = \sigma^2 \frac{\nu}{\nu-2} \text{ f"ur }  \nu > 2
  \end{align*}
  
 \noindent Zusammenhang: $Y|\theta \sim \mathrm{N}(\mu, \frac{\sigma^2}{\theta}), \: \theta \sim  \mathrm{Ga}(\frac{\nu}{2}, \frac{\nu}{2}) \Rightarrow Y \sim \mathrm{t}_\nu(\mu, \sigma)$ 
  
	\paragraph{Betaverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{Be}(a, b) \text{ mit } a,b > 0,\: y \in \left[0,1\right]\\
    & p(y|a, b) =\frac{\Gamma \left( a+b\right) }{\Gamma (a) \Gamma (b)} y^{a-1} (1-y)^{b-1} \\
    & \mathrm{E}(Y|a, b) = \frac{a}{a+b},\\
    & \mathrm{Var}(Y|a, b) = \frac{ab}{\left(a+b\right)^2(a+b+1)}, \\
    & \mathrm{mod}(Y|a, b) = \frac{a-1}{a+b-2} \text{ f"ur } a,b > 1
  \end{align*}
  
  
  	\paragraph{Gammaverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{Ga}(a, b) \text{ mit } a,b > 0,\: y > 0\\
    & p(y|a, b) = \frac{ b^a }{\Gamma (a)} y^{a-1} \exp (-by) \\
    & \mathrm{E}(Y|a, b) = \frac{a}{b},\\
    & \mathrm{Var}(Y|a, b) = \frac{a}{b^a}, \\
    & \mathrm{mod}(Y|a, b) = \frac{a-1}{b} \text{ f"ur } a \ge 1
  \end{align*}

  	\paragraph{Invers-Gammaverteilung}
  
    \begin{align*}
    & Y \sim \mathrm{IG}(a, b) \text{ mit } a,b > 0,\: y > 0\\
    & p(y|a, b) = \frac{ b^a }{\Gamma (a)} y^{-a-1} \exp (-\frac{b}{y}) \\
    & \mathrm{E}(Y|a, b) = \frac{b}{a-1} \text{ f"ur } a > 1,\\
    & \mathrm{Var}(Y|a, b) = \frac{b^2}{(a-1)^2(a-2)} \text{ f"ur } a \ge 2, \\
    & \mathrm{mod}(Y|a, b) = \frac{b}{a+1}
  \end{align*}
  
\noindent Zusammenhang: $Y^{-1} \sim \mathrm{Ga}(a, b) \Leftrightarrow Y \sim \mathrm{IG}(a, b)$
  
  
  	\paragraph{Exponentialverteilung} Zeit zwischen Poisson-Ereignissen
  
    \begin{align*}
    & Y \sim \mathrm{Exp}(\lambda) \text{ mit } \lambda > 0,\: y \geq 0\\
    & p(y|\lambda) = \lambda\exp (-\lambda y) \\
    & \mathrm{E}(Y|\lambda) = \frac{1}{\lambda}, \:
	\mathrm{Var}(Y|\lambda) = \frac{1}{\lambda^2}
  \end{align*}
  
  
  	\paragraph{Chi-Quadrat-Verteilung} quadrierte standardnormalverteilte Zufallsvariablen mit $\nu$ Freiheitsgraden
  
    \begin{align*}
    & Y \sim \chi^2(\nu) \text{ mit } \nu > 0,,\: y \in \mathbb{R}\\
    & p(y|\nu) = \frac{y^{\frac{\nu}{2}-1}e^{-\frac{y}{2}}}{2^{\frac{\nu}{2}}\Gamma \left(\frac{\nu}{2}\right)} \\
    & \mathrm{E}(Y|\nu) = \nu, \:
	\mathrm{Var}(Y|\nu) = 2\nu
  \end{align*}

\end{multicols}

\begin{multicols}{2}[\subsubsection{Exponential Family}][4cm]

  \paragraph{Definition} \ \\
  \noindent Zur Exponentialfamilie gehören alle Verteilungen, deren Dichte wie folgt geschrieben werden kann:
  
  $$f_Y(y,\theta) = \exp^{t^T(y)\theta - \kappa (\theta)}h(y)$$
  
  mit $h(y) \geq 0$, $t(y)$ Vektor der kanonischen Statistiken, $\theta$ Parametervektor und $\kappa (\theta)$ Normalisationskonstante.
  
  \paragraph{Normalisierungskonstante}
  
  \begin{align*}
  1 &= \int \exp^{t^T(y)\theta}h(y)dy \exp^{ - \kappa (\theta)} \\
  \Leftrightarrow \kappa (\theta) &= \log \int \exp^{t^T(y)\theta}h(y)dy
  \end{align*}
  
  \noindent $\kappa (\theta)$ ist die kumulanterzeugende Funktion, somit $\frac{\partial\kappa(\theta)}{\partial\theta} = \mathrm{E}(t(Y))$ und $\frac{\partial^2\kappa(\theta)}{\partial\theta^2} = \mathrm{Var}(t(Y))$
  
  
  
  \paragraph{Mitglieder}
  
  \begin{itemize}
  \item \textbf{Poissonverteilung}
  \item \textbf{Geometrische Verteilung}
  \item \textbf{Exponentialverteilung}
  \item \textbf{Normalverteilung}
   $t(y) = \left(-\frac{y^2}{2},y \right)^T$,  
   $\theta = \left(\frac{1}{\sigma^2}, \frac{\mu}{\sigma^2}\right)^T$,
   $h(y) = \frac{1}{\sqrt{2\pi}}$,
   $\kappa ( \theta ) = \frac{1}{2} \left( -\log \frac{1}{\sigma^2} + \frac{\mu^2}{\sigma^2} \right)$
  \item \textbf{Gammaverteilung}
  \item \textbf{Chi-Quadrat-Verteilung}
  \item \textbf{Betaverteilung}
  \end{itemize}
  


\end{multicols}

\begin{multicols}{2}[\subsection{Limit Theorems}][4cm]

  \paragraph{Gesetz der großen Zahlen}
  
  \paragraph{Zentraler Grenzwertsatz}
  
  $$Z_n \overset{d}{\longrightarrow} \mathrm{N}(0, \sigma^2)$$
  mit  $Z_n = \sum_{i=1}^{n} \frac{Y_i}{\sqrt{n}}$ und $Y_i$ i.i.d. mit $\mu=0$ und Varianz $\sigma^2$
\begin{Proof}
Für eine normalverteilte Zufallsvariable $Z \sim \mathrm{N}(\mu, \sigma^2)$ gilt $K_Z(t)=\mu t + \frac{1}{2}\sigma^2t^2$. Die ersten beiden Ableitungen $\frac{\partial^kK_Z(t)}{\partial t^k} \bigg|_{t = 0}$ entsprechen $\mu$ und $\sigma$. Alle anderen Momente sind null. 

\noindent Für $Z_n = (Y_1 + Y_2 + ... +Y_n)/\sqrt{n}$ gilt:
\begin{align*}
M_{Z_n}(t) &= \mathrm{E}\left(e^{t(Y_1 + Y_2 + ... +Y_n)/\sqrt{n}}\right)\\
&= \mathrm{E}\left(e^{tY_1/\sqrt{n}} \cdot e^{tY_2/\sqrt{n}}\cdot ... \cdot e^{tY_n/\sqrt{n}}\right) \\
&= \mathrm{E}\left(e^{tY_1/\sqrt{n}}\right) \mathrm{E}\left(e^{tY_2/\sqrt{n}}\right) ... \mathrm{E}\left(e^{tY_n/\sqrt{n}}\right) \\
&= M_Y^n(t/\sqrt{n})
\end{align*}
Analog gilt: $K_{Z_n}(t) = nK_Y(t/\sqrt{n})$.
\begin{align*}
 \frac{\partial K_{Z_n}(t)}{\partial t} \bigg|_{t = 0} &= \frac{n}{\sqrt{n}} \frac{\partial K_Y(t)}{\partial t} \bigg|_{t = 0} = \sqrt{n}\mu \\
 \frac{\partial^2K_{Z_n}(t)}{\partial t^2} \bigg|_{t = 0} &= \frac{n}{n} \frac{\partial^2 K_Y(t)}{\partial t^2} \bigg|_{t = 0} = \sigma^2
\end{align*}
Mithilfe der Taylorreihe können wir $K_{Z_n}(t) = 0 + \sqrt{n}\mu t + \frac{1}{2}\sigma^2t^2 + ...$ schreiben, wobei die Terme in $...$ alle für $n \rightarrow \infty$ gegen 0 gehen.

\noindent Damit gilt $K_{Z_n}(t) \overset{n\rightarrow\infty}{\longrightarrow} K_{Z}(t)$ mit $Z \sim \mathrm{N}(\sqrt{n}\mu,\sigma^2)$.
\end{Proof}

\end{multicols}


%-------------------------------------------------------------------------------

% SECTION: INFERENZ

%-------------------------------------------------------------------------------


\section{Inference}


\subsection{Method of Moments}

Die theoretischen Momente werden durch die empirischen geschätzt: \ \\
\vspace{0.5em}
$\mathrm{E}_{\hat{\theta}_{MM}}(Y^k) = m_k(y_1,...,y_n)$ \ \\
\vspace{0.5em}
\noindent Für die Exponentialfamilie gilt: $\hat{\theta}_{MM} = \hat{\theta}_{ML}$


\begin{multicols}{2}[\subsection{Loss Functions}][4cm]

\paragraph{Verlust}

$$\mathcal{L}: \mathcal{T} \times \Theta \rightarrow \mathbb{R}^+$$

\noindent mit Parameterraum $\Theta \subset \mathbb{R}$, $t\in \mathcal{T}$ mit $t:\mathbb{R}^n \rightarrow \mathbb{R}$ eine Statistik, die den Parameter $\theta$ schätzt. 
Es gilt: $\mathcal{L}(\theta, \theta) = 0$

\begin{itemize}
\item \textbf{absoluter Verlust (L1)}: $\mathcal{L}(t, \theta) = \left|t-\theta \right|$
\item \textbf{quadratischer Verlust (L2)}: $\mathcal{L}(t, \theta) = (t-\theta)^2$
\end{itemize}

\noindent Da $\theta$ unbekannt ist, ist der Verlust eine theoretische Größe. Zudem ist er die Realisation einer Zufallsvariable, da er von einer konkreten Stichprobe abhängt.

\paragraph{Risiko}

\begin{align*}
R(t(.), \theta) &= \mathrm{E}_\theta \left(\mathcal{L}(t(Y_1,...,Y_n),\theta)\right) \\
&= \int_{-\infty}^\infty \mathcal{L}(t(Y_1,...,Y_n),\theta) \prod_{i=1}^n f(y_i;\theta)dy_i
\end{align*}

\paragraph{Minimax-Regel} \ \\
\noindent Das Risiko beruht immer noch auf dem wahren Parameter $\theta$.
Vorsichtige Schätzung: Wähle $\theta$ so, dass das Risiko maximal wird, und danach $t(.)$ so, dass das Risiko minimiert wird:

$$\hat{\theta}_{minimax} = \underset{t(.)}{\text{arg min }} \left(\underset{\theta \in \Theta}{\text{max }} R(t(.);\theta)\right)$$

\noindent Es wird der Worst Case minimiert.

\paragraph{Mean Squared Error (MSE)}
\begin{align*}
MSE(t(.), \theta) &= \mathrm{E}_\theta\left(\{t(Y)-\theta\}^2\right) \\
&=\mathrm{Var}_\theta\left(t(Y_1,...,Y_n)\right) + Bias^2((t(.); \theta)
\end{align*}

\noindent mit $Bias(t(.);\theta) = \mathrm{E}_\theta\left(t(Y_1,...,Y_n)\right)-\theta$

\begin{Proof}
Sei $\mathcal{L}(t, \theta) = (t-\theta)^2$
\begin{align*}
R(t(.), \theta) = &\: \mathrm{E}_\theta (\{t(Y) -\theta\}^2) \\
= &\: \mathrm{E}_\theta (\{t(Y) - \mathrm{E}_\theta(t(Y)) + \mathrm{E}_\theta(t(Y))-\theta\}^2) \\
= &\: \mathrm{E}_\theta (\{t(Y) - \mathrm{E}_\theta(t(Y))\}^2) +
\mathrm{E}_\theta (\{\mathrm{E}_\theta(t(Y))-\theta\}^2) \\
& +  2\mathrm{E}_\theta (\{t(Y)-\mathrm{E}_\theta \left(t(Y))\}\{\mathrm{E}_\theta (t(Y)\right)-\theta\}) \\
= &\: \mathrm{Var}_\theta(t(Y_1,...,Y_n)) + Bias^2((t(.); \theta) + 0
\end{align*}
\end{Proof}

\textbf{Cramér-Rao-Ungleichung}

$$MSE(\hat{\theta}, \theta) \geq Bias^2(\hat{\theta},\theta) + 
\frac{\left(1+
\frac{\partial Bias(\hat{\theta}, \theta)}{partial \theta}
\right)^2}{I(\theta)}$$

\begin{Proof}
Für ungebiaste Schätzer: $ \theta = \mathrm{E}_\theta(\hat{\theta}) = \int t(y)f(y;\theta)dy$
\begin{align*}
1 &= \int t(y) \frac{\partial f(y;\theta)}{\partial \theta} dy \\
&= \int t(y) \frac{\partial \log f(y;\theta)}{\partial \theta} f(y;\theta) dy \\
&= \int t(y) s(y;\theta) f(y;\theta) dy \\
&= \int \left( t(y)-\theta\right)\left(s(\theta;y)-0\right)f(y;\theta)dy
& \overset{\text{1. Bartlett-Gleichung}}{\mathrm{E}_\theta \left(s(\theta;y)\right) = 0}\\
&= \mathrm{Cov}_\theta\left(t(Y);s(\theta;Y)\right) \\
&\geq \sqrt{\mathrm{Var}_\theta(t(Y))} \sqrt{\mathrm{Var}_\theta(s(\theta;Y))} & \text{Cauchy-Schwarz} \\
&= \sqrt{MSE(t(Y);\theta)} \sqrt{I(\theta)}
\end{align*}
\end{Proof}

\paragraph{Kullback-Leibler-Divergenz} Vergleich von Verteilungen

$$KL(t,\theta) = \int_{-\infty}^\infty \log\frac{f(\tilde{y};\theta)}{f(\tilde{y};t)} f(\tilde{y};\theta) d\tilde{y}$$

\noindent Die KL-Divergenz ist keine Distanz, da sie nicht symmetrisch ist.
\noindent Sie ist 0 für $t=\theta$ und größer/gleich 0 sonst.
\begin{Proof}
Folgt aus $\log (x) \leq x-1 \forall x \geq 0$, mit Gleichheit für $x=1$.
\end{Proof}

\noindent $R_{KL} (t(.), \theta)$ wird durch den MSE approximiert.
\begin{Proof}
\vspace{-1.5em}
\begin{align*}
& R_{KL} (t(.), \theta) = \\ =&\: \int_{-\infty}^\infty \mathcal{L}_{KL}(t(Y_1,...,Y_n),\theta) \prod_{i=1}^n f(y_i;\theta)dy_i \\
=&\: \int\int \log\frac{f(\tilde{y};\theta)}{f(\tilde{y};t)} f(\tilde{y};\theta) d\tilde{y} \prod_{i=1}^n f(y_i;\theta)dy_i \\
=&\: \int\int \left(\log f(\tilde{y};\theta) - \log f(\tilde{y};t) \right) f(\tilde{y};\theta) d\tilde{y} - \prod_{i=1}^n f(y_i;\theta)dy_i \\
\approx & \: - \int \underbrace{\left( \int \frac{\partial \log f(\tilde{y};\theta)}{\partial \theta} f(\tilde{y};\theta)d\tilde{y}\right)}_{0}\left(t-\theta\right) \prod_{i=1}^n f(y_i;\theta)dy_i  \\
+ &  \frac{1}{2} \int \underbrace{\left( - \int \frac{\partial^2 \log f(\tilde{y};\theta)}{\partial \theta^2} f(\tilde{y};\theta)d\tilde{y}\right)}_{I(\theta)}\left(t-\theta\right)^2 \prod_{i=1}^n f(y_i;\theta)dy_i 
\end{align*}
Wobei der letzte Schritt durch die Taylorreihe approximiert wurde:
$\log f(\tilde{y}, t) \approx \log f(\tilde{y},\theta) + \frac{\partial\log f(\tilde{y}, \theta)}{\partial\theta} (t-\theta) + \frac{1}{2} \frac{\partial^2\log f(\tilde{y}, \theta)}{\partial\theta^2} (t-\theta)^2$
\end{Proof}
\end{multicols}


\begin{multicols}{2}[\subsection{Maximum Likelihood (ML)}][4cm]
  \paragraph{Voraussetzungen}
  
  \begin{itemize}
  \item $Y_i \sim f(y;\theta)\:\: i.i.d.$
  \item $\theta \in \mathbb{R}^p$
  \item $f(.;\theta)$ Fisher-regulär:
  \begin{itemize}
  \item $\{ y: f (y; \theta > 0) \}$ unabhängig von $\theta$
  \item Möglicher Parameterraum $\Theta$ ist offen
  \item $f(y;\theta)$ zweimal differenzierbar
  \item $\int \frac{\partial}{\partial \theta} f(y;\theta)dy = \frac{\partial}{\partial \theta} \int f(y;\theta)dy$
  \end{itemize}
  \end{itemize}
  
  \paragraph{Zentrale Funktionen} \ \\
  \begin{itemize}
  \item \textbf{Likelihood} $L(\theta;y_1,...,y_n)$: $\prod_{i=1}^n f(y_i;\theta)$
  \item \textbf{log-Likelihood} $l(\theta;y_1,..y_n)$: 
$\log L(\theta;y_1,...,y_n) = \sum_{i=1}^n \log f(y_i;\theta)$
  \item \textbf{Score} $s(\theta;y_1,...,y_n)$: $\frac{\partial l(\theta;y_1,..y_n)}{\partial \theta}$
  \item \textbf{Fisher-Information} $I(\theta)$: $-\mathrm{E}_\theta\left(\frac{\partial s(\theta;Y)}{\partial\theta}\right)$
  \item \textbf{beobachtete Fisher-Information} $I_{obs}(\theta)$: $-\mathrm{E}_\theta\left(\frac{\partial s(\theta;y)}{\partial\theta}\right)$
  \end{itemize}
  
  
  \paragraph{Eigenschaften der Score-Funktion} \ \\
  
  erste Bartlett-Gleichung:
  
  $$\mathrm{E}\left(s(\theta;Y)\right) = 0$$
  
\begin{Proof}
\vspace{-1.5em}
\begin{align*}
1 &= \int f(y;\theta) dy \\
0 = \frac{\partial 1}{\partial\theta} &= \int \frac{\partial f(y;\theta)}{\partial \theta}dy = \int \frac{\partial f(y;\theta) / \partial\theta}{f(y;\theta)} f(y;\theta) dy \\ &= \int \frac{\partial}{\partial\theta} \log f(y;\theta) f(y;\theta) dy = \int s(\theta;y) f(y;\theta) dy
\end{align*}
\end{Proof}
  
  zweite Bartlett-Gleichung:
  
  $$\mathrm{Var}_\theta\left(s(Y;\theta)\right) = \mathrm{E}_\theta\left(-\frac{\partial^2 log f(Y;\theta)}{\partial\theta^2}\right) = I(\theta)$$
  
\begin{Proof}
\vspace{-1.5em}
\begin{align*}
0 = \frac{\partial 0}{\partial \theta} =& \frac{\partial}{\partial\theta}\int \frac{\partial}{\partial\theta} \log f(y;\theta) f(y;\theta) dy \hspace{2em}\text{      siehe oben}\\
=& \int \left( \frac{\partial^2}{\partial \theta^2} \log f(y;\theta)\right) f(y;\theta) dy  \\
&+ \int\frac{\partial \log f(y;\theta)}{\partial \theta}\frac{\partial f(y;\theta)}{\partial \theta}dy \\
=& \: \mathrm{E}_\theta \left( \frac{\partial^2}{\partial \theta^2} \log f(Y;\theta)\right)  \\
&+ \int\frac{\partial \log f(y;\theta)}{\partial \theta}\frac{\partial \log f(y;\theta)}{\partial \theta} f(y;\theta) dy 
\end{align*}

$$\Leftrightarrow \mathrm{E}_\theta \left(s(\theta;Y) s(\theta;Y)\right) = \mathrm{E}_\theta \left(- \frac{\partial^2}{\partial \theta^2} \log f(Y;\theta)\right)$$
\noindent Bartletts zweite Gleichung gilt dann, weil $\mathrm{E}\left(s(\theta;Y)\right) = 0$
\end{Proof}
  
  \paragraph{ML-Schätzer}
   $$\hat{\theta}_{ML} = \text{arg max } l(\theta; y_1,...y_n)$$
   
\noindent für Fisher-reguläre Verteilungen: $\hat{\theta}_{ML}$ hat asymptotisch die kleinstmögliche Varianz, gegeben durch die Cramér-Rao-Ungleichung, 
   $s\left(\hat{\theta}_{ML};y_1,...,y_n\right) = 0$
   
$\hat{\theta} \overset{a}{\sim} \mathrm{N}\left(\theta, I^{-1}(\theta)\right)$
   
\noindent Der ML-Schätzer ist invariant: $\hat{\gamma} = g(\hat{\theta})$ wenn $\gamma = g(\theta)$. 
   
\begin{Proof}
$\gamma = g(\theta)\, \Leftrightarrow \,\theta = g^{-1}(\gamma)$

\noindent Für die Loglikelihood von $\gamma$ an der Stelle $\hat{\theta}$ gilt:

$$\frac{\partial l(g^{-1}(\hat{\gamma}))}{\partial \gamma} = \frac{\partial g^{-1}(\gamma)}{\partial\gamma} \underbrace{\frac{\partial l(\hat{\theta})}{\partial \theta}}_{=0} = 0$$
\end{Proof}   

\noindent Die Fisher-Information ist dann $\frac{\partial\theta}{\partial\gamma} I(\theta) \frac{\partial\theta}{\partial\gamma}$

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
I_{\gamma}(\gamma) &= 
-\mathrm{E}\left(\frac{\partial^2 l(g^{-1}(\hat{\gamma}))}{\partial \gamma^2} \right)= 
-\mathrm{E}\left(\frac{\partial}{\partial\gamma} \left( \frac{\partial g^{-1}(\gamma)}{\partial\gamma} \frac{\partial l(\theta)}{\partial\theta} \right)\right) \\
&= -\mathrm{E}\left(\underbrace{\frac{\partial^2 g^{-1}(\gamma)}{\partial\gamma}\frac{\partial l(\theta)}{\partial\theta}}_{\text{Erwartungswert 0}} + \frac{\partial g^{-1}(\gamma)}{\partial\gamma}\frac{\partial^2l(\theta)}{\partial\theta^2}\frac{\partial g^{-1}(\gamma)}{\partial\gamma}\right) \\
&= \frac{\partial g^{-1}(\gamma)}{\partial\gamma} I(\theta) \frac{\partial g^{-1}(\gamma)}{\partial\gamma} = \frac{\partial \theta}{\partial\gamma} I(\theta) \frac{\partial \theta}{\partial\gamma}
\end{align*}
\end{Proof}

\noindent Delta-Regel: $\gamma \overset{a}{\sim} \mathrm{N}(\hat{\gamma}, \frac{\partial \theta}{\partial\gamma} I^{-1}(\theta) \frac{\partial \theta}{\partial\gamma} $

\paragraph{Numerical computation of the ML estimate}
Fisher- Scoring as statistical version of the Newton-Raphson procedure

\begin{enumerate}
\item Initialize $\theta_{(0)}$
\item Repeat: $\theta_{(t+1)} := \theta_{(t)} + I^{-1}(\theta_{(t)})s(\theta_{(t)};y)$ \label{repeat}
\item Stop if $\Vert \theta_{(t+1)} -\theta_{(t)} \Vert < \tau$; return $\hat{\theta}_{ML}=\theta_{(t+1)}$
\end{enumerate}

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
&0 = s(\hat{\theta}_{ML};y) \overset{Taylor}{\underset{Series}{\approx}} s(\theta;y) + \frac{\partial s(\theta;y)}{\partial \theta} (\hat{\theta}_{ML} - \theta) \Leftrightarrow \\
&\hat{\theta}_{ML} \approx \theta - \left(\frac{\partial s(\theta;y)}{\partial \theta}\right)^{-1} s(\theta;y) \approx \theta - I^{-1}(\theta)s(\theta;y)
\end{align*}
As $\frac{\partial s(\theta;y)}{\partial \theta}$ is often complicated, its expectation $I(\theta)$ is used.
\end{Proof}

\noindent The second part in \ref{repeat} can be weighted with a step size $\delta$ or $\delta(t)$ $\in (0,1)$, e.\,g.\ to ensure convergence.

\noindent If $I(\theta)$ can't be analytically derived, simulation from $f(y;\theta_{(t)})$ can be used. For the exponential family, step \ref{repeat} then changes to $\theta_{(t+1)} := \theta_{(t)} + \hat{\mathrm{Var}}_{\theta_{(t)}}(t(Y))^{-1} \mathrm{E}_{\theta_{(t)}}(t(Y))$ as the ML estimate is the expectation.

\paragraph{Log Likelihood Ratio}

$$lr(\theta,\hat{\theta}) := l(\hat{\theta}) - l(\theta) = \log \frac{L(\hat{\theta})}{L(\theta)}$$

with $2\cdot lr(\theta,\hat{\theta}) \overset{a}{\sim} \chi^2_1$

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
l(\theta) & \overset{Taylor}{\underset{Series}{\approx}} l(\hat{\theta}) + \underbrace{\frac{\partial l(\hat{\theta})}{\partial \theta}}_{=0} (\theta - \hat{\theta}) + \frac{1}{2}\underbrace{\frac{\partial^2 l(\hat{\theta})}{\partial \theta^2}}_{\approx I^{-1}(\theta)s(\theta;Y)}(\underbrace{\theta - \hat{\theta}}_{\approx -I(\theta)})^2\\
&\approx l(\hat{\theta}) - \frac{1}{2} \frac{s^2(\theta, Y)}{I(\theta)}
\end{align*}
$s(\theta,Y)$ is asymptotically normal.
\end{Proof}

If $\theta \in \mathbb{R}^p$ the corresponding distribution is $\chi^2_p$.
  

\end{multicols}


\begin{multicols}{2}[\subsection{Sufficiency und Consistency}][4cm]

\paragraph{Suffizienz} \ \\
\noindent Eine Statistik $t(y_1,...,y_n)$ ist suffizient für $\theta$, wenn die bedingte Verteilung $f(y_1,...,y_n|t_0 = t(y_1,...,y_n);\theta)$ unabhängig von $\theta$ ist. \vspace{0.5em}

\textbf{Neyman-Kriterium:}
$$t(Y_1,...,Y_n) \text{ suffizient } \Leftrightarrow f(y;\theta) = h(y)g\left(t(y);\theta\right)$$
\begin{Proof}
``$\Rightarrow$'':
$$f(y;\theta) = \underbrace{f(y|t = t(y);\theta)}_{h(y)} \underbrace{f_t(t|y;\theta)}_{g(t(y);\theta)}$$

\noindent ``$\Leftarrow$'':
$$f_t(t;\theta) = \int_{t=t(y)} f(y;\theta)dy = \int_{t=t(y)} h(y) g(t;\theta)dy$$
\indent Damit:
$$f\left(y|t=t(y);\theta\right) = \frac{f(y,t=t(y);\theta)}{f_t(t,\theta)}
= \begin{cases}
\frac{h(y)g(t;\theta)}{g(t;\theta)} & t=t(y) \\
0 & \, \text{sonst}
\end{cases}$$
\end{Proof}

\textbf{Minimalsuffizienz:} \ \\
$t(.)$ ist suffizient und $\forall\: \tilde{t}(.)\: \exists\: h(.) \text{ s.t. } t(y) = h(\tilde{t}(y))$

\paragraph{(schwache) Konsistenz}
$$MSE(\hat{\theta},\theta) \overset{n\rightarrow\infty}{\longrightarrow} 0 \Rightarrow \hat{\theta} \text{ konsistent}$$

\end{multicols}


\begin{multicols}{2}[\subsection{Confidence Intervals}][4cm]

\paragraph{Definition}
$$\begin{gathered}
\left[t_l(Y),t_r(Y)\right] \text{ Konfidenzintervall } \\
\Leftrightarrow \\
P_\theta\left((t_l(Y) \leq \theta \leq t_r(Y)\right) \geq 1-\alpha
\end{gathered}$$


\noindent mit $1-\alpha$ Konfidenzlevel und $\alpha$ Signifikanzlevel

\paragraph{Pivotale Statistik}

$$\begin{gathered}
g(Y;\theta) \text{ pivotal}\\
\Leftrightarrow \\
\text{Verteilung von } g(Y;\theta) \text{ unabhängig von } \theta
\end{gathered}$$

\textbf{Approximativ pivotale Statistik}

$$g(\hat{\theta};\theta) = \frac{\hat{\theta} - \theta}{\sqrt{\textrm{Var}(\hat{\theta})}} \overset{\alpha}{\sim} \mathrm{N}(0,1)$$

mit $\hat{\theta} = t(Y) \overset{\alpha}{\sim} \mathrm{N}(\theta,\mathrm{Var}(\hat{\theta})$

$$KI = \left[ \hat{\theta} - z_{1-\frac{\alpha}{2}}\sqrt{\mathrm{Var}(\hat{\theta})}, \hat{\theta} + z_{1-\frac{\alpha}{2}}\sqrt{\mathrm{Var}(\hat{\theta})} \right]$$

\begin{Proof}
$1-\alpha \approx P \left( z_{\frac{\alpha}{2}} \leq \frac{\hat{\theta} - \theta}{\sqrt{\textrm{Var}(\hat{\theta})}} \leq z_{1-\frac{\alpha}{2}}\right)$
\end{Proof}

\paragraph{Exakte binomiale Konfidenzintervalle}

\end{multicols}

  
%-------------------------------------------------------------------------------

% SECTION: HYPOTHESENTESTS

%-------------------------------------------------------------------------------

\section{Hypothesis Testing}

\subsection{Tests for One Sample}

\begin{multicols}{2}[\subsubsection{Normal Distribution}][4cm]

  \paragraph{ $\mu$ gesucht, $\sigma^2 $ bekannt (Einfacher Gauß-Test)}
  


\end{multicols}


%-------------------------------------------------------------------------------

% SECTION: REGRESSION

%-------------------------------------------------------------------------------

\section{Regression}

\subsection{Assumptions}

\subsection{Procedure}
\begin{multicols}{2}[\subsubsection{Ordinary Least Squares (OLS)}][4cm]

\paragraph{KQ-Schätzer (Einfachregression)}

$$\hat{\beta}_1=\frac{Cov(x,y)}{Var(x)}=\frac{S_{xy}}{S_{xx}}= \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}} \cdot \sqrt{\frac{S_{yy}}{S_{xx}}}=r\sqrt{\frac{S_{yy}}{S_{xx}}}$$

\begin{Proof}
$Cov(x,y)=Cov(x,\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x})=\hat{\beta}_1Var(x)$

\raggedleft
$ \iff \hat{\beta}_1= \frac{Cov(x,y)}{Var(x)}$
\end{Proof}

$$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$

\begin{Proof}
$E\left[y\right] = E\left[\hat{\beta}_0+\hat{\beta}_1 x+\hat{e}\right] \iff \hat{\beta}_0 = E\left[y\right] - \hat{\beta}_1E\left[x\right]$
\end{Proof}

\end{multicols}

\subsection{Model}

\begin{multicols}{2}[\subsubsection{Simple Linear Regression}][4cm]

\paragraph{Theoretisches Modell}

$$y_i=\beta_0+\beta_1x_i+u_i$$

\paragraph{Empirisches Modell}

$$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_i+e_i$$

\paragraph{Eigenschaften der Regressionsgeraden}
\begin{equation*}
\begin{split}
\hat{y}_i & = \hat{\beta}_0+\hat{\beta}_1x_i  =\bar{y}+ \hat{\beta}_1(x_i-\bar{x}) \\
\hat{e}_i  & =  y_i-\hat{y}_i = y_i-(\hat{\beta}_0+\hat{\beta}_1x_i) \\
 & =y_i-(\bar{y}+ \hat{\beta}_1(x_i-\bar{x})) \\
\sum\limits_{i=1}^n\hat{e}_i & = \sum\limits_{i=1}^ny_i-\sum\limits_{i=1}^n\bar{y}-\hat{\beta}_1\sum\limits_{i=1}^n(x_i-\bar{x}) \\
 & = n\bar{y}-n\bar{y}-\hat{\beta}_1(n\bar{x}-n\bar{x})=0 \\
\bar{\hat{y}} & = \frac{1}{n}\sum\limits_{i=1}^n\hat{y}_i=\frac{1}{n}(n\bar{y}+\hat{\beta}_1(n\bar{x} - n\bar{x})) = \bar{y}
\end{split}
\end{equation*}



\end{multicols}

\subsubsection{Multivariate Linear Regression}

%\subsubsection{Spezialfall: Zeitreihen}

\begin{multicols}{2}[\subsection{Analysis of Variances (ANOVA)}][4cm]

$$SS_{Total}=SS_{Explained}+SS_{Residual}$$

mit
\begin{align*}
SS_{Total}  &=  \sum\limits_{i=1}^n(y_i-\bar{y})^2 \\
SS_{Explained} &= \sum\limits_{i=1}^n(\hat{y}_i-\bar{y})^2 \\
SS_{Residual} &= \sum\limits_{i=1}^n(y_i-\hat{y}_i)^2=\sum\limits_{i=1}^n e_i^2=S_{yy}-\hat{\beta}^2S_{xx} 
\end{align*}

\end{multicols}

\subsection{Goodness of Fit}

\begin{multicols}{2}[\subsubsection{Bestimmtheitsmaß}][4cm]

$$R^2=\frac{SS_{Explained}}{SS_{Total}}=1-\frac{SS_{Residual}}{SS_{Total}}=r^2$$

Range: $0 \le R^2 \le 1$

\end{multicols}

%-------------------------------------------------------------------------------

% SECTION: KLASSIFIKATION

%-------------------------------------------------------------------------------

\section{Classification}

\subsection{Diskriminant Analysis (Bayes)}


%-------------------------------------------------------------------------------

% SECTION: CLUSTERANALYSE

%-------------------------------------------------------------------------------

\section{Cluster Analysis}

%-------------------------------------------------------------------------------

% SECTION: BAYESSCHE STATISTIK

%-------------------------------------------------------------------------------

\section{Bayesian Statistics}

\begin{multicols}{2}[\subsection{Basics}][4cm] 

\paragraph{Bayes-Formel}
$$P(A|B)=\frac{P(B|A)P(A)}{P(B)} \hspace{0.5cm} \text{für } P(A), P(B) > 0$$

\begin{center}oder allgemeiner:\end{center}

\vspace{-1 em}

\begin{align*}
  f(\theta | X) &= \frac{f(X | \theta ) \cdot f(\theta)}{\int f(X|\tilde{\theta}) f(\tilde{\theta})  d \tilde{\theta}}\\
  &= C \cdot f(X | \theta ) \cdot f(\theta) \hspace{1 em} \text{wähle C so, dass $\int f(\theta | X)=1$} \\
  &\propto f(X | \theta ) \cdot f(\theta)
\end{align*}

\paragraph{Punktschätzer}

\paragraph{Kredibilitätsintervall}

\paragraph{Sensitivitätsanalyse}

\paragraph{Prädiktive Posteriori}

$$f(x_Z|\mathbf{x}) =\int f(x_Z, \lambda|\mathbf{x})d\lambda = \int f(x_Z|\lambda)p(\lambda|\mathbf{x})$$

\paragraph{Uninformative Priori} ~\\

$f(\theta)=const. \text{ für } \theta > 0$
,  damit:
 $f(\theta | X) = C \cdot f(X | \theta )$

\noindent (Da $\int f(\theta) =1$ so nicht möglich, ist das eigentlich keine Dichte)

\paragraph{Konjugierte Priori}

\begin{Extensiv}
~\\
  \noindent Wenn die Priori- und die Posteriori-Verteilung denselben Typ hat für eine gegebene Likelihoodfunktion, so nennt man sie konjugiert.
  
\end{Extensiv}

\vspace{1 em}
\noindent Binomial-Beta-Modell: \begin{itemize}
  \vspace{-0.7 em}
\setlength\itemsep{-0.7 em}
\item Priori $ \sim Be(\alpha,\beta)$
\item $X$ $ \sim Binom(n,p,k)$
\item Posteriori $\sim Be(\alpha + k, \beta + n - k)$
  
\end{itemize}

\end{multicols}


\begin{multicols}{2}[\subsection{Markov Chain / Monte Carlo}][4cm]



\end{multicols}

\end{document}
