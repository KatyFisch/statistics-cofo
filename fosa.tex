% To Do: Links between things that are connected
%        
%      
\documentclass[8pt]{extarticle}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage[a4paper,left=2.3cm,right=1.2cm,top=2cm,bottom=2cm]{geometry} 
\usepackage{blindtext}
\usepackage{setspace}
\usepackage{float}
\usepackage{titletoc}
\usepackage{titlesec}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{amsmath} 
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{verbatimbox}
\usepackage{enumitem}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{bigints}
\onehalfspacing
\usepackage[hidelinks]{hyperref}
\usepackage[all]{nowidow} %funktioniert nicht....
\clubpenalty=9996
\widowpenalty=9999
\brokenpenalty=4991
\predisplaypenalty=10000
\postdisplaypenalty=1549
\displaywidowpenalty=1602

\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

\allowdisplaybreaks

\setlength\parindent{0pt}

\newcommand{\zerodisplayskips}{%
  \setlength{\abovedisplayskip}{2pt}%
  \setlength{\belowdisplayskip}{2pt}%
  \setlength{\abovedisplayshortskip}{2pt}%
  \setlength{\belowdisplayshortskip}{2pt}}
\appto{\normalsize}{\zerodisplayskips}
\appto{\small}{\zerodisplayskips}
\appto{\footnotesize}{\zerodisplayskips}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}



%Hier sind die unterschiedlichen Ausführlichkeitsgrade definiert
\includecomment{Extensiv} 
\includecomment{Proof} 
\includecomment{Annahmen}
\includecomment{Mathspez}
\includecomment{Mathfolg}
\includecomment{Rechreg}
\mdfdefinestyle{MyFrame}{%
    linecolor=black!20!,
    outerlinewidth=0.2pt,
    roundcorner=5pt,
    innertopmargin=0.5\baselineskip,
    innerbottommargin=0.5\baselineskip,
    innerrightmargin=10pt,
    innerleftmargin=10pt,
    backgroundcolor=white}
\specialcomment{Proof}{\begin{mdframed}[style=MyFrame,nobreak=true] Proof: \ \\}{\end{mdframed}}
\specialcomment{Rechreg}{\noindent \textit{Calculation Rules:} \begin{itemize}[nosep,label=$\star$] }{\end{itemize}}
\renewcommand\ThisComment[1]{% Fix for Umlauts in comments
  \immediate\write\CommentStream{\unexpanded{#1}}%
}

% Hier die Ausführlichkeit bestimmen:
%\excludecomment{Extensiv} 
%\excludecomment{Proof} 
%\excludecomment{Annahmen}
%\excludecomment{Mathspez}
%\excludecomment{Mathfolg}

% Inhaltsverzeichnis mit zwei Spalten
\usepackage[toc]{multitoc}
\renewcommand*{\multicolumntoc}{2}




%Überschriftengrößen anpassen, so dass Paragraph kleiner ist als Subsubsection
\titleformat{\section}
  {\normalfont\fontsize{16}{15}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\fontsize{14}{15}\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\fontsize{12}{15}\bfseries}{\thesubsubsection}{1em}{}


\begin{document}

\topskip0pt
\vspace*{18em}

\hrule
\begin{center}
{\fontsize{30}{60}\selectfont \textbf{Statistics}} \\ \

{\fontsize{20}{60}\selectfont Collection of Formulas}
\end{center}
\hrule

\tableofcontents




% weitere Anpassungen im Hauptteil des Dokuments
\raggedright %linksbündig
\setlength{\parindent}{15pt} %Einzuglänge festsetzen
\setlength{\columnseprule}{0.3pt} %Liniendicke zwischen zwei Multicols





%-------------------------------------------------------------------------------

% SECTION: DESKRIPTIVE STATISTIK

%-------------------------------------------------------------------------------

\section{Descriptive Statistics}


\subsection{Summary Statistics}

\begin{multicols}{2}[\subsubsection{Location}][4cm] 

\paragraph{Mode}

 Most frequent value of $x_i$. Two or more modes are possible (bimodal).

\paragraph{Median}

$$\tilde{x}_{0.5}=\begin{cases} x_{((n+1)/2)} & \text{falls }n\text{ ungerade} \\ \frac{1}{2}(x_{(n/2)}+x_{(n/2+1)} & \text{falls }n\text{ gerade} \end{cases}$$

\paragraph{Quantile}

$$\tilde{x}_\alpha=\begin{cases} x_{(k)} & \text{falls } n\alpha \notin \mathbb{N}\\ \frac{1}{2}(x_{(n\alpha)}+ x_{(n\alpha+1)}) & \text{falls } n\alpha \text{ ganzzahlig} \end{cases}$$

with

\begin{tabular}{l l}
 $k=\min x$ $\in \mathbb{N}$, &  $x$ $>$ $n\alpha$ \\
\end{tabular}

\paragraph{Minimum/Maximum}


$$x_{\min}=\min_{i \in \{ 1,...,N\}} (x_i) \hspace{0.8cm}   x_{\max}=\max_{i \in \{ 1,...,N\}} (x_i)$$
 


\paragraph{Arithmetic Mean}

 $$\bar{x}=\frac{1}{n}\sum\limits_{i=1}^n x_i$$

\noindent Estimates the expectation
$\mu = E[X]$ (first~moment).

\begin{Rechreg}
\item $E(a+b\cdot X)=a+b\cdot E(X)$
\item $E(X\pm Y)=E(X)\pm E(Y)$
\end{Rechreg}

\paragraph{Geometric Mean}

$$\bar{x}_G=\sqrt[n]{\sum\limits_{i=1}^n x_i} $$

\noindent For growth factors: $\bar{x}_G=\sqrt[n]{\frac{B_n}{B_0}}$

\paragraph{Harmonic Mean}

$$\bar{x}_H=\frac{\sum\limits_{i=1}^n w_i}{\sum\limits_{i=1}^n \frac{w_i}{x_i}}$$


\end{multicols}


\begin{multicols}{2}[\subsubsection{Dispersion}][4cm] 

\paragraph{Range}

$$R=x_{(n)}-x_{(1)}$$

\paragraph{Interquartile Range}

$$d_Q=\tilde{x}_{0.75}-\tilde{x}_{0.25}$$

\paragraph{(Empirical) Variance}

$$s^2=\frac{1}{n}\sum\limits_{i=1}^n(x_i-\bar{x})^2=\frac{1}{n}\sum\limits_{i=1}^nx_i^2-\bar{x}^2$$

\noindent Estimates the second centralized moment.

\begin{Rechreg}
\item $Var(aX+b)=a^2\cdot Var(X)$
\item $Var(X\pm Y)= Var(X)+Var(Y) + 2Cov(X,Y)$
\end{Rechreg}

\paragraph{(Empirical) Standard Deviation}

$$s=\sqrt{s^2}$$

\paragraph{Coefficient of Variation}

$$ \nu=\frac{s}{\bar{x}}$$

\paragraph{Average Absolute Deviation}


$$ \mathit{e} = \frac{1}{n}\sum_{i=1}^n \left|x_i - \bar{x}\right|$$

\noindent Estimates the first absolute centralized moment.

\end{multicols}



\begin{multicols}{2}[\subsubsection{Concentration}][4cm] 

\paragraph{Gini Coefficient}

\begin{equation*} 
\begin{split}
G & = \frac{2\sum\limits_{i=1}^n ix_{(i)}-(n+1)\sum\limits_{i=1}^n x_{(i)}}{n\sum\limits_{i=1}^n x_{(i)}}  = 1-\frac{1}{n}\sum\limits_{i=1}^n(v_{i-1}+v_i)
\end{split}
\end{equation*}

with

$$  u_i=\frac{i}{n}, \hspace{0.3cm} v_i= \frac{\sum\limits_{j=1}^i x_{(j)}}{\sum\limits_{j=1}^i x_{(j)}} \hspace{0.7cm} (u_0=0, \hspace{0.2cm} v_0=0 )$$


\noindent These are also the values for the Lorenz curve.

\ \\

\indent Range: $ 0 \le G \le \frac{n-1}{n}$




\paragraph{Lorenz-Münzner Coefficient (normed $G$)}

$$G^+=\frac{n}{n-1}G$$

\indent Range: $ 0 \le G^+ \le 1$






\end{multicols}




\begin{multicols}{2}[\subsubsection{Shape}][4cm] 

\paragraph{(Empirical) Skewness}
$$\nu = \frac{n}{(n-1)(n-2)} \sum_{i=1}^n \left(\frac{x_i-\bar{x}}{s}\right)^3$$

\noindent Estimates the third centralized moment, scaled with $(\sigma^2)^{\frac{2}{3}}$

\paragraph{(Empirical) Kurtosis}

$$k=\left[n(n+1) \cdot \sum_{i=1}^n \left(\frac{x_i-\bar{x}}{s}\right)^4 - 3(n-1)\right] \cdot \frac{n-1}{(n-2)(n-3)}+3$$

\noindent Estimates the fourth centralized moment, scaled with $(\sigma^2)^2$

\paragraph{Excess}

$$\gamma=k-3$$

\end{multicols}



\begin{multicols}{2}[\subsubsection{Dependence}][4cm]

\subsubsection*{\textit{for two nominal variables}}

\paragraph{$\chi^2$-Statistic}

\begin{equation*}
\begin{split}
\chi^2 & =\sum\limits_{i=1}^k \sum\limits_{j=1}^l \frac{(n_{ij}-\frac{n_{i+}n_{+j}}{n})^2}{\frac{n_{i+}n_{+j}}{n}}  =n\left(\sum\limits_{i=1}^k \sum\limits_{j=1}^l \frac{n_{ij}^2}{n_{i+}n_{+j}}-1\right)
\end{split}
\end{equation*}

Range: $ 0 \le \chi^2 \le n(\min(k,l)-1)$

\paragraph{Phi-Coefficient}

$$\Phi=\sqrt{\frac{\chi^2}{n}}$$

Range: $ 0 \le \Phi \le \sqrt{\min(k,l)-1}$

\paragraph{Cram\'er's $V$}

$$ V= \sqrt{\frac{\chi^2}{\min(k,l)-1}}$$

Range: $ 0 \le V \le 1$

\paragraph{Contingency Coefficient $C$}

$$C=\sqrt{\frac{\chi^2}{\chi^2 + n}}$$

Range: $ 0 \le C \le \sqrt{\frac{\min(k,l)-1}{\min(k,l)}} $

\paragraph{Corrected Contingency Coefficient $C_{corr}$}

$$C_{corr}= \sqrt{\frac{\min(k,l)}{\min(k,l)-1}} \cdot \sqrt{\frac{\chi^2}{\chi^2 + n}} $$

Range $ 0 \le C_{corr} \le 1 $

\paragraph{Odds-Ratio}

$$OR=\frac{ad}{bc} = \frac{n_{ii}n_{jj}}{n_{ij}n_{ji}}$$

Range: $0 \le OR < \infty$

\subsubsection*{\textit{for two ordinal variables}}

\paragraph{Gamma (Goodman and Kruskal)}

$$\gamma=\frac{K-D}{K+D}$$


\begin{tabular}{l l }
$ K=\sum_{i<m}\sum_{j<n}n_{ij}n_{mn}$ & Number of concordant pairs \\
$ D=\sum_{i<m}\sum_{j>n}n_{ij}n_{mn}$ & Number of reversed pairs \\
\end{tabular}

\ \\

Range: $-1 \le \gamma \le 1$

\paragraph{Kendall's $\tau_b$}

$$ \tau_b=\frac{K-D}{\sqrt{(K+D+T_X)(K+D+T_Y)}}$$

with

\begin{tabular}{l l } 
$ T_X=\sum_{i=m}\sum_{j<n}n_{ij}n_{mn}$ & Number of ties w.r.t. $X$ \\
$ T_Y=\sum_{i<m}\sum_{j=n}n_{ij}n_{mn}$ & Number of ties w.r.t. $Y$ \\
\end{tabular}

\ \\

Range: $-1 \le \tau_b \le 1$

\paragraph{Kendall's/Stuart's $\tau_c$}

$$\tau_c=\frac{2\min(k,l)(K-D)}{n^2(\min(k,l)-1)}$$

Range: $-1 \le \tau_c \le 1$

\paragraph{Spearman's Rank Correlation Coefficient}

$$\rho=\frac{n(n^2-1)-\frac{1}{2}\sum\limits_{j=1}^J b_j(b_j^2-1)-\frac{1}{2}\sum\limits_{k=1}^K c_k(c_k^2-1)-6\sum\limits_{i=1}^n d_i^2}{\sqrt{n(n^2-1)-\sum\limits_{j=1}^J b_j(b_j^2-1)}\sqrt{n(n^2-1)-\sum\limits_{k=1}^Kc_k(c_k^2-1)}}$$

or

$$\rho=\frac{s_{rg_xrg_y}}{\sqrt{s_{rg_xrg_x}s_{rg_yrg_y}}}$$

 Without ties:

$$\rho=1-\frac{6\sum\limits_{i=1}^nd_i^2}{n(n^2-1)}$$

with

\begin{tabular}{l l } 
 $d_i=R(x_i)-R(y_i)$ & rank difference \\ 
\end{tabular}

\ \\

Range: $-1 \le \rho \le 1$

\subsubsection*{\textit{for two metric variables}}

\paragraph{Correlation Coefficient (Bravais-Pearson)}

$$r=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}=\frac{s_{xy}}{\sqrt{s_{xx}s_{yy}}}$$

with

\begin{tabular}{l l } 
$S_{xy}=\sum\limits_{i=1}^n(x_i-\bar{x})^2(y_i-\bar{y})^2$ & or $s_{xy}=\frac{S_{xy}}{n}$ \\
$S_{xx}=\sum\limits_{i=1}^n(x_i-\bar{x})^2$ & or $s_{xx}=\frac{S_{xx}}{n}$ \\ 
$S_{yy}=\sum\limits_{i=1}^n(y_i-\bar{y})^2$ & or $s_{yy}=\frac{S_{yy}}{n}$ \\
\end{tabular}

\ \\

Range: $-1 \le r \le 1$


%\subsubsection*{\textit{Für zwei unterschiedliche Variablen}}


\end{multicols}

%\subsection{Tables}

\subsection{Diagrams}

\begin{multicols}{2}[\subsubsection{Histogram}][4cm]



\begin{tikzpicture}

\draw[-latex] (0.5,0) -- (0.5,3) node[below left]{Frequency};
\draw[-latex] (0.5,0) -- (5,0) node[below]{Variable};

\draw[fill=black!50!] (1,0) rectangle (2,1.5);
\draw[fill=black!50!] (2,0) rectangle (3,2.5);
\draw[fill=black!50!] (3,0) rectangle (4,1);

\draw[dashed] (1,1.5) -- (0.4,1.5) node[left]{$v_0$};
\draw[dashed] (1,0) -- (1,-0.5) node[below]{$t_0$};
\draw[dashed] (2,0) -- (2,-0.5) node[below]{$t_1$};
\draw[dashed] (4,0) -- (4,-0.5) node[below]{$t_m$};
\draw[<->] (1,-0.2) -- (2,-0.2) node[midway, below]{$h$};

\end{tikzpicture}

\begin{minipage}{\columnwidth}
sample: $X=\{ x_1,x_2,...;x_n\}$ \\
$k$-th bin: $ B_k=\left[t_k,t_{k+1}\right), k=\{0,1,...,m-1\} $ \\
Number of observations in the $k$-th bin: $v_k$ \\
bin width: $h=t_{k+1}-t_k, \forall k$ \\
\end{minipage}

\paragraph{Scott's Rule}

$$h^* \approx 3.5\sigma n^{-\frac{1}{3}}$$

\noindent For approximately normal distributed data (min.\ MSE)


\end{multicols}

%\subsubsection{QQ-Plot}


%\subsubsection{Scatterplot}

%-------------------------------------------------------------------------------

% SECTION: PROBABILITY
%-------------------------------------------------------------------------------

\section{Probability}

\subsection{Combinatorics}

% first column
\begin{minipage}[t]{0.7\textwidth}
\addvbuffer[12pt 8pt]{\begin{tabular}{l r || c | c}
& & without replacement & with replacement \\
\midrule
Permutations & & $n!$ & $\frac{n!}{n_1!\cdot\cdot\cdot n_s!}$ \\
\midrule
Combinations: & without order & $\binom{n}{m}$ & $\binom{n+m-1}{m}$ \\
& with order & $\binom{n}{m}m!$ & $n^m$ \\
\end{tabular}}
\end{minipage}
%second column
\begin{Mathspez}
\begin{minipage}[b]{0.2\textwidth}
with: 


 $n!=n\cdot (n-1)\cdot ... \cdot 1$

 $\binom{n}{m} = \frac{n!}{m!(n-m)!}$

\end{minipage}
\end{Mathspez}


\begin{multicols}{2}[\subsection{Probability Theory}][4cm]


\paragraph{Laplace}

$$P(A)=\frac{|A|}{|\Omega|}$$


\paragraph{Kolmogorov Axioms} mathematical definition of probability

\addvbuffer[12pt 8pt]{\begin{tabular}{c l}

(1) & $0 \le P(A) \le 1 \hspace{0.5 cm} \forall A \in \mathcal{A} = \sigma\text{-algebra}(\Omega)$ \\

(2) & $P(\Omega) = 1$ \\

(3) & $P(\bigcup_{i=1}^{\infty}{A_i}) = \sum_{i=1}^{\infty}{P(A_i)}$ \\ 
    & $\forall A_i \in \mathcal{A}, i=1,...,\infty \text{ with } A_i \cap A_j = \emptyset \text{ for } i \neq j$ \\

\end{tabular}}


\begin{Mathfolg}

\noindent Implications:
\begin{itemize}
\setlength\itemsep{0em}
\item $P(\bar{A})=1-P(A)$
\item $P(\emptyset)=0$
\item $P(A\cup B)=P(A)+P(B)-P(A\cap B)$
\item $A \subseteq B \Rightarrow P(A) \le P(B)$
\end{itemize}

\end{Mathfolg}

\paragraph{Probability (Mises)} frequentist definition of probability

$$P(A) = \lim\limits_{n \to \infty}\frac{n_A(n)}{n}$$

\noindent with $n$ repetitions of a random experiment and $n_A(n)$ events $A$

\paragraph{Conditional Probability}

$$P(A|B)=\frac{P(A \cap B)}{P(B)} \hspace{0.5cm} \text{for } P(B) > 0$$


$ \Rightarrow P(A \cap B)=P(B|A)P(A)=P(A|B)P(B)$

\paragraph{Law of Total Probability}

$$P(B)=\sum\limits_{i=1}^nP(B|A_i)P(A_i) \hspace{0.5cm}\text{for } \Omega=\bigcup_{i=1}^{\infty}{A_i} \text{ and } A_i {\cap} A_j = \emptyset$$

\paragraph{Bayes' Theorem}

$$P(A|B)=\frac{P(B|A)P(A)}{P(B)} \hspace{0.5cm} \text{for } P(A), P(B) > 0$$

\paragraph{Stochastic Independence}

\begin{alignat*}{2}
 \text{A, B independent}  \Leftrightarrow  && P(A\cap B) &= P(A)\cdot P(B) \\
 \text{X, Y independent}  \Leftrightarrow && f_{XY}(x,y) &= f_X(x)\cdot f_Y(y) \hspace{0.5cm} \forall x,y
\end{alignat*}

\end{multicols}


\begin{multicols}{2}[\subsection{Random Variables/Vectors}][4cm]

\subsubsection*{\textit{Random Variables $\in \mathbb{R}$}}

\paragraph{Definition}

$$Y: \Omega \to \mathbb{R}$$

\noindent The subset of possible values for $\mathbb{R}$ is called support.

\noindent Notation: Realisations of $Y$ are depicted with lower case letters. $Y=y$ means, that $y$ is the realisation of $Y$.

\paragraph{Discrete and Continuous Random Variables} \ \\

\noindent If the support is uncountably infinite, the random variable is called \textit{continuous}, otherwise it is called \textit{discrete}.

\begin{itemize}
\item \textbf{Density \boldmath$f(\cdot)$} (positive, integrates out to 1): 

For continuous variables:
$P(Y \in \left[a, b\right]) = \int_{a}^{b} f_Y(y) dy$

For discrete variables the density (and other functions) can be depicted like the corresponding function for continuous variables, if the notation is extended as follows: 
$\int_{-\infty}^{y} f_Y(\tilde{y})d\tilde{y} := \sum_{k:k \leq y} P(Y=k)$. This notation is used.

If $Y = g(X)$, then $f_Y(y) = \left|\frac{d g^{-1}(y)}{d y}\right|f_X(g^{-1}(y))$.

\item \textbf{Cumulative Distribution Function \boldmath$F(\cdot)$:} 
$$F_Y(y) =P(Y\leq y)$$

with $\lim_{y\rightarrow-\infty}F_Y(y)=0$ and $\lim_{y\rightarrow\infty}F_Y(y)=1$
\end{itemize}

Relationship:

$$F_Y(y) = \int_{-\infty}^{y} f_Y(\tilde{y})d\tilde{y}$$



\paragraph{Moments}

\begin{itemize}
\item \textbf{Expectation (1.\ Moment)}: $\mu = E(Y) = \int y f_Y(y)dy$
with $E(a{+}bX) = a+b E(X)$ and $E(X{\pm}Y) = E(X) \pm E(Y)$
\item \textbf{Variance (2.\ centralized Moment)}: $\sigma^2 = Var(Y) = E(\{Y-E(Y)\}^2) = \int (y - E(Y))^2 f_Y(y) dy$ \\
with $Var(a+bX) = b^2Var(X)$ and $Var(X{\pm}Y)= Var(X)+Var(Y)\pm 2 Cov(X,Y)$ \\
Note: $E(\{Y-\mu\}^2) = E(Y^2) - \mu^2$
\begin{Proof}
$E(\{Y-\mu\}^2) = E(Y^2 - 2Y\mu + \mu^2) = E(Y^2) - 2\mu^2 + \mu^2 = E(Y^2) - \mu^2$
\end{Proof}
\item \textbf{$k$th Moment}: $E(Y^k) = \int y^k f_Y(y) dy$,\\ \textbf{$k$th centralized Moment}: $E(\{Y-E(Y)\}^k)$
\end{itemize}

\paragraph{Moment Generating Function }

$$M_Y(t) = \mathrm{E}_Y(e^{tY})$$

with $\frac{\partial^kM_Y(t)}{\partial t^k} \bigg|_{t = 0} = \mathrm{E}(Y^k)$ 

Cumulant Generating Function $K_Y(t) = \log M_Y(t)$

\noindent A random variable is uniquely defined by its moment generating function and vice versa (as long as moments and cumulants are finite).

\paragraph{Chebyshev's Inequality}
with $E(X) =\mu$ and $Var(X) = \sigma^2$

$$P(|X-\mu| \geq c) \leq \frac{\sigma^2}{c^2}$$
  
\subsubsection*{\textit{Random Vectors $\in \mathbb{R}^q$}}

\paragraph{Definition}

$$(Y_1,Y_2,...,Y_q)$$ 

with random variables $Y_i$ 

\paragraph{Density and Cumulative Distribution Function}

$$F(y_1, ..., y_q) = P(Y_1 \leq y_1, ..., Y_q \leq y_q)$$
$$P(a_1 {\leq} Y_1 {\leq} b_1, ..., a_q {\leq} Y_q {\leq} b_q) = \int\limits_{a_1}^{b_1} ...\int\limits_{a_q}^{b_q} f(y_1, .., y_q)dy_1...dy_q$$

\paragraph{Marginal Density}

$$f_{Y_1}(y_1) = \int_{-\infty}^{\infty}...\int_{-\infty}^{\infty} f(y_1,...,y_k)dy_2...dy_k$$

\paragraph{Conditional Density} two-dimensional case

$$f_{Y_1|Y_2}(y_1|y_2) = \frac{f(y_1, y_2)}{f(y_2)} \text{ for } f(y_2) > 0$$

\paragraph{Covariance and Correlation}

$$Cov(Y_j,Y_k) = E(Y_jY_k) - E(Y_j)E(Y_k)$$

$$Cor(Y_j,Y_k) = \frac{Cov(Y_j,Y_k)}{\sqrt{Var(Y_j)Var(Y_k)}}$$

\paragraph{Iterated Expectation}

$$\mathrm{E}(Y)=\mathrm{E}_X(\mathrm{E}(Y|X))$$
\begin{Proof}
$$\mathrm{E}(Y) = \int yf(y)dy = \int\int y f(y|x)dy f_X(x)dx = \mathrm{E}_X(\mathrm{E}(Y|X))$$
\end{Proof}
$$\mathrm{Var}(Y) = \mathrm{E}_X(\mathrm{Var}(Y|X)) + \mathrm{Var}_X(\mathrm{E}(Y|X))$$
\begin{Proof}
\begin{align*}
\mathrm{Var}(Y) =&  \int (y- \mu_Y)^2 f(y)dy\\
=& \int (y- \mu_Y)^2 f(y|x)f(x)dydx\\
=& \int (y- \mu_{Y|x} + \mu_{Y|x} - \mu_Y)^2 f(y|x)f(x)dydx\\
=& \int (y- \mu_{Y|x})^2 f(y|x)f(x)dydx + \\
 & \int (\mu_{Y|x} - \mu_Y)^2 f(y|x)f(x)dydx + \\
 & 2 \int (y- \mu_{Y|x})(\mu_{Y|x} - \mu_Y) f(y|x)f(x)dydx \\
=& \int \mathrm{Var}(Y|x)f(x)dx + \int (\mu_{Y|x} - \mu_Y)^2f(x)dx\\
=& \mathrm{E}_X(\mathrm{Var}(Y|X)) + \mathrm{Var}_X(\mathrm{E}(Y|X))
\end{align*}
\end{Proof}

\end{multicols}

\subsection{Probability Distributions}

\begin{multicols}{2}[\subsubsection{Discrete Distributions}][4cm]

  \paragraph{Discrete Uniform}
  
    \begin{align*}
    & Y \sim \mathrm{U}(\{y_1, ..., y_k\}),\: y \in \{y_1, ..., y_k\} \\
    & P(Y=y_i) =\frac{1}{k},\: i = 1, ...,k \\
    & \mathrm{E}(Y) = \frac{k+1}{2} ,\: \mathrm{Var}(Y) = \frac{k^2 - 1}{12}
  \end{align*}
  
  
    \paragraph{Binomial}
  successes in independent trials \\ \noindent with special case Bernoulli: $Y \sim \mathrm{Bin}(1, \pi)$

  \begin{align*}
    & Y \sim \mathrm{Bin}(n, \pi) \text{ with } n \in \mathbb{N}, \pi \in \left[0,1\right] ,\: y \in \{0, ..., n\} \\
    & P(Y=y|\lambda) = \binom{n}{y}\pi^k(1-\pi)^{n-y} \\
    & \mathrm{E}(Y|\pi,n) = n\pi ,\: \mathrm{Var}(Y|\pi,n) = n\pi(1-\pi)
  \end{align*}

  \paragraph{Poisson}
  Counting model for rare events

\noindent only one event at a time, no autocorrelation, mean number of events over time is constant and proportional to length of the considered time interval 

  \begin{align*}
    & Y \sim \mathrm{Po}(\lambda) \text{ with } \lambda \in \left[ 0, + \infty \right] ,\: y \in \mathbb{N}_0 \\
    & P(Y=y|\lambda) =\frac{\lambda^y exp^{-\lambda}}{y!} \\
    & \mathrm{E}(Y|p) = \lambda ,\: \mathrm{Var}(Y|p) = \lambda
  \end{align*}

\noindent The model tends to overestimate the variance (Overdispersion).
  
\noindent   \textit{Approximation} of the Binomial for small $p$ and big $n$

  
    \paragraph{Geometric}

  \begin{align*}
    & Y \sim \mathrm{Geom}(\pi) \text{ with } \pi \in \left[0,1\right] ,\: y \in \mathbb{N}_0 \\
    & P(Y=y|\pi) = \pi(1-\pi)^{y-1} \\
    & \mathrm{E}(Y|\pi) = \frac{1}{\pi} ,\: \mathrm{Var}(Y|\pi) = \frac{1-\pi}{\pi^2}
  \end{align*}
  
    \paragraph{Negative Binomial}

  \begin{align*}
    & Y \sim \mathrm{NegBin}(\alpha, \beta) \text{ with } \alpha, \beta \geq 0 ,\: y \in \mathbb{N}_0 \\
    & P(Y=y|\alpha, \beta) = \binom{\alpha + y - 1}{\alpha - 1} \left(\frac{\beta}{\beta - 1}\right)^{\alpha} \left(\frac{1}{\beta + 1}\right)^y \\
    & \mathrm{E}(Y|\alpha,\beta) = \frac{\alpha}{\beta} ,\: \mathrm{Var}(Y|\alpha,\beta) = \frac{\alpha}{\beta^2}(\beta+1)
  \end{align*}

\end{multicols}

\begin{multicols}{2}[\subsubsection{Continuous Distributions}][4cm]

	\paragraph{Continuous Uniform}
  
    \begin{align*}
    & Y \sim \mathrm{U}(a,b) \text{ with } \alpha, \beta \in \mathbb{R}, a \le b,\: y \in \left[a,b\right] \\
    & p(y|a,b) =\frac{1}{b-a} \\
    & \mathrm{E}(Y|a,b) = \frac{a+b}{2} ,\: \mathrm{Var}(Y|a,b) = \frac{(b-a)^2}{12}
  \end{align*}
  
    	\paragraph{Exponential} Time between Poisson events
  
    \begin{align*}
    & Y \sim \mathrm{Exp}(\lambda) \text{ with } \lambda > 0,\: y \geq 0\\
    & p(y|\lambda) = \lambda\exp (-\lambda y) \\
    & \mathrm{E}(Y|\lambda) = \frac{1}{\lambda}, \:
	\mathrm{Var}(Y|\lambda) = \frac{1}{\lambda^2}
  \end{align*}
  
    \paragraph{Univariate Normal} symmetric with $\mu$ and $\sigma^2$
  
    \begin{align*}
    & Y \sim \mathrm{N}(\mu, \sigma^2) \text{ with } \mu \in \mathbb{R}, \sigma^2 > 0,\: y \in \mathbb{R} \\
    & p(y|\mu, \sigma^2) =\frac{1}{\sqrt{2\pi\sigma^2}} \exp \left(-\frac{(y-\mu)^2}{2 \sigma^2} \right) \\
    & \mathrm{E}(Y|\mu, \sigma^2) = \mu ,\: \mathrm{Var}(Y|\mu, \sigma^2) = \sigma^2
  \end{align*}
  
  \noindent   \textit{Approximation} of \\ Binomial for $np(1{-}p)\geq 9$ with $\mathrm{N}(np,np(1{-}p))$ \\
  Poisson for $\lambda \geq 10$ with $\mathrm{N}(\lambda,\lambda)$
  
    \paragraph{Log-Normal}
  
    \begin{align*}
    & Y \sim \mathrm{LogN}(\mu, \sigma^2) \text{ eith } \mu \in \mathbb{R}, \sigma^2 > 0,\: y > 0 \\
    & p(y|\mu, \sigma^2) =\frac{1}{\sqrt{2\pi\sigma^2 y}} \exp \left(-\frac{(\log y-\mu)^2}{2 \sigma^2} \right) \\
    & \mathrm{E}(Y|\mu, \sigma^2) = \exp (\mu + \frac{\sigma^2}{2}) ,\\
    & \mathrm{Var}(Y|\mu, \sigma^2) = \exp (2\mu + \sigma^2)(\exp (\sigma^2) - 1)
  \end{align*}
  
\noindent Relationship: $\log (Y) \sim \mathrm{N}(\mu, \sigma^2) \Rightarrow Y \sim \mathrm{LogN}(\mu, \sigma^2)$
  
    \paragraph{non-standardized Student's t} statistical tests for $\mu$ with unknown (estimated) variance and $\nu$ degrees of freedom
  
    \begin{align*}
    & Y \sim \mathrm{t}_\nu(\mu, \sigma^2) \text{ with } \mu \in \mathbb{R}, \sigma^2, \nu > 0,\: y \in \mathbb{R}\\
    & p(y|\mu, \sigma^2, \nu) =\frac{\Gamma \left( \frac{\nu + 1}{2}\right) }{\Gamma (\frac{\nu}{2}) \Gamma (\sqrt{\nu\pi}\sigma)} \left(1+ \frac{(y-\mu)^2}{\nu \sigma^2} \right)^{-\frac{\nu + 1}{2}} \\
    & \mathrm{E}(Y|\mu, \sigma^2, \nu) = \mu \text{ for }  \nu > 1,\\
    & \mathrm{Var}(Y|\mu, \sigma^2, \nu) = \sigma^2 \frac{\nu}{\nu-2} \text{ for }  \nu > 2
  \end{align*}
  
 \noindent Relationship: $Y|\theta \sim \mathrm{N}(\mu, \frac{\sigma^2}{\theta}), \: \theta \sim  \mathrm{Ga}(\frac{\nu}{2}, \frac{\nu}{2}) \Rightarrow Y \sim \mathrm{t}_\nu(\mu, \sigma)$ 
 $t_\nu(\mu,\sigma^2)$ has heavier tails then the normal distribution. $t_\infty(\mu,\sigma^2)$ approaches $\mathrm{N}(\mu,\sigma^2)$.
  
	\paragraph{Beta}
  
    \begin{align*}
    & Y \sim \mathrm{Be}(a, b) \text{ with } a,b > 0,\: y \in \left[0,1\right]\\
    & p(y|a, b) =\frac{\Gamma \left( a+b\right) }{\Gamma (a) \Gamma (b)} y^{a-1} (1-y)^{b-1} \\
    & \mathrm{E}(Y|a, b) = \frac{a}{a+b},\\
    & \mathrm{Var}(Y|a, b) = \frac{ab}{\left(a+b\right)^2(a+b+1)}, \\
    & \mathrm{mod}(Y|a, b) = \frac{a-1}{a+b-2} \text{ for } a,b > 1
  \end{align*}
  
  
  	\paragraph{Gamma}
  
    \begin{align*}
    & Y \sim \mathrm{Ga}(a, b) \text{ with } a,b > 0,\: y > 0\\
    & p(y|a, b) = \frac{ b^a }{\Gamma (a)} y^{a-1} \exp (-by) \\
    & \mathrm{E}(Y|a, b) = \frac{a}{b},\\
    & \mathrm{Var}(Y|a, b) = \frac{a}{b^a}, \\
    & \mathrm{mod}(Y|a, b) = \frac{a-1}{b} \text{ for } a \ge 1
  \end{align*}

  	\paragraph{Inverse-Gamma}
  
    \begin{align*}
    & Y \sim \mathrm{IG}(a, b) \text{ with } a,b > 0,\: y > 0\\
    & p(y|a, b) = \frac{ b^a }{\Gamma (a)} y^{-a-1} \exp (-\frac{b}{y}) \\
    & \mathrm{E}(Y|a, b) = \frac{b}{a-1} \text{ for } a > 1,\\
    & \mathrm{Var}(Y|a, b) = \frac{b^2}{(a-1)^2(a-2)} \text{ for } a \ge 2, \\
    & \mathrm{mod}(Y|a, b) = \frac{b}{a+1}
  \end{align*}
  
\noindent Relationship: $Y^{-1} \sim \mathrm{Ga}(a, b) \Leftrightarrow Y \sim \mathrm{IG}(a, b)$
  
  

  
  
  	\paragraph{Chi-Squared} sum of squares for standard normal random variables with $\nu$ degrees of freedom
  
    \begin{align*}
    & Y \sim \chi^2(\nu) \text{ with } \nu > 0,\: y \in \mathbb{R}\\
    & p(y|\nu) = \frac{y^{\frac{\nu}{2}-1}e^{-\frac{y}{2}}}{2^{\frac{\nu}{2}}\Gamma \left(\frac{\nu}{2}\right)} \\
    & \mathrm{E}(Y|\nu) = \nu, \:
	\mathrm{Var}(Y|\nu) = 2\nu
  \end{align*}
  
    \paragraph{Weibull} failure rate is proportional to a power of time
  
    \begin{align*}
    & Y \sim \mathrm{WB}(\lambda, k) \text{ with } \lambda> 0\\
    & p(y|\lambda,k) =
\frac{k}{\lambda}\left(\frac{y}{\lambda}\right)^{k-1}\mathrm{e}^{-(y/\lambda)^{k}} \\
  \end{align*}

\end{multicols}

\begin{multicols}{2}[\subsubsection{Exponential Family}][4cm]

  \paragraph{Definition} \ \\
  \noindent The exponential family comprises all distributions, whose density can be written as follows:
  
  $$f_Y(y,\theta) = e^{t^T(y)\theta - \kappa (\theta)}h(y)$$
  
  \noindent with $h(y) \geq 0$, $t(y)$ vector of the canonical statistic, parameter vector $\theta$ and $\kappa (\theta)$ as the normalising constant.
  
  \paragraph{Normalising Constant}
  
  \begin{align*}
  1 &= \int \exp^{t^T(y)\theta}h(y)dy \exp^{ - \kappa (\theta)} \\
  \Leftrightarrow \kappa (\theta) &= \log \int \exp^{t^T(y)\theta}h(y)dy
  \end{align*}
  
  \noindent $\kappa (\theta)$ is the cumulant generating function, therefore e.\,g.\ $\frac{\partial\kappa(\theta)}{\partial\theta_1} = \mathrm{E}(t_1(Y))$ 
  %and $\frac{\partial^2\kappa(\theta)}{\partial\theta^2} = \mathrm{Var}(t(Y))$
  
  
  
  \paragraph{Members}
  
  \begin{itemize}
  \item \textbf{Poisson}
  \item \textbf{Geometric}
  \item \textbf{Exponential}
  \item \textbf{Normal}
   $t(y) = \left(-\frac{y^2}{2},y \right)^T$,  
   $\theta = \left(\frac{1}{\sigma^2}, \frac{\mu}{\sigma^2}\right)^T$,
   $h(y) = \frac{1}{\sqrt{2\pi}}$,
   $\kappa ( \theta ) = \frac{1}{2} \left( -\log \frac{1}{\sigma^2} + \frac{\mu^2}{\sigma^2} \right)$
  \item \textbf{Gamma}
  \item \textbf{Chi-Squared}
  \item \textbf{Beta}
  \item \textbf{Binomial}
  \end{itemize}
  


\end{multicols}


\begin{multicols}{2}[\subsection{Multivariate Distributions}][4cm]

\paragraph{Multivariate Normal} symmetric with $\mu_i$ and $\Sigma$
  
    \begin{align*}
    & Y \sim \mathrm{N}(\mu, \Sigma) \text{ with } \mu \in \mathbb{R}^d, \Sigma \in \mathbb{R}^{d\times d} s.p.d.,\: y \in \mathbb{R}^d \\
    & p(y|\mu, \Sigma) = (2\pi)^{-\frac{d}{2}} \det (\Sigma)^{-\frac{1}{2}} \exp \left( -\frac{1}{2}(y-\mu)^{T} \Sigma^{-1}(y-\mu)\right) \\
    & \mathrm{E}(Y|\mu, \Sigma) = \mu ,\: \mathrm{Var}(Y|\mu, \Sigma) = \Sigma
  \end{align*}
  
  \paragraph{General Copulas}
  
  $$F(y_1,..,y_q) = C(F_1(y_1),...,F_q(y_q))\text{ with } C: \left[0,1\right]^q \rightarrow \left[0,1\right]$$
  with $C$ monotonically increasing as a cdf on $[0,1]^q$
  
  Modelled as follows:
  \begin{enumerate}
  \item marginal distributions $F_j(y_j) = C\left(F_j(y_j),1...,1\right)$
  \item dependence structure $\hat{u}_{i} = (\hat{u}_{i1},...,\hat{u}_{iq}) \overset {iid}{\sim} C(.)$ with $\hat{u}_{ij} := \hat{F}_j(y_{ij})$. 
  
  The copula density is $c(u_{1:q}) = \frac{\partial^q C(u_{1:q})}{\partial u_1...\partial u_q}$ and  $f(y_{1:q}) = c(F_1(y_1),...,F_q(y_q))\prod_{j=1}^q f_j(y_j)$.
   \end{enumerate}
   
   \textbf{Tail Dependence} \ \\
   upper: $ \lambda_{u}  := \underset{u\rightarrow 1}{\lim} P(Y_1 \geq F_1^{-1}(u)|Y_2\geq F_2^{-1}(u))$  
   
   $ = \underset{u\rightarrow 1}{\lim} \frac{1-2u+C(u,u)}{1-u}$
   
   lower: $ \lambda_{l}  := \underset{u\rightarrow 0}{\lim} P(Y_1 \leq F_1^{-1}(u)|Y_2\leq F_2^{-1}(u))$   
   
   $ = \underset{u\rightarrow 0}{\lim} \frac{C(u,u)}{u}$
   
   \paragraph{Gaussian Copula} coefficients for pairwise dependences 
   $$c(u_{1:q}) = \frac{1}{|R|^{1/2}} \exp\left(-\frac{1}{2}u^TR^{-1}u\right)$$  
   For $Y_{ij} \sim \mathrm{N}(\mu_j,\sigma_j)$: $f(y_{ij};\mu_j, \sigma^2_j) = \frac{1}{\sigma_j}\phi(Z_{ij})$ with $Z_{ij}$ the standardized $Y_{ij}$. With $u_{ij} = \phi^{-1}(Z_{ij})$, $R$ can be estimated.
   
   $\lambda_l = 0$, $\lambda_u = 0$
   
   \paragraph{Archimedean Copulas} few parameters even in high dimensions
   $$\psi(;\theta):[0,1] \rightarrow [0,\infty)$$
with the parametric generator function $\psi(u,\theta)$ continuous, strictly decreasing, convex, and $\psi(1,\theta) = 0\; \forall \theta$
 $$C(u_{1:q};\theta) = \psi^{-1}(\psi(u_1;\theta)+...+ \psi(u_q;\theta);\theta)$$
 
 \begin{itemize}
 \item \textbf{Clayton} $\psi(t;\theta) = \frac{1}{\theta}(\theta^{-1}-1)$: $\lambda_l = 2^{-1/\theta}$, $\lambda_u = 0$
 \item \textbf{Frank}  $\psi(t;\theta) = -\log \frac{\exp(-\theta t)-1}{\exp(-\theta )-1}$: $\lambda_l = 0$, $\lambda_u = 0$
  \item \textbf{Gumbel} $\psi(t;\theta) = (-\log(t))^\theta$: $\lambda_l = 0$, $\lambda_u = 2-2^{1/\theta}$
 \end{itemize}
 
 \paragraph{Pair Copulas} flexible pairwise dependences
 $$f_{123} = c_{12}c_{23}c_{23|1} \prod_{j=1}^3 f_j$$
 
 \paragraph{Generalized Extreme Value Distribution (GEV)} \ \\
 
 \noindent for block maxima $M_n := \max(Y_{1:n})$: $F_{M_n}(y) = P(M_n\leq y) = P(Y_{1:n}\leq y) = (F_Y(y))^n$
 
 $$ \underset{n\rightarrow \infty}{\lim}f_{M_n}(y) =  \begin{cases}
      1, & \text{if}\ F(y)=1 \\
      0, & \text{otherwise}
    \end{cases}$$
    
    \noindent For $\{a_n\}^\infty_{n=1}$, $\{b_n\}^\infty_{n=1}$ fixed sequences, the standardized maximum $\frac{M_n - a_n}{b_n}$ converges to a GEV as $n\rightarrow\infty$.
    
    $$G(x) = \begin{cases}
      \exp(-(1+\gamma z)^{-1/\gamma}), & \text{for}\ \gamma \neq 0 \\
      \exp(-\exp(-z)), & \text{for}\ \gamma = 0 
    \end{cases}$$
    
    with location $\mu$, scale $\sigma$, and shape $\gamma$ and $z=\frac{x-\mu}{\sigma}$
    
    
  \begin{itemize}
  \item \textbf{Gumbel} $\gamma=0$
  \item \textbf{Weibull} $\gamma> 0$
  \item \textbf{Frechet-Pareto} $\gamma<0$
  \end{itemize}
    
    
    
 
 
 
 
 
 







\end{multicols}




\begin{multicols}{2}[\subsection{Limit Theorems}][4cm]

  \paragraph{Law of Large Numbers} \ \\
  
  $$\lim_{n\rightarrow \infty} P\left(\left|\bar{X}_n {-} \mu\right| < c\right)=1 \hspace{1cm} \forall c>0$$
  
  \noindent with $X_i$ i.i.d., $E(X_i)=\mu$, and $\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$
  
  \paragraph{Central Limit Theorem}
  
  $$Z_n \overset{d}{\longrightarrow} \mathrm{N}(0, \sigma^2)$$
  with  $Z_n = \sum_{i=1}^{n} \frac{Y_i}{\sqrt{n}}$ and $Y_i$ i.i.d. with expectation $0$ and variance $\sigma^2$
  
\begin{Proof}
For normal random variables $Z \sim \mathrm{N}(\mu, \sigma^2)$: $K_Z(t)=\mu t + \frac{1}{2}\sigma^2t^2$. The first two derivatives $\frac{\partial^kK_Z(t)}{\partial t^k} \bigg|_{t = 0}$ are $\mu$ and $\sigma$. All other moments are zero. 

\noindent For $Z_n = (Y_1 + Y_2 + ... +Y_n)/\sqrt{n}$:
\begin{align*}
M_{Z_n}(t) &= \mathrm{E}\left(e^{t(Y_1 + Y_2 + ... +Y_n)/\sqrt{n}}\right)\\
&= \mathrm{E}\left(e^{tY_1/\sqrt{n}} \cdot e^{tY_2/\sqrt{n}}\cdot ... \cdot e^{tY_n/\sqrt{n}}\right) \\
&= \mathrm{E}\left(e^{tY_1/\sqrt{n}}\right) \mathrm{E}\left(e^{tY_2/\sqrt{n}}\right) ... \mathrm{E}\left(e^{tY_n/\sqrt{n}}\right) \\
&= M_Y^n(t/\sqrt{n})
\end{align*}
Analoguously: $K_{Z_n}(t) = nK_Y(t/\sqrt{n})$.
\begin{align*}
 \frac{\partial K_{Z_n}(t)}{\partial t} \bigg|_{t = 0} &= \frac{n}{\sqrt{n}} \frac{\partial K_Y(t)}{\partial t} \bigg|_{t = 0} = \sqrt{n}\mu \\
 \frac{\partial^2K_{Z_n}(t)}{\partial t^2} \bigg|_{t = 0} &= \frac{n}{n} \frac{\partial^2 K_Y(t)}{\partial t^2} \bigg|_{t = 0} = \sigma^2
\end{align*}
Using the Taylor Expansion, we can write $K_{Z_n}(t) = 0 + \sqrt{n}\mu t + \frac{1}{2}\sigma^2t^2 + ...$, where the terms in $...$ are tending towards 0 as $n \rightarrow \infty$.

\noindent Therefore: $K_{Z_n}(t) \overset{n\rightarrow\infty}{\longrightarrow} K_{Z}(t)$ with $Z \sim \mathrm{N}(\sqrt{n}\mu,\sigma^2)$.
\end{Proof}

\end{multicols}


%-------------------------------------------------------------------------------

% SECTION: INFERENCE

%-------------------------------------------------------------------------------


\section{Inference}


\subsection{Method of Moments}

The theoretical moments are estimated by their empirical counterparts: \ \\
\vspace{0.5em}
$\mathrm{E}_{\hat{\theta}_{MM}}(Y^k) = m_k(y_1,...,y_n) = \frac{1}{n} \sum_{i=1}^{n} y_i^k$ \ \\
\vspace{0.5em}
\noindent For the exponential family: $\hat{\theta}_{MM} = \hat{\theta}_{ML}$


\begin{multicols}{2}[\subsection{Loss Functions}][4cm]

\paragraph{Loss}

$$\mathcal{L}: \mathcal{T} \times \Theta \rightarrow \mathbb{R}^+$$

\noindent with parameter space $\Theta \subset \mathbb{R}$, $t\in \mathcal{T}$ with $t:\mathbb{R}^n \rightarrow \mathbb{R}$ a statistic, that estimates the parameter $\theta$,
 $\mathcal{L}(\theta, \theta) = 0$ holds

\begin{itemize}
\item \textbf{absolute loss (L1)}: $\mathcal{L}(t, \theta) = \left|t-\theta \right|$
\item \textbf{quadratic loss (L2)}: $\mathcal{L}(t, \theta) = (t-\theta)^2$
\end{itemize}

\noindent As $\theta$ is unknown, the loss is a theoretical quantity. It is also the realisation of a random variable as it depends on a sample.

\paragraph{Risk}

\begin{align*}
R(t(.), \theta) &= \mathrm{E}_\theta \left(\mathcal{L}(t(Y_1,...,Y_n),\theta)\right) \\
&= \int_{-\infty}^\infty \mathcal{L}(t(Y_1,...,Y_n),\theta) \prod_{i=1}^n f(y_i;\theta)dy_i
\end{align*}

\paragraph{Minimax Approach} minimizing the worst case \\
\noindent The risk still depends on the true parameter $\theta$.

\noindent Tentative estimation: Choose $\theta$, s.\,t.\ the risk is maximal and then $t(.)$, so that the risk is minimized:

$$\hat{\theta}_{minimax} = \underset{t(.)}{\text{arg min }} \left(\underset{\theta \in \Theta}{\text{max }} R(t(.);\theta)\right)$$


\paragraph{Mean Squared Error (MSE)}
\begin{align*}
MSE(t(.), \theta) &= \mathrm{E}_\theta\left(\{t(Y)-\theta\}^2\right) \\
&=\mathrm{Var}_\theta\left(t(Y_1,...,Y_n)\right) + Bias^2((t(.); \theta)
\end{align*}

\noindent with $Bias(t(.);\theta) = \mathrm{E}_\theta\left(t(Y_1,...,Y_n)\right)-\theta$

\begin{Proof}
Let $\mathcal{L}(t, \theta) = (t-\theta)^2$
\begin{align*}
R(t(.), \theta) = &\: \mathrm{E}_\theta (\{t(Y) -\theta\}^2) \\
= &\: \mathrm{E}_\theta (\{t(Y) - \mathrm{E}_\theta(t(Y)) + \mathrm{E}_\theta(t(Y))-\theta\}^2) \\
= &\: \mathrm{E}_\theta (\{t(Y) - \mathrm{E}_\theta(t(Y))\}^2) +
\mathrm{E}_\theta (\{\mathrm{E}_\theta(t(Y))-\theta\}^2) \\
& +  2\mathrm{E}_\theta (\{t(Y)-\mathrm{E}_\theta \left(t(Y))\}\{\mathrm{E}_\theta (t(Y)\right)-\theta\}) \\
= &\: \mathrm{Var}_\theta(t(Y_1,...,Y_n)) + Bias^2((t(.); \theta) + 0
\end{align*}
\end{Proof}

\textbf{Cramér-Rao Bound}

$$MSE(\hat{\theta}, \theta) \geq Bias^2(\hat{\theta},\theta) + 
\frac{\left(1+
\frac{\partial Bias(\hat{\theta}, \theta)}{\partial \theta}
\right)^2}{I(\theta)}$$

\begin{Proof}
For unbiased estimates: $ \theta = \mathrm{E}_\theta(\hat{\theta}) = \int t(y)f(y;\theta)dy$
\begin{align*}
1 &= \int t(y) \frac{\partial f(y;\theta)}{\partial \theta} dy \\
&= \int t(y) \frac{\partial \log f(y;\theta)}{\partial \theta} f(y;\theta) dy \\
&= \int t(y) s(y;\theta) f(y;\theta) dy \\
&= \int \left( t(y)-\theta\right)\left(s(\theta;y)-0\right)f(y;\theta)dy
& \overset{\text{1. Bartlett equation}}{\mathrm{E}_\theta \left(s(\theta;y)\right) = 0}\\
&= \mathrm{Cov}_\theta\left(t(Y);s(\theta;Y)\right) \\
&\geq \sqrt{\mathrm{Var}_\theta(t(Y))} \sqrt{\mathrm{Var}_\theta(s(\theta;Y))} & \text{Cauchy-Schwarz} \\
&= \sqrt{MSE(t(Y);\theta)} \sqrt{I(\theta)}
\end{align*}
\end{Proof}

\paragraph{Kullback-Leibler Divergence} Comparing distributions

$$KL(\theta,t) = \int_{-\infty}^\infty \log\frac{f(\tilde{y};\theta)}{f(\tilde{y};t)} f(\tilde{y};\theta) d\tilde{y}$$

\noindent The KL divergence is not a distance as it is not symmetric.
\noindent It is $0$ for $t=\theta$ and $\geq 0$ otherwise.
\begin{Proof}
Follows from $\log (x) \leq x-1 \forall x \geq 0$, with equality for $x=1$.
\end{Proof}

\noindent $R_{KL} (\theta, t(.) )$ is approximated by the MSE.
\begin{Proof}
\vspace{-1.5em}
\begin{align*}
& R_{KL} (\theta,t(.)) = \\ =&\: \int_{-\infty}^\infty \mathcal{L}_{KL}(t(Y_1,...,Y_n),\theta) \prod_{i=1}^n f(y_i;\theta)dy_i \\
=&\: \int\int \log\frac{f(\tilde{y};\theta)}{f(\tilde{y};t)} f(\tilde{y};\theta) d\tilde{y} \prod_{i=1}^n f(y_i;\theta)dy_i \\
=&\: \int\int \left(\log f(\tilde{y};\theta) - \log f(\tilde{y};t) \right) f(\tilde{y};\theta) d\tilde{y}  \prod_{i=1}^n f(y_i;\theta)dy_i \\
\approx & \: - \int \underbrace{\left( \int \frac{\partial \log f(\tilde{y};\theta)}{\partial \theta} f(\tilde{y};\theta)d\tilde{y}\right)}_{0}\left(t-\theta\right) \prod_{i=1}^n f(y_i;\theta)dy_i  \\
+ &  \frac{1}{2} \int \underbrace{\left( - \int \frac{\partial^2 \log f(\tilde{y};\theta)}{\partial \theta^2} f(\tilde{y};\theta)d\tilde{y}\right)}_{I(\theta)}\left(t-\theta\right)^2 \prod_{i=1}^n f(y_i;\theta)dy_i 
\end{align*}
The last step is approximated by the Taylor Expansion:
$\log f(\tilde{y}, t) \approx \log f(\tilde{y},\theta) {+} \frac{\partial\log f(\tilde{y}, \theta)}{\partial\theta} (t{-}\theta) {+} \frac{1}{2} \frac{\partial^2\log f(\tilde{y}, \theta)}{\partial\theta^2} (t{-}\theta)^2$
\end{Proof}
\end{multicols}


\begin{multicols}{2}[\subsection{Maximum Likelihood (ML)}][4cm]
  \paragraph{Prerequisites}
  
  \begin{itemize}
  \item $Y_i \sim F(y;\theta)\:\: i.i.d.$
  \item $\theta \in \mathbb{R}^p$
  \item $f(.;\theta)$ Fisher-regular:
  \begin{itemize}
  \item $\{ y: f (y; \theta)> 0 \}$ independent of $\theta$
  \item Parameter space $\Theta$ is open
  \item $f(y;\theta)$ twice differentiable
  \item $\int \frac{\partial}{\partial \theta} f(y;\theta)dy = \frac{\partial}{\partial \theta} \int f(y;\theta)dy$
  \end{itemize}
  \end{itemize}
  
  \paragraph{Central Functions} \ \\
  \begin{itemize}
  \item \textbf{Likelihood} $L(\theta;y_1,...,y_n)$: $\prod_{i=1}^n f(y_i;\theta)$
  \item \textbf{log-Likelihood} $l(\theta;y_1,..y_n)$: 
$\log L(\theta;y_1,...,y_n) = \sum_{i=1}^n \log f(y_i;\theta)$
  \item \textbf{Score} $s(\theta;y_1,...,y_n)$: $\frac{\partial l(\theta;y_1,..y_n)}{\partial \theta}$
  \item \textbf{Fisher-Information} $I(\theta)$: $-\mathrm{E}_\theta\left(\frac{\partial s(\theta;Y)}{\partial\theta}\right)$
  \item \textbf{observed Fisher-Information} $J(\theta)$: $-\mathrm{E}_\theta\left(\frac{\partial s(\theta;y)}{\partial\theta}\right)$
  \end{itemize}
  
  
  \paragraph{Attributes of the Score-Function} \ \\
  
  first Bartlett-Equation:
  
  $$\mathrm{E}\left(s(\theta;Y)\right) = 0$$
  
\begin{Proof}
\vspace{-1.5em}
\begin{align*}
1 &= \int f(y;\theta) dy \\
0 = \frac{\partial 1}{\partial\theta} &= \int \frac{\partial f(y;\theta)}{\partial \theta}dy = \int \frac{\partial f(y;\theta) / \partial\theta}{f(y;\theta)} f(y;\theta) dy \\ &= \int \frac{\partial \log f(y;\theta)}{\partial\theta}  f(y;\theta) dy = \int s(\theta;y) f(y;\theta) dy
\end{align*}
\end{Proof}
  
  second Bartlett-Equation:
  
  $$\mathrm{Var}_\theta\left(s(Y;\theta)\right) = \mathrm{E}_\theta\left(-\frac{\partial^2 log f(Y;\theta)}{\partial\theta^2}\right) = I(\theta)$$
  
\begin{Proof}
\vspace{-1.5em}
\begin{align*}
0 = \frac{\partial 0}{\partial \theta} =& \frac{\partial}{\partial\theta}\int \frac{\partial \log f(y;\theta)}{\partial\theta}  f(y;\theta) dy \hspace{2em}\text{      see above}\\
=& \int  \frac{\partial^2 \log f(y;\theta)}{\partial \theta^2}  f(y;\theta) dy  \\
&+ \int\frac{\partial \log f(y;\theta)}{\partial \theta}\frac{\partial f(y;\theta)}{\partial \theta}dy \\
=& \: \mathrm{E}_\theta \left( \frac{\partial^2 \log f(Y;\theta}{\partial \theta^2} )\right)  \\
&+ \int\frac{\partial \log f(y;\theta)}{\partial \theta}\frac{\partial \log f(y;\theta)}{\partial \theta} f(y;\theta) dy 
\end{align*}

$$\Leftrightarrow \mathrm{E}_\theta \left(s(\theta;Y) s(\theta;Y)\right) = \mathrm{E}_\theta \left(- \frac{\partial^2 \log f(Y;\theta)}{\partial \theta^2} \right)$$
\noindent Bartlett's second equation holds then as $\mathrm{E}\left(s(\theta;Y)\right) = 0$
\end{Proof}
  
  \paragraph{ML-Estimate}
   $$\hat{\theta}_{ML} = \text{arg max } l(\theta; y_1,...y_n)$$
   
\noindent for Fisher-regular distributions: $\hat{\theta}_{ML}$ has asymptotically the smallest variance, given by the Cramér-Rao bound, 
   $s\left(\hat{\theta}_{ML};y_1,...,y_n\right) = 0$
   
$\hat{\theta} \overset{a}{\sim} \mathrm{N}\left(\theta, I^{-1}(\theta)\right)$

\noindent If the true model is unknown, the distribution is $\hat{\theta} \overset{a}{\sim} \mathrm{N}\left(\theta, I^{-1}(\theta)V(\theta)I^{-1}(\theta)\right)$ with $V(\theta)$ variance of the score function.
   
\noindent The ML-estimate is invariant for a bijective and differentiable function $g(.)$: $\hat{\gamma} = g(\hat{\theta})$ if $\gamma = g(\theta)$. 
   
\begin{Proof}
$\gamma = g(\theta)\, \Leftrightarrow \,\theta = g^{-1}(\gamma)$

\noindent For the log-likelihood of $\gamma$ at the location $\hat{\theta}$ holds:

$$\frac{\partial l(g^{-1}(\hat{\gamma}))}{\partial \gamma} = \frac{\partial g^{-1}(\gamma)}{\partial\gamma} \underbrace{\frac{\partial l(\hat{\theta})}{\partial \theta}}_{=0} = 0$$
\end{Proof}   

\noindent Then, the Fisher information is $\frac{\partial\theta}{\partial\gamma} I(\theta) \frac{\partial\theta}{\partial\gamma}$

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
I_{\gamma}(\gamma) &= 
-\mathrm{E}\left(\frac{\partial^2 l(g^{-1}(\hat{\gamma}))}{\partial \gamma^2} \right)= 
-\mathrm{E}\left(\frac{\partial}{\partial\gamma} \left( \frac{\partial g^{-1}(\gamma)}{\partial\gamma} \frac{\partial l(\theta)}{\partial\theta} \right)\right) \\
&= -\mathrm{E}\left(\underbrace{\frac{\partial^2 g^{-1}(\gamma)}{\partial\gamma}\frac{\partial l(\theta)}{\partial\theta}}_{\text{Expectation 0}} + \frac{\partial g^{-1}(\gamma)}{\partial\gamma}\frac{\partial^2l(\theta)}{\partial\theta^2}\frac{\partial g^{-1}(\gamma)}{\partial\gamma}\right) \\
&= \frac{\partial g^{-1}(\gamma)}{\partial\gamma} I(\theta) \frac{\partial g^{-1}(\gamma)}{\partial\gamma} = \frac{\partial \theta}{\partial\gamma} I(\theta) \frac{\partial \theta}{\partial\gamma}
\end{align*}
\end{Proof}

\noindent Delta rule: $\gamma \overset{a}{\sim} \mathrm{N}(\hat{\gamma}, \frac{\partial \theta}{\partial\gamma} I^{-1}(\theta) \frac{\partial \theta}{\partial\gamma} )$




\paragraph{Numerical computation of the ML estimate}
Fisher- Scoring as statistical version of the Newton-Raphson procedure

\begin{enumerate}
\item Initialize $\theta_{(0)}$
\item Repeat: $\theta_{(t+1)} := \theta_{(t)} + I^{-1}(\theta_{(t)})s(\theta_{(t)};y)$ \label{repeat}
\item Stop if $\Vert \theta_{(t+1)} -\theta_{(t)} \Vert < \tau$; return $\hat{\theta}_{ML}=\theta_{(t+1)}$
\end{enumerate}

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
&0 = s(\hat{\theta}_{ML};y) \overset{Taylor}{\underset{Series}{\approx}} s(\theta;y) + \frac{\partial s(\theta;y)}{\partial \theta} (\hat{\theta}_{ML} - \theta) \Leftrightarrow \\
&\hat{\theta}_{ML} \approx \theta - \left(\frac{\partial s(\theta;y)}{\partial \theta}\right)^{-1} s(\theta;y) \approx \theta - I^{-1}(\theta)s(\theta;y)
\end{align*}
As $\frac{\partial s(\theta;y)}{\partial \theta}$ is often complicated, its expectation $I(\theta)$ is used.
\end{Proof}

\noindent The second part in \ref{repeat} can be weighted with a step size $\delta$ or $\delta(t)$ $\in (0,1)$, e.\,g.\ to ensure convergence.

\noindent If $I(\theta)$ can't be analytically derived, simulation from $f(y;\theta_{(t)})$ can be used. For the exponential family, step \ref{repeat} then changes to $\theta_{(t+1)} := \theta_{(t)} + \widehat{\mathrm{Var}}_{\theta_{(t)}}(t(Y))^{-1} \widehat{\mathrm{E}}_{\theta_{(t)}}(t(Y))$ as the ML estimate is the expectation.

\paragraph{Log Likelihood Ratio}

$$lr(\theta,\hat{\theta}) := l(\hat{\theta}) - l(\theta) = \log \frac{L(\hat{\theta})}{L(\theta)}$$

with $2\cdot lr(\theta,\hat{\theta}) \overset{a}{\sim} \chi^2_1$

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
l(\theta) & \overset{Taylor}{\underset{Series}{\approx}} l(\hat{\theta}) + \underbrace{\frac{\partial l(\hat{\theta})}{\partial \theta}}_{=0} (\theta - \hat{\theta}) + \frac{1}{2}\underbrace{\frac{\partial^2 l(\hat{\theta})}{\partial \theta^2}}_{\approx -I(\theta)}(\underbrace{\theta - \hat{\theta}}_{\substack{\approx I^{{-}1}(\theta)\cdot\\s(\theta;Y)}})^2\\
&\approx l(\hat{\theta}) - \frac{1}{2} \frac{s^2(\theta, Y)}{I(\theta)}
\end{align*}
$s(\theta,Y)$ is asymptotically normal.
\end{Proof}

If $\theta \in \mathbb{R}^p$ the corresponding distribution is $\chi^2_p$.

\paragraph{Relation to Kullback-Leibler divergence} 

$$\hat{\theta}_{ML} = \text{arg }\min \text{KL}(g,f)$$

with $f$ distributional model used and $g$ true model

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
KL(g,f) &= \int \log\frac{g(y)}{f(y)} g(y) dy\\
&= \int \log(g(y)) g(y) dy - \int \log(f(y)) g(y) dy
\end{align*}
To minimize that, the second component needs to be maximized. Its derivative is $\int s(\theta;y)g(y)dy = \mathrm{E}_g(s(\theta;Y)) = 0$
\end{Proof}
  

\end{multicols}


\begin{multicols}{2}[\subsection{Consistency and Sufficiency}][4cm]

\paragraph{Statistic} \ \\
$$t: \mathbb{R}^n \rightarrow \mathbb{R}$$
$t(Y_1, ..., Y_n)$ depends on sample size n and is a random variable

\paragraph{(Weak) Consistency} $\hat{\theta}$ gets closer to $\theta$ as $n$ grows
$$MSE(\hat{\theta},\theta) \overset{n\rightarrow\infty}{\longrightarrow} 0 \Rightarrow \hat{\theta} \text{ consistent}$$

\begin{Proof}
$P(|\hat{\theta} - \mathrm{E}_{\theta}(\hat{\theta})| \geq \delta) \leq \frac{Var_\theta(\hat{\theta})}{\delta^2}$ using the inequality of Chebyshev
and $MSE(t(.), \theta) = \mathrm{Var}_\theta(t(Y_1,...,Y_n)) + Bias^2((t(.); \theta)$
\end{Proof}

\paragraph{Sufficiency} \ \\
\noindent A statistic $t(y_1,...,y_n)$ is sufficient for $\theta$, if the conditional distribution $f(y_1,...,y_n|t_0 = t(y_1,...,y_n);\theta)$ is independent of $\theta$. \vspace{0.5em}

\textbf{Neyman criterion:}
$$t(Y_1,...,Y_n) \text{ sufficient } \Leftrightarrow f(y;\theta) = h(y)\:g(t(y);\theta)$$
\begin{Proof}
``$\Rightarrow$'':
$$f(y;\theta) = \underbrace{f(y|t {=} t(y);\theta)}_{h(y)} \underbrace{f_t(t|y;\theta)}_{g(t(y);\theta)}$$

\noindent ``$\Leftarrow$'':
$$f_t(t;\theta) = \int_{t=t(y)} f(y;\theta)dy = \int_{t=t(y)} h(y) g(t;\theta)dy$$
\indent Therefore:
$$f\left(y|t{=}t(y);\theta\right) = \frac{f(y,t{=}t(y);\theta)}{f_t(t,\theta)}
= \begin{cases}
\frac{h(y)g(t;\theta)}{g(t;\theta)} & t=t(y) \\
0 & \, \text{otherwise}
\end{cases}$$
\end{Proof}

\textbf{Minimal Sufficiency:} \ \\
$t(.)$ is sufficient and $\forall\: \tilde{t}(.)\: \exists\: h(.) \text{ s.t. } t(y) = h(\tilde{t}(y))$



\end{multicols}




  
%-------------------------------------------------------------------------------

% SECTION: HYPOTHESIS TESTS

%-------------------------------------------------------------------------------
 
\section{Statistical Hypothesis Testing}

\begin{multicols}{2}[\subsection{Significance and Confidence Intervals}][4cm]

\paragraph{Significance Test}  \ \\
\noindent Assuming two states $H_0$ and $H_1$ and two corresponding decisions ``$H_0$'' and ``$H_1$'', a decision rule (a threshold $c \in \mathbb{R}$ for the test statistic $T(X)$) is constructed s.\,t.:

$$p = P(``H_1"|H_0) \leq \alpha$$

\begin{center}
\begin{tabular}{c |c c}
%\hline\hline
& ``$H_0$'' & ``$H_1$'' \\
\hline
$H_0$ & $1-p$ (correct) & $p$ (type I error)\\
$H_1$ & $\beta$ (type II error) & $1-\beta$ (correct) \\
%\hline\hline
\end{tabular}
\end{center}

\paragraph{Power} concerns the type II error \ \\

$$power = P(``H_1"|H_1) = 1-\beta$$

\paragraph{p-Value} measures the amount of evidence against $H_0$

$$p\leq \alpha \Leftrightarrow ``H_0"$$

\noindent The $p$-value is uniformly distributed on $[0,1]$ under $H_0$.




\paragraph{Confidence Interval}
$$\begin{gathered}
\left[t_l(Y_{1:n}),t_r(Y_{1:n})\right] \text{ Confidence Interval } \\
\Leftrightarrow \\
P_\theta\left(t_l(Y_{1:n}) {\leq} \theta {\leq} t_r(Y_{1:n})\right) \geq 1{-}\alpha \:\forall \theta
\end{gathered} $$


\noindent with $1-\alpha$ confidence level und $\alpha$ significance level
\vspace{0.5em}

\textbf{Corresponding Test}

$$\theta_0 \notin \left[t_l(y_{1:n}), t_r(y_{1:n})\right] \Leftrightarrow ``H_1"$$


%\paragraph{Exakte binomiale Konfidenzintervalle}

\paragraph{Specificity}  or True Negative Rate ($1 -$empirical type I error)

$$TNR = \frac{\#TN}{\#N} = \frac{\#TN}{\#TN + \#FP}$$

\paragraph{Sensitivity}  or True Positive Rate, Recall (empirical power)

$$TPR = \frac{\#TP}{\#P} = \frac{\#TP}{\#TP + \#FN}$$

\end{multicols}

\begin{multicols}{2}[\subsection{Tests for One Sample}][4cm]

\subsubsection*{\textit{Normal Distribution $X_i \overset{iid}{\sim} N(\mu,\sigma^2)$}}

  \paragraph{Test for $\mu$, known $\sigma^2 $ (Simple Gauss-Test)} \ \\
  
  \noindent $H_0\!:\; \mu = \mu_0 \;\;\; vs. \;\;\; H_1\!:\; \mu \neq \mu_0$
  
  $$T(X) = \frac{\bar{X} -\mu_0}{\sigma/\sqrt{n}} \;\overset{H_0}{\sim} \;\mathrm{N}(0,1)$$
  
  
  
  \paragraph{Test for $\mu$, unknown $\sigma^2 $ (Simple t-Test)} \ \\
  
  \noindent $H_0\!:\; \mu = \mu_0 \;\;\; vs. \;\;\; H_1\!:\; \mu \neq \mu_0$
  
  $$T(X) = \frac{\bar{X} -\mu_0}{\hat{\sigma}/\sqrt{n}} \;\overset{H_0}{\sim}\; t_{n-1}$$
  
  \noindent with $\hat{\sigma} = \sqrt{\frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2}$
  
  \subsubsection*{\textit{Binomial Distribution $X_i \overset{iid}{\sim} Bin(1,p)$}}
  
  \paragraph{Approximate test for $p$} \ \\
  
  \noindent $H_0\!:\; p = p_0 \;\;\; vs. \;\;\; H_1\!:\; p \neq p_0$
  
  $$T(X) = \frac{\frac{X}{n} -p_0}{\sqrt{p_0(1-p_0)}/\sqrt{n}} \;\overset{H_0, appr.}{\sim}\; \mathrm{N}(0,1)$$

  
  
\subsubsection*{\textit{ML Estimate $\hat{\theta} \overset{a}{\sim} \mathrm{N}(\theta, I^{-1}(\theta))$}}
  
  \paragraph{Wald Test} \ \\
  
  \noindent $H_0\!:\; \theta = \theta_0 \;\;\; vs. \;\;\; H_1\!:\; \theta \neq \theta_0$
  
  $$T(X) = |\hat{\theta} - \theta_0|\; \overset{H_0}{\sim}\; \mathrm{N}(0, I^{-1}(\theta_0))$$
  
  \noindent As $\hat{\theta}$ converges to $\theta_0$ under $H_0$, it can also be used to calculate the variance: $I^{-1}(\hat{\theta})$.
  
    \paragraph{Score Test} \ \\
  
  \noindent $H_0\!:\; \theta = \theta_0 \;\;\; vs. \;\;\; H_1\!:\; \theta \neq \theta_0$
  
  $$T(X) = |s(\theta_0;y)|\; \overset{H_0}{\sim}\; \mathrm{N}(0, I(\theta_0))$$
  
  \noindent Advantage compared to the Wald Test: $\hat{\theta}$ does not have to be calculated.
  
      \paragraph{Likelihood Ratio Test} \ \\
  
  \noindent $H_0\!:\; \theta = \theta_0 \;\;\; vs. \;\;\; H_1\!:\; \theta \neq \theta_0$
  
  $$T(X) = 2(l(\hat{\theta}) - l(\theta_0)) \; \overset{H_0}{\sim}\; \chi^2_1$$
  
      \paragraph{Neyman-Pearson Test} \ \\
  
  \noindent $H_0\!:\; \theta = \theta_0 \;\;\; vs. \;\;\; H_1\!:\; \theta = \theta_1$
  
  $$T(X) = l(\theta_0) - l(\theta_1)$$
  
  \noindent For a given significance level $\alpha$, the Neyman Pearson Test is the most powerful test for comparing two estimates for $\theta$.
  
\begin{Proof}
Decision rule of the NP-Test: $\varphi^* {=} \begin{cases} 1 & \text{if } \frac{f(y;\theta_0)}{f(y;\theta_1)} \leq \mathrm{e}^c\\ 0 & \text{otherwise} \end{cases}$

\noindent Need to show: $P(\varphi(Y) {=} 1|\theta_1) \leq P(\varphi^*(Y) {=} 1|\theta_1)\; \forall \varphi$ 

\begin{align*}
\intertext{$P(\varphi^* {=} 1|\theta_1) - P(\varphi {=} 1|\theta_1) = $}
&= \int \{\varphi^*(y) {-} \varphi(y)\} f(y;\theta_1)dy\\
&\geq \frac{1}{\mathrm{e}^c} \int_{\varphi^*{=}1} \{\varphi^*(y) {-} \varphi(y)\}f(y;\theta_0)dy & f(y;\theta_1) \geq \frac{f(y;\theta_0)}{\mathrm{e}^c} \\
&+ \frac{1}{\mathrm{e}^c} \int_{\varphi^*{=}0} \{\varphi^*(y) {-} \varphi(y)\}f(y;\theta_0)dy & f(y;\theta_1) \leq \frac{f(y;\theta_0)}{\mathrm{e}^c} \\
&= \frac{1}{\mathrm{e}^c} \int \{\varphi^*(y) {-} \varphi(y)\}f(y;\theta_0)dy = 0
\end{align*}

\noindent As $\alpha = \int \varphi^*(y)f(y;\theta_0)dy = \int \varphi(y)f(y;\theta_0)dy$
\end{Proof}


\end{multicols}

%\begin{multicols}{2}[\subsection{Tests for Two Samples}][4cm]

%\end{multicols}

\begin{multicols}{2}[\subsection{Tests for Goodness of Fit}][4cm]

\paragraph{Discrete (Chi-Squared)} \ \\
  
  \noindent $H_0\!:\; X_i \sim F_0 \;\;\; vs. \;\;\; H_1\!:\; X_i \sim F \neq F_0$
  
  $$T(X) = \sum_{k=1}^K \frac{(n_k - l_k)^2}{l_k} \; \overset{H_0}{\sim}\; \chi^2_{K-1-p}$$
  
  with the following contingency table:
  
  \begin{center}
  \begin{tabular}{r|cccc}
  & 1 & 2 & & K\\
  \hline
  observed & $n_1$ & $n_2$ & ... & $n_K$\\
  expected under $H_0$ & $l_1$ & $l_2$ & ... & $l_K$\\
  \end{tabular}
  \end{center}

\noindent $l_k>5$ and $l_k>n-5$ for the $\chi^2_{K-1-p}$-distribution to hold, 

\noindent $F_0$ needs to be known, but its $p$ parameters can be estimated.

\noindent The test can be applied to discretized continuous variables. 

\paragraph{Continuous (Kolmogorov-Smirnov Test)} \ \\
  
  \noindent $H_0\!:\; X_i \sim F_0 \;\;\; vs. \;\;\; H_1\!:\; X_i \sim F \neq F_0$
  
  $$T(X) = \sup_x |F_n(x) - F_0(x;\theta)| \; \overset{H_0}{\sim}\; KS$$

\noindent with the distribution function $F(x;\theta)$ and the empirical counterpart $F_n(x) = \frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{X_i\leq x\}}$


\begin{Proof}
\vspace{-1em}
\begin{align*}
\intertext{$P(\sup_x |F_n(x) - F(x;\theta)|\leq t) = $}
&= P(\sup_y |F^{-1}(y;\theta) - x|\leq t) & \substack{x\in\left[0,1\right],\: x {=} F^{-1}(y;\theta)\\ F(F^{-1}(y;\theta);\theta){=}y}\\
&\overset{*}{=} P(\sup_y |\frac{1}{n}\sum_{i=1} \mathbbm{1}_{\{U_i\leq y\}} - y|\leq t) & \text{ with } U_i \sim U(0,1)
\end{align*}
$$^*F_n(F^{{-}1}(y;\theta)) = \frac{1}{n}\sum_{i=1}^n\mathbbm{1}_{\{X_i\leq F^{{-}1}(y;\theta)\}}  = \frac{1}{n}\sum_{i=1}^n\mathbbm{1}_{\{F(y;\theta) \leq y\}}$$
\end{Proof}

\noindent For an estimated parameter the distribution of $T(X)$ is not independent of $F_0$: $T(X) \overset{H_0}{\sim} KS$ only holds asymptotically.

\paragraph{Pivotal Statistic}

$$\begin{gathered}
g(Y;\theta) \text{ pivotal}\\
\Leftrightarrow \\
\text{distribution of } g(Y;\theta) \text{ independent of } \theta
\end{gathered}$$

\textbf{Approximative Pivotal Statistic}
\indent $H_0\!:\; X_i \sim F \text{ pivotal} \;\;\; vs. \;\;\; H_1\!:\; X_i \sim F \text{ not pivotal }$

$$g(\hat{\theta};\theta) = \frac{\hat{\theta} - \theta}{\sqrt{\textrm{Var}(\hat{\theta})}} \overset{\alpha}{\sim} \mathrm{N}(0,1)$$

with $\hat{\theta} = t(Y) \overset{\alpha}{\sim} \mathrm{N}(\theta,\mathrm{Var}(\hat{\theta}))$

$$KI = \left[ \hat{\theta} - z_{1-\frac{\alpha}{2}}\sqrt{\mathrm{Var}(\hat{\theta})}, \hat{\theta} + z_{1-\frac{\alpha}{2}}\sqrt{\mathrm{Var}(\hat{\theta})} \right]$$

\begin{Proof}
$1-\alpha \approx P \left( z_{\frac{\alpha}{2}} \leq \frac{\hat{\theta} - \theta}{\sqrt{\textrm{Var}(\hat{\theta})}} \leq z_{1-\frac{\alpha}{2}}\right)$
\end{Proof}


\end{multicols}

\begin{multicols}{2}[\subsection{Multiple Tests}][4cm]

\paragraph{Family-Wise Error Rate (FWER)} 
as $p \sim U(0,1)$

For m tests:

$$\alpha \leq P\left(\bigcup_{k=1}^m (p_k \leq \alpha)| H_{0k},k=1,...,m\right) \leq m\alpha$$

$$FWER := P(\exists k: ``H_{1k}"|\forall k:\: H_{0k})$$


\paragraph{Bonferoni Adjustment} 

$$\alpha_{B} = \frac{\alpha}{m}$$

\paragraph{\v{S}id\'{a}k Adjustment} only for independent tests

$$\alpha_{S} = 1- (1-\alpha)^{1/m}$$

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
 \alpha &\overset{!}{=} P\left(\cup_{k=1}^m (p_k \leq \alpha)| H_{0k},k=1,...,m\right)\\
 &= 1- (1-\alpha)^{1/m}
\end{align*}
\end{Proof}

\paragraph{Holm's Procedure} also takes power into account

\noindent Order the p-values: $p_{(0)} \leq...\leq p_{(m)} $

\noindent Step $x \in \{0,...,m\}$: if $p_{(x)} > \frac{\alpha}{m - x}$ reject $H_{01}$ to $H_{0x}$ and stop, else move on to step $x+1$. 

\paragraph{False Discovery Rate (FDR)} balances type I and II errors, especially for $n<<m$ problems

$$FDR = \mathrm{E}\left(\frac{\#``H1"|H_0}{\#``H1"}\right)$$

\noindent Order the p-values: $p_{(1)} \leq...\leq p_{(m)} $, choose $\alpha\in(0,1)$

\noindent j is largest index s.\,t.\ $p_{(j)} \leq \alpha j/m$, reject all $H_{0i}$ for $i\leq j$

\vspace{1em}
It can be shown that $FDR \leq m_0\alpha/m$, with $m_0 = \# H_0$

\end{multicols}



%-------------------------------------------------------------------------------

% SECTION: REGRESSION

%-------------------------------------------------------------------------------

\section{Regression}



\subsection{Models}

\begin{multicols}{2}[\subsubsection{Simple Linear Model}][4cm]



\paragraph{Theoretical Model}

$$y_i=\beta_0+\beta_1x_i+u_i$$

\paragraph{Empirical Model}

$$y_i=\hat{\beta}_0+\hat{\beta}_1x_i+e_i$$

$$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_i$$

\paragraph{Assumptions}

\begin{itemize}
\item \textbf{Independent Observations} $y_1,...y_n$ are independent
\item \textbf{Linearity of the Mean} $E(Y|x)= \beta_0 + \beta_1x$ or $\mathrm{E}(e|x)= 0$  
\item \textbf{Constant Variation} $Var(Y|x) = \sigma^2$
\end{itemize}
For the normal linear model:
\begin{itemize}
\item \textbf{Normality} $\;e|x \sim \mathrm{N}(0,\sigma^2)\;$; $\;Y|x \sim \mathrm{N}(\hat{y},\sigma^2)\;$
\end{itemize}

\paragraph{Attributes of the Regression Line}
\begin{align*}
\hat{y}_i & = \hat{\beta}_0+\hat{\beta}_1x_i  =\bar{y}+ \hat{\beta}_1(x_i-\bar{x}) \\
\hat{e}_i  & =  y_i-\hat{y}_i = y_i-(\hat{\beta}_0+\hat{\beta}_1x_i) \\
 & =y_i-(\bar{y}+ \hat{\beta}_1(x_i-\bar{x})) \\
\sum\limits_{i=1}^n\hat{e}_i & = \sum\limits_{i=1}^ny_i-\sum\limits_{i=1}^n\bar{y}-\hat{\beta}_1\sum\limits_{i=1}^n(x_i-\bar{x}) \\
 & = n\bar{y}-n\bar{y}-\hat{\beta}_1(n\bar{x}-n\bar{x})=0 \\
\bar{\hat{y}} & = \frac{1}{n}\sum\limits_{i=1}^n\hat{y}_i=\frac{1}{n}(n\bar{y}+\hat{\beta}_1(n\bar{x} - n\bar{x})) = \bar{y}
\end{align*}

\paragraph{Estimates (OLS)}

$$\hat{\beta}_1=\frac{Cov(x,y)}{Var(x)}=\frac{S_{xy}}{S_{xx}}= \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}} \cdot \sqrt{\frac{S_{yy}}{S_{xx}}}=r\sqrt{\frac{S_{yy}}{S_{xx}}}$$
\begin{Proof}
$Cov(x,y)=Cov(x,\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x})=\hat{\beta}_1Var(x)$

\raggedleft
$ \iff \hat{\beta}_1= \frac{Cov(x,y)}{Var(x)}$
\end{Proof}
$$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$

\begin{Proof}
$E\left[y\right] = E\left[\hat{\beta}_0+\hat{\beta}_1 x+\hat{e}\right] \iff \hat{\beta}_0 = E\left[y\right] - \hat{\beta}_1E\left[x\right]$
\end{Proof}

The estimates are the same as for the ML procedure.

\paragraph{Estimates (ML)} $Y|x \sim \mathrm{N}(\beta_0 + \beta_1x, \sigma^2)$

\begin{align*}
\hat{\beta}_0 &= \frac{1}{n}\sum_{i=1}^n y_i - \frac{1}{n}\sum_{i=1}^n x_i \hat{\beta}_1\\
\hat{\beta}_1 &=  \sum_{i=1}^n x_i(y_i - \hat{\beta}_0)  /\sum_{i=1}^n  x_i^2\\
\hat{\sigma}^2 &= \frac{1}{n}\sum_{i=1}^n (y_i -\hat{\beta}_0 - x_i\hat{\beta}_1)^2
\end{align*}

The $\beta$-estimates are the same as for the OLS procedure.

\begin{Proof}
\vspace{-2em}
$$l(\beta_0, \beta_1,\sigma^2) = \sum_{i=1}^n\left\{-\frac{1}{2} \sigma^2 - \frac{1}{2} \frac{(y_i-\beta_0 -\beta_1x_i)^2}{\sigma^2}\right\}$$
\end{Proof}

\end{multicols}

\begin{multicols}{2}[\subsubsection{Multivariate Linear Model}][4cm]

\paragraph{Theoretical Model}

$$Y = X\beta + u$$

\paragraph{Empirical Model}

$$Y = X\hat{\beta} + e$$
$$\hat{Y} = X\hat{\beta} $$

\noindent $y = (y_1, ... , y_n)^T$, $e = (e_1, ... , e_n)^T$, $X = \begin{pmatrix}
1 & x_{11}& \ldots&x_{1p} \\
 \vdots & \vdots&\ddots & \vdots\\
1 & x_{n1} &\ldots&x_{np}
\end{pmatrix}$

\paragraph{Assumptions}

\begin{itemize}
\item \textbf{Independent Observations} $y_1,...y_n$ are independent
\item \textbf{Linearity of the Mean} $\mathrm{E}(Y|x_{1:p})= X\beta$ or $\mathrm{E}(e|x_{1:p})= 0$ 
\item \textbf{Constant Variation} $Var(Y|x) = \sigma^2$
\end{itemize}
For the normal linear model:
\begin{itemize}
\item \textbf{Normality} $\;e_i|x_{1:p} \sim \mathrm{N}(0,\sigma^2)\;$; $\;Y|x \sim \mathrm{N}(\hat{y},\sigma^2)\;$
\end{itemize}

\paragraph{Estimates (ML)} $Y|x_{1:p} \sim \mathrm{N}(X\beta, \sigma^2)$

\begin{align*}
\hat{\beta} &= (X^TX)^{-1}X^Ty\\
Var(\hat{\beta}) &= \sigma^2(X^TX)^{-1} = I^{-1}(\beta)
\end{align*}

\begin{Proof}
$l(\beta,\sigma^2) = -\frac{n}{2}\log\sigma^2 -\frac{1}{2\sigma^2}(y-X\beta)^T(y-X\beta)$
\end{Proof}
The estimates are the same as for the OLS procedure.

\noindent $\beta$ is the \textbf{B}est \textbf{L}inear \textbf{U}nbiased \textbf{E}stimator

\begin{Proof}
Unbiased because of the Gauß-Markov Theorem:
$\mathrm{E}(\hat{\beta}) = (X^TX)^{-1}X^T\mathrm{E}(Y|X) = (X^TX)^{-1}X^TX\beta = \beta$
\end{Proof}
$$\hat{\sigma}^2 = \frac{1}{n}(y-X\hat{\beta})^T(y-X\hat{\beta});\;\;\;\;\; \hat{\beta} \sim \mathrm{N}(\beta,\sigma^2(X^TX)^{-1})$$

The ML-estimate for $\sigma^2$ is biased.

\begin{Proof}
$H{:=}X(X^TX)^{-1}X^T$ hat matrix; $HH{=}H{=}H^T$ (idempotent)
\begin{align*}
\mathrm{E}((Y{-}X\hat{\beta})^T(Y{-}X\hat{\beta})) &= \mathrm{E}((Y^T(I_n{-}H)^T((I_n{-}H)Y)\\
&= \mathrm{E}(tr(Y^T(I_n{-}H)Y)\\
&= \mathrm{E}(tr((I_n{-}H)YY^T)\\
&= tr((I_n{-}H)\mathrm{E}(YY^T))\\
&= tr((I_n{-}H)\mathrm{E}(X\beta\beta^TX^T + \sigma I_n))\\
&= \sigma^2tr((I_n{-}H))\\
&= \sigma^2 (n-p)
\end{align*}
\end{Proof}

$$s^2 = \frac{1}{n-p}(y-X\hat{\beta})^T(y-X\hat{\beta});\;\;\;\;\; \hat{\beta} \sim t_{n-p}(\beta,s^2(X^TX)^{-1})$$

with $s$ an unbiased estimator

\end{multicols}

%\subsubsection{Spezialfall: Zeitreihen}

\begin{multicols}{2}[\subsubsection{Bayesian Linear Model}][4cm]

\paragraph{Prior} flat prior\ \\

$$f_{\beta,\sigma^2} (\beta,\sigma^2) = \frac{1}{\sigma^2}$$

\paragraph{Posterior} \ \\

\noindent Resulting posterior: $$f_{post}(\beta,\sigma^2|y)\propto (\sigma^2)^{-\frac{n}{2}+1} \mathrm{e}^{-\frac{1}{2\sigma^2} (y{-}X\beta)^T(y{-}X\beta)}$$

Note: $f_{post}(\beta,\sigma^2|y) = f(\beta|\sigma^2,y)f(\sigma^2|y)$



\begin{align*}
\beta|\sigma^2,y &\sim \mathrm{N}\left(\hat{\beta}, \sigma^2(X^TX)^{-1}\right)\\
\sigma^2|y &\sim \mathrm{IG}\left(\frac{n-p}{2},\frac{s^2(n-p)}{2}\right)\\
\beta|y &\sim t_{n-p}\left(\hat{\beta}, s^2(X^TX)^{-1}\right)
\end{align*}
\vspace{1em}

\noindent The two distributions for $\beta$ mirror the results for $\hat{\beta}$ in the linear model. 

\end{multicols} 

\begin{multicols}{2}[\subsubsection{Quantile Regression}][4cm]

\paragraph{Prediction Interval} range of $1-\alpha$ fraction of the data

$$Var(\hat{Y}|x_{1:p}) = Var(X\hat{\beta}) + \sigma^2$$

\noindent Determined by estimation variance (usually captured by confidence intervals) plus residual variance.

\paragraph{Quantile}

$$Q(\tau) = \inf \{y:F(y)\geq\tau\}$$

If $F$ is invertable: $Q(\tau) =F^{-1}(\tau)$, $\tau\in (0,1)$


\paragraph{Model}

$$Q(\tau|x_{1:p}) = X\beta$$


\noindent For median regression: $\hat{\beta} = \text{arg } \min \sum_{i=1}^n \left| y_i - x_i^T\beta\right|$

In general: $$\hat{Q}(\tau) = \text{arg } \underset{\beta}{\min} \left( \sum_{i=1}^n \delta_\tau (y_i - x_i^T\beta)\right)$$
\indent with check function $\delta_\tau(y) = y\left(\tau - \mathbbm{1}_{\{y<0\}}\right)$

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
Q(\tau) &=\text{arg }\underset{q}{\min}\:\mathrm{E}(\delta_\tau(Y-q))\\
&= 
\text{arg }\underset{q}{\min} \left\{ (\tau{-}1)\!\!\int\displaylimits_{-\infty}^q (y{-}q)f(y)dy + \tau\!\! \int\displaylimits_{q}^\infty (y{-}q)f(y)dy\right\}
\end{align*}
Differentiating w.r.t. $q$ gives $(\tau{-}1)\!\!\int\displaylimits_{-\infty}^q f(y)dy - \tau\!\! \int\displaylimits_{q}^\infty f(y)dy$

\noindent $= (1{-}\tau)F(q) - \tau(1{-}F(q) = F(q) -\tau$
\end{Proof}

\paragraph{Estimates} \ \\

\noindent The estimates for $\beta$ can be computed with linear programming and are normally distributed with mean $\beta$.

\end{multicols}

\begin{multicols}{2}[\subsubsection{Flexible Regression}][4cm]


\paragraph{Assumptions}

\begin{itemize}
\item \textbf{Independent Observations} $y_1,...y_n$ are independent
\item \textbf{Constant Variation} $Var(Y|x) = \sigma^2$
\item \textbf{Normality} $\;e_i|x_{1:p} \sim \mathrm{N}(0,\sigma^2)\;$; $\;Y|x \sim \mathrm{N}(\hat{y},\sigma^2)\;$
\end{itemize}

\paragraph{Knot Placement}

\begin{itemize}
\item equidistant
\item based on quantiles (more structure where data is dense)
\item all data points plus penalization
\end{itemize}


\paragraph{Penalized Regression Splines}

$$|| y {-} X\beta||^2 + \lambda \int_{x_1}^{x_n} \left[ f''(x)\right]^2dx= || y {-} X\beta||^2 + \lambda \beta^TD \beta$$

$l_p(\beta,\sigma^2,\lambda) =  l(\beta,\sigma^2) - \frac{\lambda}{2\sigma_\epsilon^2} \beta^T D\beta$

$\hat{\beta} = (X^TX+\lambda D)^{-1}X^Ty$

\paragraph{Difference Penalty} 

\begin{itemize}
\item first order: $\beta^T D\beta = \sum_{j=1}^p (\beta_{j+1} - \beta_j)^2$
\item second order: $\beta^T D\beta = \sum_{j=1}^p (\beta_{j+1} - 2\beta_j + \beta_{j-1})^2$
\end{itemize}


\paragraph{Choosing $\boldsymbol{\lambda}$} Model complexity


$$\mathrm{dim}(\lambda) = tr\left\{(X^TX + \lambda D)^{-1} (X^TX)\right\} $$\vspace{0.1em}

$AIC(\lambda) = fit(\lambda) + 2\mathrm{dim}(\lambda)$

Numerically complex. Alternative: \textbf{Bayes}

$\beta \sim \mathrm{N}(0,\sigma_\beta^2D^-)$ with $(D^-)^- = D$ (generalized inverse)

$$\log f(\beta, \sigma^2;\sigma_\beta^2|y) \propto l(\beta,\sigma^2) - \frac{rk(D^-)}{2}\log(\sigma^2_\beta) - \frac{1}{2\sigma_\beta^2}\beta^TD^-\beta$$

\noindent As $\lambda = \frac{1}{\sigma_\beta^2}$, marginal posterior for $\sigma_\beta^2$ can be derived. E.\,g.\ set $\lambda$ to the posterior mode estimate.

\end{multicols}

\begin{multicols}{2}[\subsubsection{Generalized Regression}][4cm]

\paragraph{Assumptions}
\begin{itemize}
\item \textbf{Independent Observations} $y_1,...y_n$ are independent
\item \textbf{Linearity of the Mean} $\mathrm{E}(Y|x_{1:p})= X\beta$ or $\mathrm{E}(e|x_{1:p})= 0$ 
\item \textbf{Exponential Family} $ Y|x \sim \exp\left\{t(y)\theta(x) - \kappa(\theta(x))\right\}h(y)$
\end{itemize}

\paragraph{Link Function} \ \\

\noindent Linear predictor $\eta = X\beta$; $\mu = \frac{\partial \kappa(\theta)}{\partial \theta}= \mathrm{E}(t(Y);\theta)$

$$\mu = g^{-1}(\eta)$$

If $\lambda = 0$, \textit{canonical link}:

$$\theta = \eta$$

\begin{itemize}
\item score function: $s(\beta) = X^T\left(t(y) - \mathrm{E}(t(Y);\eta)\right)$
\item estimate $\hat{\beta} =X^T\mathrm{E}(t(Y);\hat{\eta}) = X^Tt(y)$
\item Fisher matrix $I(\beta) = X^TWX$ \\with $W$ diagonal and $W_{ii} = \frac{\partial^2 \kappa(\eta_i)}{\partial \eta^2} = Var(t(Y_i), \eta_i)$
\end{itemize}

\noindent Examples:
\begin{itemize}
\item \textbf{Logistic}: $\text{logit} P(Y_i{=}1|x_i) = \log\frac{P(Y_i{=}1|x_i)}{1{-}P(Y_i{=}1|x_i)} = \eta$ \\
$Var(Y_i|x_i) = P(Y_i{=}1|x_i)\cdot(1{-}P(Y_i{=}1|x_i))$

\item \textbf{Poisson}: $log \mathrm{E}(Y_i|x_i) = \eta$ \\
$Var(Y_i|x_i) = \mathrm{E}(Y_i|x_i) = e^{\eta}$
\end{itemize}


\end{multicols}

\begin{multicols}{2}[\subsubsection{Weighted Regression}][4cm]

\paragraph{Different Precision} variance heterogeneity: $e_i \sim \mathrm{N}(0,\sigma_i^2)$

$$l(\beta,\sigma^2) = -\frac{n}{2} \log \sigma^2 - \frac{1}{2\sigma^2}(y-X\beta)^TW(y-X\beta)$$

with $W=diag(\frac{1}{a_1},...,\frac{1}{a_n})$ and $a_i = \frac{\sigma_i^2}{\sigma^2}$

\begin{align*}
\hat{\beta}_{ML} &= (X^TWX)^{-1}(X^TWy) \\
Var(\hat{\beta}_{ML}) &= \sigma^2(X^TWX)^{-1}
\end{align*}


\paragraph{Different Group Representation}

$$Y_i|x_{i,1:p}, z_i \sim \mathrm{N}(x_{i,1:p}\beta_{z_i},\sigma^2)$$

with $z_i$ indicating group affiliation


\end{multicols}





\subsection{Goodness of Fit}

\begin{multicols}{2}[\subsubsection{Coefficient of Determination}][4cm]

$$R^2=\frac{SS_{Explained}}{SS_{Total}}=1-\frac{SS_{Residual}}{SS_{Total}}=r^2$$

Range: $0 \le R^2 \le 1$

\end{multicols}

%-------------------------------------------------------------------------------

% SECTION: CLASSIFICATION

%-------------------------------------------------------------------------------

%\section{Classification}

%\subsection{Diskriminant Analysis (Bayes)}


%-------------------------------------------------------------------------------

% SECTION: CLUSTER ANALYSIS

%-------------------------------------------------------------------------------

%\section{Cluster Analysis}

%-------------------------------------------------------------------------------

% SECTION: SURVIVAL ANALYSIS

%-------------------------------------------------------------------------------

\section{Survival Analysis}

\begin{multicols}{2}[\subsection{Basics}][4cm] 

\paragraph{Time-to-Event Data} outcome tuple $(t_i, \delta_i)$ is observed 
 
\noindent with time $0<t<\infty$ and event/censoring indicator $\delta_i$ 

\vspace{0.5em}
\noindent typically: $\delta_i=  \left\{
\begin{array}{rl}
1, & i\text{ experienced event} \\
0, & i\text{ is (right-)censored} \\
\end{array}
\right. $ 



\paragraph{Censoring}

$T_i \sim F$ times to event \& $C_i \sim G$  censoring times

\begin{itemize}[itemsep=-0.3em]
\item \textbf{right-censoring:} observe $t_i = \min(T_i, C_i)$
\item \textbf{left-censoring:} observe $t_i = \max(T_i, C_i)$
\item \textbf{interval-censoring:} only known $t_i \in \left[C_i^l, C_i^r\right]$
\end{itemize}

\noindent Censoring and event distributions need to be independent given  explanatory variables for models to be unbiased



\paragraph{Truncation} a form of sampling bias

\begin{itemize}[itemsep=-0.3em]
\item \textbf{left-truncation:} observations with $t_i < a$ excluded
\item \textbf{right-truncation:} observations with $t_i > a$ excluded
\end{itemize}

\paragraph{Description of the Distribution} 

\begin{itemize}
\item \textbf{survival function}  $$S(t) := P(T>t) = 1 - F(t) $$
\item \textbf{hazard rate} $$h(t) := \lim_{\delta \rightarrow 0} \frac{P(T \in [t,t+\delta]|T\geq t)}{\delta} = \frac{f(t)}{S(t)}$$
\item \textbf{cumulative hazard rate} $$H(t) := \int_0^t h(u)du = -\log(S(t))$$
\end{itemize}

%\paragraph{Log Rank Test} for comparing survival 
%Ein Nachteil davon ist, dass dieser bei nicht-proportionalen Hazards eine geringe Trennschärfe hat.

\end{multicols}


\begin{multicols}{2}[\subsection{Modelling}][4cm] 

\paragraph{Kaplan-Meier} non-parametric estimate for $S(t)$

\noindent For ordered event times $t_{(1)}, ..., t_{(f)}$:
$$\hat{S}\left(t_{(f)}\right) = \prod_{i=1}^f \hat{P}\left(T>t_{(i)}|T\geq t_{(i)}\right)$$
$\hat{P}\left(T>t_{(i)}|T\geq t_{(i)}\right) = 1-\frac{d_i}{n_i}$, with $d_i$ number of events at $t_{(i)}$  and $n_i$ number of observations under risk at $t_{(i)}$
\begin{itemize}[itemsep=-0.5em]
\item also called product-limit estimator 
\item formula closely related to path rule
\item assumes piecewise constant survival (step function)
\end{itemize}

\textbf{Greenwoods' Formula} basis for confidence intervals
$$\widehat{Var}\left(\hat{S}(t)\right) = \hat{S}(t)^2\sum_{t_{(k)} \leq t} \frac{d_k}{n_k(n_k-d_k)}$$
\paragraph{Nelson-Aalen} non-parametric estimate for $H(t)$
$$H(t) = \sum_{k:t_{(k)} \leq t} \hat{h}_k^d$$
\noindent with $h_k^d$ discrete time hazard rate for $k$-th interval $\left[t_{(k-1)}, t_{(k)}\right]$
\begin{itemize}[itemsep=-0.5em]
\item results in piecewise constant estimate (step function)
\item $\exp\left(-\hat{H}(t)\right)$ gives Breslow estimate for $S(t)$
\end{itemize}

\paragraph{Accelerated Failure Time} parametric modelling of $T$ 
\begin{itemize}[itemsep=-0.5em]
\item[] $T\sim \mathrm{WB}(\lambda, k)$: leads to proportional hazards
\item[] $T\sim \mathrm{Exp}(\lambda) = \mathrm{WB}(\lambda, 1)$: results in constant hazard rate
\end{itemize}
Location-Scale-Model (with $\lambda=\exp(-\eta)$ and $k=\frac{1}{\sigma}$): 
\begin{align*}
\log(T) &= x^T\beta + \sigma\epsilon \hspace{1em} \Longleftrightarrow \\
T &= \exp(\beta_0)\exp(\beta_1x_1)...\exp(\beta_px_p)\exp(\sigma\epsilon)
\end{align*}

\noindent incrementing $x_1$ means survival time stretches by $\exp(\beta_{1})$, c.\,p.

\vspace{0.3em}
\textbf{Diagnostics}\vspace{-0.3em}
\begin{itemize}[itemsep=-0.5em]
\item $\ln(-\ln S(t)) = \ln(\lambda) + \sigma\ln(t)$, use $\hat{S}_{KM}$ for graphical check (for single categorical $x$ only)
\item Distribution assumption (for Weibull): $\epsilon \sim \mathrm{Gumbel}\left(\frac{y-\eta}{\sigma}\right)$
\end{itemize} 

\vspace{0.3em}
\textbf{ML estimation} under right-censoring \vspace{-0.3em}
$$L(\theta) = \prod_{i=1}^n \left(f(t_i|\theta,x_i)\right)^{\delta_i} \left(S(t_i|\theta,x_i)\right)^{1-\delta_i}$$

standard likelihood theory applies

\paragraph{Cox Proportional Hazards} semi-parametric
$$h(t,x) = h_0(t) \cdot\exp (x^T\beta)$$

\noindent with non-parametric baseline hazard rate $h_0(t)$ independent of $x$, but can depend on (part of) $x$ in stratified models

\vspace{0.3em}

\textbf{Proportional Hazards}
incrementing $x_1$ means hazard rate 

multiplies by $\exp(\beta_{1})$, ceteris paribus
\begin{Proof}
\vspace{-2.7em}
$$\frac{h(t|x^{(a)})}{h(t|x^{(b)})} = \exp((x^{(a)}-x^{(b)})^T\beta)$$
\end{Proof}

\textbf{Diagnostics}
\begin{itemize}[itemsep=-0.5em]
\item \textbf{Proportional Hazard:} Schoenfeld residuals ($E(r_i)=0$) and corresponding  test ($H_0:$ proportional hazards)
\item \textbf{Overall Fit:} Cox Snell residuals $\sim \mathrm{Exp}(1)$
\item \textbf{Functional Form:} Martingale residuals ($E(r_i)=0$)
\item \textbf{Outliers:} Deviance residuals ($E(r_i)=0$)
\item \textbf{Leverage:} Jack-Knife residuals
\end{itemize}

\textbf{Partial Likelihood} consider risk sets $R_i = R(t_{(i)})$ for 

ordered event times $t_{(1)}, ..., t_{(f)}$:
\begin{align*}
PL(\beta) &= \prod_{i=1}^m P(t_i=t_{(i)}|\exists j\in R_i:t_j=t_{(j)})\\
&= \prod_{i=1}^m \frac{\exp\left(x^T_{(i)}\beta\right)}{\sum_{j\in R_i}\exp\left(x^T_{j}\beta\right)}
\end{align*}



\paragraph{Random Survival Forests} Machine Learning method
\begin{enumerate}[itemsep=-0.5em]
\item draw $B$ Bootstrap samples from the data
\item grow binary decision tree for each sample finding splits with log rank tests out of $p$ randomly drawn splitting candidates
\item stopping: minimum $d_0 > 0$ deaths per node
\item calculate $\hat{H}(t)$ via Nelson-Aalen and average for samples 
\item get prediction error using Out-Of-Bag samples
\end{enumerate}


\end{multicols}

\begin{multicols}{2}[\subsection{Model Comparison}][4cm] 

\paragraph{General} Some measures (e.\,g.\@ AIC) require equal likelihood estimation and are typically only valid within a model class. 

\noindent Other methods (presented below) should be used on test data.

\paragraph{C-Index} compare predictions if censor times allow it

$$C=\frac{\text{\#concordant pairs}}{\text{\#comparable pairs}}$$

with $0\leq C\leq 1$ and $C=0.5$ for random guessing

\paragraph{Brier Score} equivalent to the MSE for predicted probabilities

$$\widehat{BS}(t,\hat{S})=\frac{1}{n}\sum_{i=1}^n(\tilde{Y}_i(t)- \hat{S}(t|x_i))^2$$

with $\tilde{Y}_i(t)= \mathbbm{1}(t_i > t)$ and $t_i =\min (T_i, C_i)$

\noindent the arithmetic mean above is often weighted using Inverse Probability of Censoring Weights

\paragraph{Integrated Brier Score} as the Brier Score depends on $t$

$$IBS(\tau)=\frac{1}{\tau}\int_0^\tau \widehat{BS}(u,\hat{S}) du$$

\noindent can also be shown graphically as the Brier Score over different time points in so-called prediction error curves (PEC)


\end{multicols}
%-------------------------------------------------------------------------------

% SECTION: SPATIAL STATISTICS

%-------------------------------------------------------------------------------

\section{Spatial Statistics}

\begin{multicols}{2}[\subsection{Markov Random Field}][4cm] 
\paragraph{Lattice Data} observations from a random process observed over spatial regions,  supplemented by a neighborhood structure

\paragraph{Random Field} synonymous to stochastic process
$$(X_1,...,X_n)$$
realisations of random variable $X_i$ 
for locations $D=\{1,...,n\}$

\paragraph{Markov Properties} extend the temporal\,$X_{t{+}1}|X_t \perp X_{1,...,t-{}1}$
\begin{itemize}
\item \textbf{pairwise:} $X_i|X_{-ij}\perp X_j$ for $i\neq j$ and $i \not\sim j$
\item \textbf{local:} $X_i|X_{-\{i,n\in \partial (i)\}}\perp X_{n\in \partial (i)}$ with $\partial (i) := \{j:j\sim i\}$
\item \textbf{global:} $X_A|X_B\perp X_C$ for C separating $A$ and $B$
\end{itemize}
\noindent global implies local implies pairwise; equivalent in a GMRF

\paragraph{Neighbourhood System} set of neighbourhoods
$$\partial = \{\partial(s): s \in D\}$$
\noindent if $s \not\in \partial(s)$ (antireflexive) and $v \in \partial(s) \Leftrightarrow s \in \partial(v)$ (symmetric)

\paragraph{GMRF} Gauss Markov Random Field
$$f(x) =  ( 2\pi)^{-\frac{n}{2}}|P|^{\frac{1}{2}} \exp\left(-\frac{1}{2}(x-\mu)^TP(x-\mu)\right)$$
\noindent with precision matrix $P:=\Sigma^{-1}>0$ and $p_{ij} \neq 0 \Leftrightarrow i\sim j$

\noindent $p_{ij}$ define neighbourhoods with $X_i|X_{-ij} \perp X_j \Leftrightarrow p_{ij}{=}0$ for $i\neq j$

\noindent GMRFs fulfill the pairwise, local, and global Markov property

\textbf{canonical representation:}
$$X \sim N^k(m,P) :=N(P^{-1}m,P^{-1})$$ 
$$ \Leftrightarrow f(x) \propto |P|^{\frac{1}{2}}\exp\left(-\frac{1}{2} x^TPx +m ^Tx\right)$$

get $m$ via  completing the square

\textbf{improper GMRFs:} $P$ s.\,p.\,d.\@ with $\mathrm{rk}(P)=r<n$
$$f(x) =  ( 2\pi)^{-\frac{r}{2}}\det \!{}^{\ast}(P)^{\frac{1}{2}} \exp\left(-\frac{1}{2}(x-\mu)^TP(x-\mu)\right)$$

with $\det \!{}^{\ast}(P) = \prod_{j=1}^r \lambda_j$ with $\lambda_1,...,\lambda_r$ non-zero eigenvalues

\textbf{\textit{intrinsic}} GMRFs ($\sum_{j=1}^n p_{ij} = 0 \,\,\, \forall i=1,...,n$)  with

irregular grids: rank decline is number of independent areas

\textbf{models:}
\begin{itemize}
\item Conditional Autoregressive (\textbf{CAR}): $\mathrm{E}(X_i|X_{-i}) = \mu_i + \sum_{j:i\sim j} c_{ij}(X_j-\mu_j)$, resulting in  a GMRF with $p_{ij} =  \left\{
\begin{array}{rl}
{-}c_{ij}/(\sigma_i\sigma_j), & i\neq j \\
1/\sigma_i, &  i=j \\
\end{array}
\right. $ \\
Weights $c_{ij}$ are symmetric and often correspond to distance 

\item Intrinsic Autoregressive (\textbf{ICAR})
$p_{ij} =  \left\{
\begin{array}{rl}
{-}1/(\sigma_i\sigma_j), & i \sim j \\
|\partial(i)|/\sigma_i, &  i=j \\
0, &  \text{else}\\
\end{array}
\right. $ \\
\item Simultaneous Autoregressive (\textbf{SAR})

$\mathrm{E}(X_i|X_{-i}, Z)  =Z_i\beta + \sum_{j:i\sim j} b_{ij}X_j$ with covariates $Z$
\end{itemize}



\paragraph{GMRF Regression} Bayesian prior equals likelihood penalty 
$$Y = X \beta + Z \eta + \epsilon$$

with  $\epsilon_i \sim \mathrm{N}(0, \sigma^2)$; $\eta \sim \mathrm{N}(0, (\tau Q)^{-1})$; $\beta_k \sim N (0, \kappa_k )$

\noindent get hyperparameters: cross validation, empirical Bayes, or Bayes 

\noindent Gamma is non-informative, semi-conjugated prior on $\tau$, $\frac{1}{\sigma^2}$, \& $\frac{1}{\kappa_k}$ 




\paragraph{Drawing from a GMRF} Rue algorithm
\begin{enumerate}[itemsep=-0.5em]
\item perform Cholesky decomposition: $P=LL'$ 
\item calculate $w$ and $u$ with $Lw=m$ and $L'u=w$
\item draw $z\sim\mathrm{N}(0,I_n)$ and calculate $v$ with $L'v=z$
\item calculate result $y=u+v$
\end{enumerate}

\begin{Proof}
$u=P^{-1}m$, as $m=Lw=LL'u=Pu  $ \\
$E(v)=E(L'^{-1}z)=L'^{-1}E(z)=0$ \\
$Var(v)=Var(L'^{-1}z){=}(L'^{-1})^2 Var(z){=}(L''L')^{-1}I=P^{-1}$ \\
Therefore $y = u+v\sim \mathrm{N}(mP^{-1}, P^{-1})$.
\end{Proof}

\paragraph{latent GMRF} solution if target is not Gaussian

hierarchical model with $\tau$ as smoothing parameter:
\begin{align*}
Y_i | \eta_i &\overset{iid}{\sim} F(\eta_i) \,\,\, \text{ e.\,g.\@ Poisson for count data}\\
\eta &\sim GMRF(\tau Q)
\end{align*}

\textbf{Inference} with MCMC, with a penalized Likelihood, or \\
with INLA (fully Bayesian using approximations)

\paragraph{Ising Model}
 $X_i \sim \mathrm{B}(1, \pi_i)$, $x_i  \in \{-1,1\}$, e.\,g.\@ black/white
 
 Minimize:
 $$H(f,x) = -c\sum_{i\in I}f_ix_i -\beta\sum_{i\sim j}f_if_i -h\sum_{i\in I}f_i$$
\begin{itemize}[itemsep=-0.3em]
\item $-\sum_{i\in I}f_ix_i$ $\propto$ Hamming distance
\item $\sum_{i\sim j}f_if_i$ penalty for differing neighbors
\item $\sum_{i\in I}f_i$ to achieve a balance between values (optional)
\end{itemize}

\textbf{Bayesian View}\vspace{-0.8em} 
\begin{itemize}[itemsep=-0.4em]
\item ${-}\sum_{i\in I}f_ix_i =-l(f|x)$ for multiplicative noise ($x_i=f_i\eta_i$, with $\eta_i$ flip indicator), the rest of the formula is the prior
\item \textbf{Perfect Sampling:} Do MCMC from all possible start values. Combine paths with same status. Once all paths are combined, distribution is independent of starting value.
\end{itemize}

\textbf{Potts Model} as extension to categorical data

\end{multicols}

\begin{multicols}{2}[\subsection{Geostatistics}][4cm] 


\paragraph{Spatially Continuous Random Field} random variables $Y_S$

$$Y_S:(\mathbb{R}^d, \mathcal{B}^d,\mathbb{P}) \rightarrow (Z, \mathcal{F})$$

i.\,e.\@ probability space $\rightarrow$ status space (including $\sigma$-Algebra)


\paragraph{Variogram} typically plot over distance $|s-s'|$
$$\gamma(s,s'):= \frac{1}{2}\mathrm{Var}(Y(s) -Y(s'))$$

\textbf{Covariogram} $c(s,s'):= \mathrm{Cov}(Y(s),Y(s'))$ ($=$ covariance)

It holds: $\gamma(s,s') =\frac{1}{2}\mathrm{Var}(Y(s)) +\frac{1}{2}\mathrm{Var}(Y(s')) -c(s,s')$


\paragraph{Gauss Random Field} $Y=\{Y(s),s\in D \subseteq \mathbb{R}^d\}$
$$\forall s_1,...,s_n: (Y(s_1),...,Y(s_n))^T \text{  multivariate normal}$$
uniquely defined by $\mu(s)$ and $c(s,s')$

\paragraph{Stationarity} strong $\Rightarrow$ weak $\Rightarrow$ intrinsic
\begin{itemize}[leftmargin=6.1em,itemsep=-0.5em]
\item[\textbf{intrinsic}] $\mathrm{E}(Y(s)) = \text{const.}$ and $\gamma(s,s')=\gamma(\Delta s)$
\item[\textbf{weak}] $\mathrm{E}(Y(s)) =  \text{const.}$ and $c(s,s') = c(\Delta s) \,\forall s,s' \in S$
\item[\textbf{strong}] all $n$-dimensional distributions translation invariant
\end{itemize}
For stationary processes: \textbf{Correlogram} $\rho(\Delta s):=\frac{c(\Delta s)}{\sigma^2}$,
$c(0)=\sigma^2 = \mathrm{Var}(Y(s))$ nugget effect, and $\gamma$, $c$, $\rho$ symmetric;
$\gamma(\Delta s) = c(0)-c(\Delta s) = \sigma^2(1-\rho(\Delta s))$

\paragraph{Isotropy} compare directional variograms
$$c(s,s') = c(\| s-s'\|) \Leftrightarrow Y \text{ isotrop}$$

\begin{itemize}[itemsep=-0.5em]
\item \textbf{geometric anisotropy} same sill, different range
\item \textbf{zonal anisotropy} different sill
\end{itemize}

\paragraph{Empirical Variogram} estimated via kernel functions and nearest neighbours (similar to gliding histograms)

\hspace{2em}
\begin{tikzpicture}[scale=.6,cap=round,
tangent/.style={%
in angle={(180+#1)} ,
Hobby finish ,
designated Hobby path=next , out angle=#1,
}]
 \tikzset{axes/.style={}}
 % The graphic
\begin{scope}[style=axes]
 \draw[->] (0,0) -- (6,0) node[below left] {\tiny $\|  s{-}s'\| $};
 \draw[->] (0,0)-- (0,3.5) node[left] { \tiny  $\gamma(s,s')$};
 %\draw [gray!50]  (-5,3) -- (-2.5,1) -- (2.5,4) -- (5,-1);
 \draw (0,1) parabola[bend at end] (4,3) -- (6,3);
 \draw[<->] (-0.3,0.1) -- (-0.3,0.9) node[left,pos=0.5] {Nugget};
 \draw[<->] (-0.3,1.1) -- (-0.3,2.9) node[left,pos=0.5, align=right] {Partial\\Sill};
 \draw[<->] (4,0.1) -- (4,2.9) node[right,pos=0.5] {Sill};
 \draw[<->] (0.1,0.2) -- (3.8,0.2) node[above,pos=0.5] {Range};
 \draw[fill=black] (0,0) circle (1.7pt);
 \draw[fill=white] (0,1) circle (2pt);
\end{scope}
\end{tikzpicture}\vspace{-1em}


\paragraph{Parametric Models for Correlograms}
\begin{itemize}[itemsep=-0.3em]
\item \textbf{Exponential} $\rho(h)=\exp(-(h/\phi))$
\item \textbf{Gauss} $\rho(h)=\exp(-(h/\phi)^2)$
\item \textbf{Spherical} $\rho(h)=  \left\{
\begin{array}{rl}
1-\frac{2}{3}(h/\phi)+\frac{1}{2}(h/\phi)^3, & 0\leq h\leq \phi \\
0, &  h\geq\phi \\
\end{array}
\right. $ 
\item \textbf{Mat\'{e}rn} $\frac{1}{2^{\kappa -1}\Gamma(\kappa)}(h/\phi)^\kappa K_\kappa(h/\phi)$ with $K_\kappa$ Bessel function
\end{itemize}

\paragraph{Simple Kriging} assumes known expectation and $c(s,s')$
$$Y(s_i) = Z(s_i) + \epsilon(s_i)$$
with random field $Z$ with $\mu(s)$ and $c(s,s')=\sigma^2 \rho(s,s')$ 

\noindent and $\epsilon(s_i)$ i.i.d.\@ with expectation $0$ and variance $\sigma_\epsilon^2$

minimizing MSPE $\min \sigma_Y^2(s_0) = c(s_0,s_0)-c(s_0)'C_Y^{-1}c(s_0)$:
\begin{align*}
\hat{Z} (s_0) &=\mu(s_0)+c(s_0)'C_Y^{-1}(Y-\mu_Y)
\end{align*}
\begin{itemize}[leftmargin=6.1em,itemsep=-0.5em]
\item[\textbf{ordinary}] constant unknown mean $\mu(s)=\mu$
\item[\textbf{universal}] known beta in $\mu(s)=x(s)'\beta$
\end{itemize}

\paragraph{Gauss Model Kriging} same estimates as for simple kriging \\

\noindent Equivalent Approaches:\\
\noindent additive: $Y(s)=\mu(s)+Z(s)+\epsilon(s)$, with $Z(s)$ as GRF($0,\tau^2$)
hierarchical: $Y(s)|Z(s),\sigma^2 \sim \mathrm{N}(Z(s), \sigma^2)$, $Z(s)$ as GRF($\mu(s),\tau^2$)

\vspace{0.5em}
\textbf{Inference:}
\vspace{-0.7em}
\begin{itemize}[itemsep=-0.3em]
\item ML: via profile-log-likelihood, but biased
\item REML: via restricted (or marginal) likelihood, less biased
\item fully Bayesian: MCMC but how to choose priors
\end{itemize}


\paragraph{Kriging Procedure} \ \\
\vspace{-2em}
\begin{multicols}{2}
\begin{enumerate}[itemsep=-0.3em]
\item variogram
\item fit correlation function
\item test stationarity/isotropy
\item if so, transform for isotropy
\item Kriging
\item residual variogram
\item geoadditive regression
\item maybe generalized Kriging
\end{enumerate}
\end{multicols}

\paragraph{Geoadditive Regression}
$$Y_i=x_i\beta + \sum_{j=1}^pf_j(z_{ij})+f_{geo}(s_i)+\epsilon_i$$
Modelling $f_{geo}(s)=z_{geo}\gamma_{geo}$ with $\gamma_{geo}\sim\mathrm{N}\left(0, (\tau_{geo}K_{geo})^{-1}\right)$:
\vspace{-0.5em}
\begin{itemize}[itemsep=-0.3em]
\item \textbf{lattice data:} $f_{geo}=\phi_K$ for $s_i=K$ and GMRF pior
\item \textbf{geostatistical data:} $f_{geo}=Z\gamma_{geo}$ using radial basis functions or 2D-splines
\end{itemize}
$$Y=X\beta + \sum_{j=1}^{p,geo}Z_j\gamma_j+\epsilon \:\:\:\: \text{  with  } \gamma_j \sim \mathrm{N}\left(0, (\tau_jK_j)^{-1}\right)$$

%\noindent $K_j$ does not have full rank, can be viewed as a mixed model
\noindent with inference via REML, cross validation or fully Bayesian

\textbf{INLA} (Integrated Nested Laplace Approximation):

2 Laplace approximations for fully Bayesian procedure

\noindent AIC is impossible for spatial, but DIC works

\end{multicols}

\begin{multicols}{2}[\subsection{Point Processes}][4cm] 

\paragraph{Spatial Point Process} countable set of locations of events

$$N={s_1,...,s_n, ...} \subset S \subseteq \mathbb{R}^d$$

\noindent equivalent to $P(s_1 \in B_1, ..., s_n \in B_n)$ for any Borel sets $B_i \subseteq S$ or as counting process $P(N(B_1)=n_1, ..., N(B_k)=n_k)$ with $N(B)<\infty$ and bounded $B$ for any $k$

\noindent k-dimensional distributions are determined by void probabilities:
$$P(N(B)=0)$$

\paragraph{Intensity} first ($\lambda$) and second ($\lambda_2$) order

$$\lambda(s):=\lim_{|ds|\rightarrow 0} \frac{\mathrm{E}(N(ds))}{|ds|}$$

\noindent with $ds$ Borel set around $s$; similar to hazard rate

$$\lambda_2(s,u):=\lim_{|ds|,|du|\rightarrow 0}\frac{\mathrm{E}(N(ds)\cdot N(du))}{|ds| \cdot |du|}$$

\noindent $\lambda$ and $\lambda_2$ determine the first two moments of $N(B) \forall B\subseteq\mathbb{R}^d$

\paragraph{Stationarity} 
\vspace{-0.5em}
\begin{itemize}[leftmargin=5em,itemsep=-0.5em]
\item[\textbf{weak}] $\lambda(s)=\text{const.}$ and $\lambda_2(s,u)=\lambda_2(s+\Delta,u+\Delta) \forall \Delta$
\item[\textbf{strong}] distributions are translation invariant
\end{itemize}

\paragraph{Isotropy}
\vspace{-0.5em}
\begin{itemize}[leftmargin=5em,itemsep=-0.5em]
\item[\textbf{weak}] $\lambda_2(s,u)=\lambda_2(\|s-u\|)$ rotation invariant
\item[\textbf{strong}] distributions are rotation invariant
\end{itemize}


\paragraph{Point Patterns}
\vspace{-0.8em}
\begin{itemize}[itemsep=-0.5em]
\item complete spatial randomness (CSR)
\item aggregated/clustering
\item regular/grid
\end{itemize}
Can be tested via $\chi^2$-Goodness-of-fit for discretized $S$

\paragraph{Binomial Process} overlay of $n$ Bernoulli Processes

$$P(s_1\in B_1,...,s_n \in B_n)=\frac{|B_1|\cdot...\cdot |B_n|}{|S|^n}$$

\noindent with Bernoulli Processes: $N=\{s\}$, with $P(s\in B)=\frac{|B|}{|S|}$

\noindent As $\lambda(s)=\lambda$, the Binomial Process is homogeneous.

\begin{Proof}
\vspace{-1em}
$$\lambda(s)=\lim_{|ds|\rightarrow 0} \frac{\mathrm{E}(N(ds))}{|ds|} = \lim_{|ds|\rightarrow 0} \frac{n|ds|/|S|}{|ds|} = \frac{n}{|S|}=\:\lambda$$
\end{Proof}

\noindent However, $N(B)$ and $N(S\setminus B)$ are dependent


\paragraph{Poisson Process} \ \\

\noindent \textit{homogeneous if:} $N(B) \sim \mathrm{Po}(\lambda\cdot |B|)$  with $0<\lambda<\infty$

\noindent results in CSR and void probability $P(N(K)=0)=\exp(-\lambda|K|)$



\noindent \textit{inhomogeneous if} $\lambda(s)$ varies:

$N(B) \sim \mathrm{Po}(\mu(B))$  with $\mu(B)=\int_B\lambda(s)ds$ 


\noindent Both have independent scattering: $N(B_1) {\perp} N(B_2)$ for $B_1 {\cap} B_2 {=} \emptyset$.

\paragraph{Estimating Intensity $\boldsymbol{\lambda}$} \ \\

\noindent \textit{parametric step function:} ML estimate $\hat{\lambda}_B=\frac{N(B)}{|B|}$

\noindent \textit{parametric and likelihood based:} $\lambda(s|\theta):= \exp(z'\theta)$

\noindent $l(\theta)$ can be approximated by a weighted Poisson-log-likelihood


\noindent \textit{univariate kernel density estimation:}
$$\hat{\lambda}(s)=\frac{1}{\nu(W)b_xb_y}\sum_{i=1}^nK\left(\frac{x_i-x}{b_x}\right)K\left(\frac{y_i-y}{b_y}\right)$$
\noindent with $\nu(W)$ size of window $W$, $s=(x,y)$ and bandwidths $b_x,b_y$
\noindent \textit{bivariate kernel density estimation with edge correction:}
$$\hat{\lambda}(s)=\frac{1}{\int_W \frac{1}{b^2}K\left(\frac{s-u}{b}\right)du}\sum_{i=1}^n\frac{1}{b^2}K\left(\frac{s_i-s}{b}\right)$$

\noindent The bandwidths can be estimated via cross validation.

\paragraph{K-Function} for stationary and isotrope processes %related to $\lambda_2$

$$K(h):= \frac{\mathrm{E}\left[\sum_{i=1}^n\sum_{j=1,j\neq i}^n \frac{\mathbbm{1}(\| s_i-s_j\| \leq h)}{\nu(W_{s_i}\cap W_{s_j})}\right]}{n(n-1)/\nu(W)^2}$$

\noindent with $W$ window of observation, $W_{s_i}$ window of shape $W$ with center $s_i$, and $\nu(W)$ volume of $W$

\noindent $\lambda \cdot K(h) = \mathrm{E}(h)$: expected number of events around an event

\noindent $K(h)$ determines $\lambda_2(h)$, e.g.\@ for $d=2$: $\lambda_2(h)=\frac{\lambda^2}{2\pi h} \frac{dK(h)}{dh}$

\vspace{0.3em}
\textbf{Ripley's $\boldsymbol{\hat{K}(h)}$} $=\frac{1}{\hat{\lambda}}\hat{\mathrm{E}}(h)$, with $\hat{\lambda}=\frac{N(B)}{|B|}$ and
$$\hat{\mathrm{E}}(h) = \frac{1}{n}\sum_{i=1}^n\sum_{j=1,j\neq i}^n \frac{\mathbbm{1}(\| s_i-s_j\| \leq h)}{\nu(W_{s_i}\cap W_{s_j})}$$

for $d=2$ and CSR: $K(h)=\pi h^2$ is quadratic

\vspace{0.3em}
\textbf{L-Function} scales $K(h)$ to linear
$$L(h)=\sqrt{K(h)/\pi}$$

\textbf{Pair Correlation Function}  first derivative of $K(h)$
$$g(s,u)=\frac{\lambda_2(s,u)}{\lambda(s)\lambda(u)} =: g(\|s-u\|)$$

\noindent For tests on CSR an envelope of $\hat{K}$, $\hat{L}$ or $\hat{g}$ can be calculated.

\paragraph{Marked Point Processes}  discrete or continuous \\
\vspace{0.3em}
\noindent\textit{discrete} with 2 types ($A$, $B$) or more:
\vspace{-0.7em}
\begin{itemize}[itemsep=-0.3em]
\item \textbf{Clark-Evans aggregation index:} $CE_{AB}=\bar{d}_{AB} \cdot 2\sqrt{\lambda_B}$ with $\bar{d}_{AB}$ average distance of $A$ points to nearest $B$ point; $CE_{AB}>1$ shows repulsion, $CE_{AB}<1$ attraction 
\item \textbf{Segregation coefficient:} $S=1{-}\frac{p_{AB}+p_{BA}}{p_{A\cdot}p_{\cdot B} + p_{\cdot A}p_{B \cdot}}$ with $p_{xy}$ number of pairs where type $x$'s nearest neighbor is type $y$; $S<0$ is mixing, $S>0$ segregation, and $S=0$ independence
\item \textbf{Mingling index:} $M_k = \frac{1}{k}\mathrm{E}\left[\sum_{i=1}^k\mathbbm{1}\left(m(o)\neq m(n_i(o))\right)\right]$ with point $o$, its $k$ nearest neighbors $n_i(o)$, and mark $m(o)$
\item \textbf{Bivariate $\boldsymbol{K}$, $\boldsymbol{L}$, and $\boldsymbol{g}$:} expected $y$ points around $x$ point
\item \textbf{Mark correlation function:} $K_t(r)=\frac{C_t(r)}{C_t(\infty)}$ with radius $r>0$,  $C_t(r)=\mathrm{E}_{or}\left[t(m(o),m(r))\right]$ and test function $t(\cdot)$; $K>1$ signals mutual stimulation, $K<1$ inhibition
\end{itemize}
\noindent\textit{continuous} leading to weighted measures:
$$K_{mm}(r)=\frac{c_{mm}(r)}{\lim_{r\rightarrow \infty} c_{mm}(r)}$$
with $c_{mm}(r)=\mathrm{E}(m(o)\cdot m(r))$

\paragraph{Poisson Cluster Process}

The parent is an inhomogeneous poisson process. Each parent event creates a random amount of $n(s)$ children according to a 2D density function.

\vspace{-0.7em}
\begin{itemize}[itemsep=-0.3em]
\item \textbf{Neyman-Scott:} $\mathrm{P}(n(s)=k)=p_k$ and i.i.d.\@ densities  
\item \textbf{Mat\'{e}rn:} stationary parent and $f(s)\propto \text{const.}$ for $\| s\| <R$
\end{itemize}

\paragraph{Point Process with Pairwise Interaction}

$$f(s_1,...,s_n|\theta) = \overbrace{a(\theta)}^{\substack{\text{normalising} \\ \text{constant}}}\prod_{i=1}^n \overbrace{b(s_i|\theta)}^{\text{trend}}\prod_{i<j}\overbrace{h(s_i,s_j|\theta)}^{\text{interaction}}$$

\textbf{Strauss Process:} creates disaggregation/inhibition
$$h(s,u|\theta)=\begin{cases} \gamma & \text{for } \|s-u\| \leq r \\ 1 & \text{else}\end{cases}$$

with $b(s|\theta)=\beta>0$, $0\leq\gamma\leq1$, and $r>0$

$\gamma=1$ homogeneous Poisson and  $\gamma=0$ hard core process

\vspace{0.5em}

\textbf{Gibbs Process:} $\alpha$ self potential, $\phi$ pair potential 
$$f(s_1,...,s_n)\propto\exp\left(-\alpha-\sum_{i\neq j} \phi (\|s_i-s_j\|)\right)$$

$\phi(x)=0$ Binomial , $\phi(x)=\frac{1}{\tau x^2}$ Gaussian, and $\phi=h$ Strauss

\vspace{0.5em}


\noindent As $a( \theta)$ unknown, pseudo-likelihood and approximations are used.

\paragraph{Cox Process} models aggregated patterns, $\lambda(s)$ is random field 
$$X|\lambda(s) \text{ is inhomogeneous Poisson process}$$ 

\textbf{Mixed Poisson Process} $\lambda(s)=\lambda_0$ (homogeneous)

\noindent Cox processes have overdispersion: $\mathrm{Var}(N(B)) > \mathrm{E}(N(B))$

\noindent $\lambda(s)$ stationary/isotrope $\Rightarrow$ Cox Process stationary/isotrope

\noindent Neyman-Scott processes can be viewed as Cox processes


\paragraph{Log Gauss Cox Process}

$$\lambda(s)=\exp(z(s)'\beta + Y(s))$$

\noindent with $\{Y(s)|s\in S\}$ stationary Gauss random field

\textbf{Data Augmentation Approach} \ \\ 
As $\{Y(s),s\in S\}$ infinite, approximate with finite GRF

and approximate $\int_W \lambda(s)ds$ with Riemann sum

\textbf{Intensity as Latent GMRF} discretize $S$ to small grid
$$\lambda(s) \sim \mathrm{GRF} \Rightarrow \lambda(s_i) \sim \mathrm{GMRF}$$

often with constructed spatial covariables, e.g.\@ distance to nn

inference over INLA or other STAR algorithms

\end{multicols}
%-------------------------------------------------------------------------------

% SECTION: BAYESIAN STATISTICS

%-------------------------------------------------------------------------------

\section{Bayesian Statistics}

\begin{multicols}{2}[\subsection{Basics}][4cm] 

\paragraph{Bayes Theorem}
$$P(A|B)=\frac{P(B|A)P(A)}{P(B)} \hspace{0.5cm} \text{für } P(A), P(B) > 0$$

or more general:

\vspace{-1 em}

\begin{align*}
  f_{post}(\theta | X) &= \frac{f(X | \theta ) \cdot f_\theta(\theta)}{\int f(X|\tilde{\theta}) f_\theta(\tilde{\theta})  d \tilde{\theta}}\\
  &= C \cdot f(X | \theta ) \cdot f_\theta(\theta) \hspace{1 em} \text{choose C s.\,t.\  $\int f_{post}(\theta | X)=1$} \\
  &\propto f(X | \theta ) \cdot f_\theta(\theta)
\end{align*}

\paragraph{Point Estimates}

\begin{align*}
\hat{\theta}_{postmean} &= E_0(\vartheta|x) = \int_{\vartheta \in \Theta} \vartheta f_\theta(\vartheta|x)d\vartheta\\
\hat{\theta}_{postmode} &= \mathrm{arg} \underset{\vartheta}{\max} f_\theta(\vartheta,x)\\
\hat{\theta}_{Bayesrisk} &= \mathrm{arg} \underset{t(.)}{\min} R_{Bayes}(t(.))\\
\intertext{with Bayes risk: $ R_{Bayes}(t(.)) = \int_{\Theta} R(t(.),\vartheta)f_\theta(\vartheta)d\vartheta$}
\hat{\theta}_{postBayesrisk} &= \mathrm{arg} \underset{t(.)}{\min} R_{postBayes}(t(.)|y)\\
\intertext{with posterior Bayes risk: $ R_{postBayes}(t(.)|y) = \int \mathcal{L}(t(y),\vartheta) f_{post}(\vartheta|y) d\vartheta = \mathrm{E}_{\theta|y}(\mathcal{L}(t(y),\theta)|y)$}
\end{align*}

\vspace{-1.5em}

\noindent For squared loss: $\hat{\theta}_{postBayesrisk} = \hat{\theta}_{postmean}$


\paragraph{Credibility Interval}

$$P_\theta(\theta\in \left[t_l(y),t_r(y)\right]|y) = \int_{t_l(y)}^{t_r(y)} f_{post}(\vartheta|y)d\vartheta \overset{!}{=} 1- \alpha$$

\begin{itemize}
\item symmetric: $\int_{-\infty}^{t_l(y)} f_{post}(\vartheta|y)d\vartheta = \int_{t_r(y)}^{\infty} f_{post}(\vartheta|y)d\vartheta = \frac{\alpha}{2}$
\item highest density: $HDI = \{\theta : f_{post}(\theta|y)\geq c\}$, choose $c$ s.\,t.\  $\int_{\vartheta \in HDI(y)} f_{post}(\vartheta|y)d\vartheta = 1-\alpha$
\end{itemize}

%\paragraph{Sensitivity Analysis}

%\paragraph{Predictive Posterior}

%$$f_{post}(x_Z|\mathbf{x}) =\int f(x_Z, \lambda|\mathbf{x})d\lambda = \int f(x_Z|\lambda)p(\lambda|\mathbf{x})$$

\paragraph{Bayes Factor} evidence contained in data for $M_1$ vs. $M_2$

$$\frac{P(M_1|y)}{P(M_0|y)} =  \underbrace{\frac{f(y|M_1)}{f(y|M_0)}}_{\text{Bayes Factor}} \frac{P(M_1)}{P(M_0)}$$

with marginal likelihood $f(y|M_i) = \int f(y|\vartheta)f_\theta(\vartheta|M_i)d\vartheta$


\subsubsection*{\textit{Priors}}

\paragraph{Flat (uninformative) Prior} \ \\

\noindent $f_\theta(\theta)=const. \text{ for } \theta > 0$
,  therefore:
 $f(\theta | X) = C \cdot f(X | \theta )$

\noindent As $\int f_\theta(\theta) =1$ not possible like this, this is not a real density.

\noindent Changes for transformations of the parameter.

\begin{Proof}

\vspace{-2.3em}
\centering
For $\gamma = g(\theta)$: $f_\gamma(\gamma)=f_\theta(g^{-1}(\gamma))\left|\frac{\partial g^{-1}(\gamma)}{\partial \gamma}\right|$
\end{Proof}

 \noindent No prior is truly uninformative.


\paragraph{Jeffrey's Prior} transformation-invariant  \\

\noindent For Fisher-regular distributions: $f(\theta) \propto \sqrt{I_\theta(\theta)}$

\begin{Proof}
For $\gamma = g(\theta)$ and $f_\theta(\theta) = \sqrt{I_\theta(\theta)}$:

\noindent $f_\gamma(\gamma)  \propto f_\theta(g^{-1}(\gamma))\left|\frac{\partial g^{-1}(\gamma)}{\partial \gamma}\right| \propto \sqrt{\frac{\partial g^{-1}(\gamma)}{\partial \gamma}  I_\theta(g^{-1}(\gamma)) \frac{\partial g^{-1}(\gamma)}{\partial \gamma}}$ 

\raggedleft
$ = \sqrt{I_\gamma(\gamma)}$
\end{Proof}

\noindent\begin{minipage}{\dimexpr\linewidth}
\raggedright
Maximizes the information gained from the data (under appropriate regulatory conditions), i.\,e.\ maximizes $\mathrm{E}(KL(f_\theta(.), f_{post}(.,x))$
\end{minipage}

\paragraph{Empirical Bayes} \ \\

\noindent Let the prior depend on a hyper-parameter: $f_\theta(\theta,\gamma)$

\noindent Choose $\gamma$ s.\,t.\ $L(\gamma) = f(x;\gamma) = \int f(x;\vartheta) f_\theta(\vartheta, \gamma)d\vartheta$ is maximal.

\noindent Using the data to find the prior contradicts the Bayes approach of incorporating prior knowledge.

\paragraph{Hierarchical Prior} \ \\

$$x|\theta \sim f(x;\theta);\;\;\; \theta|\gamma \sim f_\theta(\theta, \gamma);\;\;\; \gamma \sim f_\gamma(\gamma)$$

\paragraph{Conjugate Priors}

\begin{Extensiv}
~\\
  \noindent If Prior and Posterior belong to the same family of distributions for a given likelihood function, they are called conjugate. 
\end{Extensiv}

\vspace{0.5em}
\noindent Examples:
\vspace{-0.5em}
\begin{center}
\begin{tabular}{ccc}
Prior & Likelihood & Posterior\\
\hline
$\pi \sim\mathrm{Be}(\alpha,\beta)$ & $\mathrm{Bin}(n,\pi)$ & $\mathrm{Be}(\alpha {+} k, \beta {+} n {-} k)$\\
 $\mu \sim \mathrm{N}(\gamma, \tau^2)$ & $\mathrm{N}(\mu, \sigma^2)$ & $\mathrm{N}(.,.)\overset{n\rightarrow\infty}{\longrightarrow} \mathrm{N}(\bar{y}, \frac{\sigma^2}{n})$\\
 $\sigma^2 \sim \mathrm{IG}(\alpha, \beta)$ & $\mathrm{N}(\mu, \sigma^2)$ & $\mathrm{IG}(\alpha{+}\frac{n}{2},\beta{+}\frac{1}{2}\sum_{i=1}^{n}(y_i{-}\mu)^2)$\\
 $\lambda \sim \mathrm{Ga}(\alpha, \beta)$ & $\mathrm{Po}(\lambda)$ & $\mathrm{Ga}(\alpha{+}n\bar{y}, \beta{+}n)$\\
\end{tabular}
\end{center}

\end{multicols}


\begin{multicols}{2}[\subsection{Numerical Methods for the Posterior}][4cm]

\paragraph{Numerical Integration} here: trapezoid approximation
$$\int_\Theta f(y|\vartheta)f_\theta(\vartheta)d\vartheta\approx$$ $$  \sum_{k=1}^K \frac{f(y;\theta_k)f_\theta(\theta_k) + f(y;\theta_{k-1})f_\theta(\theta_{k-1})}{2}(\theta_k {-} \theta_{k-1})$$

 \noindent only normalisation constant unknown, works well for one-dimensional integrals


\paragraph{Laplace Approximation} \ \\

$$\int_\Theta f(y|\vartheta)f_\theta(\vartheta)d\vartheta \approx f(y;\hat{\theta}_P) f_\theta(\hat{\theta}_P)(2\pi)^{p/2}\left|J_{P}(\hat{\theta}_P)\right|^{\frac{1}{2}}$$

\noindent with the one-dimensional $J_{P} := {-}\frac{\partial^2 l_{(n)}(\theta,y)}{\partial \theta^2} {-} \frac{\partial^2 \log f\theta(\theta)}{\partial\theta^2}$ Fisher information considering the prior, $\hat{\theta}_P$ posterior mode estimate s.\,t.\ $s_{P,\theta}(\hat{\theta}_P)=0$

\begin{Proof}
For $n$ independent samples:\vspace{0.2em}

$f_{post}(\theta|y)=\frac{\prod_{i=1}^n f(y_i|\theta)f_\theta(\theta)}{\int \prod_{i=1}^n f(y_i|\theta)f_\theta(\theta)d\theta}$
\vspace{0.6em}

\noindent Denominator:\vspace{0.2em}
\vspace{0.2em}
$\int \mathrm{e}^{\left\{ \sum_{i=1}^n \log f(y_i|\theta) + \log f_\theta(\theta)\right\}} d\theta =$ 

\noindent $ \int \mathrm{e}^{\left\{ l(\theta;y){+}\log f_\theta(\theta) \right\}} d\theta \overset{TS}{\approx} \int \mathrm{e}^{(l_P(\hat{\theta}_P) {-} \frac{1}{2} J_P(\hat{\theta}_P)(\vartheta {-}\hat{\theta}_P)^2)}d\vartheta$
\vspace{0.6em}

\noindent Resembles the normal distribution, therefore the inverse of the normalisation constant can be calculated, which gives the inverse of the Laplace approximation in the univariate case.
\end{Proof}

\noindent Works well for large $n$ and is numerically simple also for big $p$.

\paragraph{Monte Carlo Approximations} \ \\

\noindent The denominator can be written as $\mathrm{E}_\theta(f(y;\theta)) =$ $ \int_\Theta f(y|\vartheta)f_\theta(\vartheta)d\vartheta$, which  can be estimated by the arithmetic mean for a sample of $\theta_1,...,\theta_N$, which needs to be drawn from the prior. The following methods to draw from non-standard distributions can be used for that.

\begin{itemize}
\item \textbf{Inverse CDF} \\
$F(X)$ known. Since $F(x)=u$, $F^{-1}(u)=x$, $u \sim U(0,1)$ \\
\vspace{-0.5em}
\begin{enumerate}
\item Draw $u \sim U(0,1)$
\item Compute $F^{-1}(u)$ to get a value $x$
\end{enumerate}
\end{itemize}
\begin{Proof}
\vspace{-1.3em}
$$P(x\leq y) = P(F^{-1}(u)\leq y) = P(u\leq F(y)) = F(y)$$
\end{Proof}

\begin{itemize}
\item \textbf{Rejection Sampling} \\
An umbrella distribution $g(x)$ can be found s.\,t.\ $\frac{f(x)}{g(x)} \leq M \;\forall x$ with $f(x)>0$ when $g(x)>0$ 
\vspace{-0.5em}
\begin{enumerate}
\item Draw candidate $y\sim g(x)$
\item Acceptance probability $\alpha$ for $y$: $\alpha = \frac{f(x)}{Mg(x)}$
\item Draw $u \sim U(0,1)$ and accept if $u\leq\alpha$, else: step 1
\end{enumerate}
\end{itemize}
\begin{Proof}
\vspace{-1.3em}
\begin{align*}
P\left(Y\leq x|U\leq\frac{f(Y)}{Mg(Y)}\right) 
&= \frac{P\left(Y\leq x, U\leq\frac{f(Y)}{Mg(Y)}\right)}{P\left(U\leq\frac{f(Y)}{Mg(Y)}\right)}\\
=\frac{\int_{-\infty}^x \int_0^{\frac{f(y)}{g(x)}} du\: g(y) dy}{\int_{-\infty}^\infty \int_0^{\frac{f(y)}{g(x)}} du\: g(y) dy} 
&= \frac{\int_{-\infty}^x \frac{f(y)}{g(x)} g(y) dy}{\int_{-\infty}^\infty \frac{f(y)}{g(x)} g(y) dy}\\
= \frac{\int_{-\infty}^x f(y) dy}{\int_{-\infty}^\infty f(y) dy} &= P(X\leq x)\\
\end{align*}
\end{Proof}

\begin{itemize}
\item \textbf{Importance Sampling} \\
Directly estimate $\mathrm{E}_\theta(f(y;\theta))$.

For sampling distribution $g(x)$,
$$\frac{1}{N}\sum_{i=1}^n \frac{f(x)}{g(x)}$$
is a consistent estimator.
\end{itemize}
\begin{Proof}
\vspace{-1em}
$$\mathrm{E}_g\left(\frac{1}{N}\sum_{i=1}^n \frac{f(x)}{g(x)}\right) = \int \frac{f(x)}{g(x)}g(x)dx = \int f(x) dx = f(x)$$
\end{Proof}



\paragraph{Markov Chain Monte Carlo} sample from $f_{post}(\theta|X)$

$f(y)$ unknown, however: $$\frac{f_{post}(\theta|x)}{f_{post}(\tilde{\theta}|x)} = \frac{f(x|\theta)f_\theta(\theta)}{f(y)} \frac{f(y)}{f(x|\tilde{\theta})f_\theta(\tilde{\theta})}= \frac{f(x|\theta)f_\theta(\theta)}{f(x|\tilde{\theta})f_\theta(\tilde{\theta})}$$

\vspace{0.5em}
 \textbf{Metropolis-Hastings}: Draw Markov Chain $\theta_1^*, ..., \theta_n^*$:
\begin{enumerate}
\item Draw candidate $\theta^*$ from proposal distribution $q\left(\theta|\theta_{(t)}^*\right)$
\item Accept $\theta^*_{(t+1)} = \theta^*$ with probability $$\alpha(\theta_{(t)}|\theta^*) = \min\left\{1,\frac{f_{post}\left(\theta^*|y\right)q\left(\theta_{(t)}^*|\theta^*\right)}{f_{post}\left(\theta_{(t)}^*|y\right)q\left(\theta^*|\theta_{(t)}^*\right)}\right\}$$
else choose $\theta^*_{(t+1)} = \theta_{(t)}^*$
\end{enumerate}
This sequence has a stationary distribution for $n\rightarrow \infty$.

\noindent Choice of $q$: trade-off between exploring $\Theta$ and reaching a high $\alpha$.

\noindent Burn-in and thinning out give $i.i.d.$ samples from $f_{post}(\theta|X)$.

\textbf{Gibbs Sampling}: For high dimensions $\alpha$ is close to zero.

Sample from the marginal distributions seperately:
 $$\theta_{t{+}1,i}^* \sim f_{\theta_i | y, \theta \setminus \theta_i}\left(\theta_i^*|y, \theta_{t^*,i}\right)$$ 
 
 with $\theta_{t^*, i}$ most recent estimates without $\theta_i$
 
 \noindent A Gibbs sampled sequence converges to $f_{post}(\theta|X)$ as stationary.
 
 \noindent Can also be used on its own, if marginal densities are known.


\paragraph{Variational Bayes Principles}  \ \\

Approximate $f_{post}(\theta|X)$ by $q_\theta=\underset{q_\theta\in Q}{\min} KL(f_{post}(.|X), q_\theta(.))$

Restrict $q_\theta$ to independence: $q_\theta(\theta) = \prod_{k=1}^p q_k(\theta_k)$

Update each component iteratively.
Works well for big $p$. 
\end{multicols}

\begin{multicols}{2}[\section{Sampling}][4cm]

\paragraph{Bootstrap}

\begin{enumerate}
\item Draw $y^*_i$: $n$ samples with replacement from $y$
\item Calculate the statistic of interest $t(y_i^*)$
\item Repeat this B times
\item \noindent \textit{Plug-in Principle}:
Whenever the distribution function is involved in estimating a statistic, use the empirical version $\hat{F}_n(y) = \frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{(y_i\leq y)}$  instead.
\end{enumerate}

\noindent In a \textbf{Parametric Bootstrap} the parameter is first estimated from the data and then Bootstrap samples are drawn from the resulting distribution.

\paragraph{Bootstrap Probability} \ \\

$$P(Y_i\in Y^*) = 1-(1-\frac{1}{n})^n\overset{n\rightarrow\infty}{\rightarrow} 1-e^{-1} \approx 0.632$$


\paragraph{Subsampling}

\begin{itemize}
\item \textbf{replacement} $m$-out-of-$n$ bootstrap
\item \textbf{non-replacement} subsampling directly from true $F$
\end{itemize}

\paragraph{Permutation Test} for two variables

\begin{enumerate}
\item Calculate $t(x,y)$, e.\,g.\ differences in mean, correlation...
\item Draw samples $x^*$, $y^*$ of size $n$ from $x$ and $y$ without replacement (``shuffel'')\label{draw}
\item Calculate $t(x^*, y^*)$ 
\item $p\text{-value} = \frac{1}{B}\sum_{b=1}^B \mathbbm{1}_{\{t(x_b^*,y_b^*)\geq t(x,y)\}}$
\end{enumerate}

For a \textbf{Boostrap Test} do step \ref{draw} with replacement.

\paragraph{Bootstrap in \textbf{Regression}}
\begin{itemize}
\item \textbf{Residual based}: 1.~Get Bootstrap sample $e_i^*$ from fitted residuals $\hat{e} = y- X\hat{\beta}$, 2.~Calculate new response $y_i^* = x_i\hat{\beta} + e_i^*$, 3.~Calculate $\hat{\beta}^*$
\item \textbf{Model based} 1.~Draw a sample from $e_i \sim \mathrm{N}(0, \hat{\sigma}_{ML}^2)$, 2.~Calculate new response $y_i^* = x_i\hat{\beta} + e_i^*$, 3.~Calculate $\hat{\beta}^*$
\item \textbf{Pairwise} 1.~Draw $(y_i^*, x_i^*)$ from the original sample for $i=1,..,n$, 2.~Calculate $\hat{\beta}^*$
\item \textbf{Wild} Set $\hat{e}_i^* = V_i^*\hat{e}_i$, with $V_i^*$ from the 2-point distribution $P(V_i^* {=}\frac{\sqrt{5}{+}1}{2})=\frac{\sqrt{5}{-}1}{2\sqrt{5}}$ and $P(V_i^* {=}{-}\frac{\sqrt{5}{-}1}{2})=\frac{\sqrt{5}{+}1}{2\sqrt{5}}$, chosen as $\mathrm{E}(V_i^*)=0$, $Var(V_i^*)=1$, $\mathrm{E}(V_i^{*3})=1$
\end{itemize}

\paragraph{Consistency of a Bootstrap Estimator} 

$$ \underset{n\rightarrow\infty}{\lim} P_n\left\{\underset{t}{\sup}\left|G_n(t,F_n)-G_\infty(t,F)\right|>\epsilon\right\} = 0  \;\; \forall \epsilon $$

\noindent with $F_n(y) = \frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{Y_i\leq y\}}$ empirical distribution function, $G_n(t, F) = P(T_n\leq t)$ exact finite sample distribution of $T_n = t(Y_1,...,Y_n)$, and $P_n$ joint probability of the sample

\vspace{0.5em}
\noindent The bootstrap estimate is inconsistent for the maximum of a sample or if the $\theta$ lies on the boundary of $\Theta$.

\paragraph{Mallow's Metric}

$$\rho_p(F,G) = \underset{\mathcal{T}_{XY}}{\inf}\{E(|X-Y|)^p\}^\frac{1}{p} $$

\noindent for $F,G$ in the set of distributions where $\int_{-\infty}^\infty |t|^pdF(t) <\infty$; $(X,Y) \sim T \in\mathcal{T}_{XY}$ with $X\sim F$ and $Y\sim G$

\paragraph{Theorem of Beran and Ducharme} \ \\

$G_n (., F_n)$ is consistent if $\forall \epsilon > 0, F$ the following holds:

\begin{enumerate}
\item $\underset{n\rightarrow\infty}{\lim}P_n(\rho(F_n,F)>\epsilon)=0$
\item $G_\infty(t,F)$ is a continuous function of t
\item $\forall t$ and sequences $\{H_n\}$ s.\,t.\ $\underset{n\rightarrow\infty}{\lim}\rho(H_n,F)=0$ holds: $G_n(t,H_n) \rightarrow G_\infty(t,F)$
\end{enumerate}


\end{multicols}

\begin{multicols}{2}[\section{Model Selection}][4cm]

\paragraph{AIC (Akaike Information Criterion)}

$$AIC = -2\sum_{i=1}^n \log f(y_i;\hat{\theta}) + 2p$$

\noindent The AIC estimates $2\mathrm{E}_{Y}\left\{\mathrm{KL}(g,f)\right\} - 2\int\log(g(y))g(y)dy$. The latter component is unknown, so the absolute value of the AIC is not informative. The AIC favours complex models. 

For regressions: $AIC= 2n\log(\hat{\sigma}^2) + 2(p+2)$

\paragraph{The AIC as theoretical cross validation} \ \\

\noindent The AIC minimizes $\mathrm{E}_{Y_{1:n}}\left\{\mathrm{E}_Y\left[Y-\hat{\mu}\right]^2\right\}$ if we use the MSE instead of the Kullback-Leibler divergence. This can be estimated via cross validation.

\paragraph{Bias Corrected AIC}

$$AIC_{corr} = -2\sum_{i=1}^n \log f(y_i;\hat{\theta}) +2p\left(\frac{n}{n{-}p{-}1}\right)$$

 should be preferred if $\frac{n}{p}<40$
 
\paragraph{BIC (Bayesian Information Criterion)}
 
$$BIC = -2\sum_{i=1}^n \log f(y_i;\hat{\theta}) + \log(n)p$$

\noindent approximately maximizes the posterior probability of a model

and selects less complex models as the AIC

\paragraph{DIC (Deviance Information Criterion)} Bayesian AIC

$$DIC = D(y,\hat{\theta}_{postmean}) + 2p_D = \int D(y,\vartheta)f_{post}(\vartheta|y)d\vartheta + p_D$$

\noindent with deviance $D(y;\theta) := -2l(\theta)$ the difference in likelihood compared to the full model and $\Delta D(y;\theta,\hat{\theta}) = 2\left\{l(\hat{\theta}) {-} l(\theta)\right\} \overset{a}{\sim}\chi_p^2$ the difference in deviance

$$p_D := \mathrm{E}(\Delta D(y;\theta, \hat{\theta}_{\substack{post \\ mean}}|y) = \int\!\! D(y,\vartheta)f_{post}(\vartheta|y)d\vartheta {-} D(y,\hat{\theta}_{\substack{post \\ mean}})$$

\noindent The integral can be approximated using MCMC.

\paragraph{Model Averaging} Using probabilities as weights
$$P(M_k|y) := \frac{\exp(-\frac{1}{2}\Delta IC_k)}{\sum_{k' =1}^K\exp(-\frac{1}{2}\Delta IC_k')}$$

with $\Delta IC_k = IC_k - min(IC)$

\noindent For regressions: $P(\text{covariate } x|y) = \sum_{k=1}^K \mathbbm{1}_{\{x \:\text{in}\: M_k\}}P(M_k|y)$

\paragraph{Inference After Model Selection} neglect is a quiet scandal

\begin{align*}
Var(\hat{\theta}) &= \mathrm{E}_{model}(Var(\hat{\theta}|model)) + Var_{model}(E(\hat{\theta}|model))\\
&= \sum_{k=1}^K \pi_kVar_k(\hat{\theta}) + \sum_{k=1}^K \pi_k(\theta_k-\bar{\theta})^2
\end{align*}

\noindent The last component depends on the true parameter and will be biased if the estimates are used.

Solutions:
\begin{itemize}
\item $\widehat{Var}(\hat{\theta}) = \left[ \sum_{k=1}^K \pi_k \sqrt{\widehat{Var}_k(\hat{\theta}_k) + (\hat{\theta}_k - \hat{\bar{\theta}})^2}\right]^2$
\item Use the Variance of the full (saturated) model
\item Use bootstrap for confidence intervals
\end{itemize}

\paragraph{Lasso} least absolute shrinkage and selection operator \ \\
 
 $$l_p(\theta,\lambda) = l(\theta) - \lambda\sum_{j=1}^p |\theta_j|$$
 
 \noindent This penalized log likelihood can be solved with iterative quadratic programming using a Taylor expansion. 
 
 \noindent Using Bayesian view the penalty corresponds to a prior: $f_{\theta_j} (\theta_j)\propto \exp(-|\theta_j|)$ (Laplace prior)

\end{multicols}

\begin{multicols}{2}[\section{Dimensionality Reduction}][4cm]

\paragraph{Covariance Matrix \boldmath$\Sigma$}
\begin{itemize}
\item symmetric, $\in \mathbb{R}^{n\times n}$ therefore $\frac{q(q+1)}{2}$ parameters
\item positive definite, i.\,e.\ $\forall a \in \mathbb{R}^q: a^T\Sigma a \geq 0$
\end{itemize}

\paragraph{Marginal Independence}

$$\Sigma_{jk} = 0 \Leftrightarrow Y_{ij} \text{ and } Y_{ik} \text{ are independent}$$

\paragraph{Conditional Independence}

$$\Omega = \Sigma^{-1}_{jk} = 0 \Leftrightarrow Y_{ij} \text{ and } Y_{ik} \text{ are independent given all other } Y$$

with concentration matrix $\Omega$

\begin{Proof}
\vspace{-1.5em}
\begin{align*}
f(y_{.j}, y_{.k}|y_{.\overline{j,k}}) &= \frac{f(y)}{f_{.\overline{j,k}}} \propto f(y)
\overset{\mathrm{N}(\mu,\Sigma)}{\propto} \exp\left\{-\frac{1}{2}y^T\Sigma^{-1}y\right\}
\end{align*}
\end{Proof}

\paragraph{Graphical Models} \ \\ 
\noindent visualize conditional dependences in a graph

\paragraph{Principal Component Analysis (PCA)} \ \\
\begin{enumerate}
\item Use singular value decomposition $\Sigma = U \Lambda U^T$
with $U$ matrix of orthonomal eigenvectors and $\Lambda = diag(\lambda_1,...,\lambda_q)$ matrix of sorted eigenvalues $\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_q$ of $\Sigma$
\item Prune smallest $k=q-r$ eigenvalues in $\tilde{\Lambda}$
\item Simplify model with spectral decomposition $\tilde{Y} = \tilde{V}\tilde{\Lambda}^{1/2}\tilde{U}^T$ with $\tilde{V}$, $\tilde{U}$ first $r$ eigenvectors of $YY^T$ and $Y^TY$ respectively
\item explained variance $\sum_{i=1}^{r}\lambda_i/\sum_{i=1}^{q}\lambda_i$
\end{enumerate}

\begin{Proof}
Karhunen-Lo\`{e}ve expansion: $U\Lambda^{\frac{1}{2}}Z_{.} \sim \mathrm{N}(0, U\Lambda^{\frac{1}{2}}\Lambda^{\frac{1}{2}}U^T) = \mathrm{N}(0,\Sigma)$ with $Z_{.} \sim \mathrm{N}(0,\mathbbm{1})$, therefore $\tilde{Y}_{.} = \tilde{V}\tilde{\Lambda}^{\frac{1}{2}}\tilde{Z}_{.}$

\noindent With spectral decomposition: $Y=V\Lambda^{\frac{1}{2}}U^T$ (for column-centered $Y$)

\end{Proof}

\end{multicols}



\begin{multicols}{2}[\section{Missing/Deficient Data}][4cm]

\paragraph{Missing Completely at Random (MCAR)} independent 

$$P(R_i|Y_i) = P(R_i)$$

with $R_{ij} = \begin{cases} 0 & \text{if}\ Y_{ij}\ \text{missing} \\ 1 & \text{otherwise}\end{cases} $ and $R_i = (R_{i1},...,R_{iq})$

\noindent A complete case analysis will lead to unbiased results.

\paragraph{Missing at Random (MAR)} depends on observed variables

$$P(R_i|Y_i) = P(R_i|Y_{iO_i})$$

with $O_i = \{j:R_{ij}=1\}$ and $M_i = \{j:R_{ij}=0\}$

\noindent Complete case analysis $P(Y|X, Z)$:
\begin{itemize}
\item only response $Y_i$ MAR: unbiased
\item only covariate $X_i$ MAR: biased \\
Asymptotically unbiased with \textit{inverse probability weighting}:
1. Estimate $\pi(y_i,z_i) = P(R_{X_i}{=}1|y_i,z_i)$ \\
2. Use weighted score function $\hat{s}_{w}(\theta) = \sum_{i=1}^n \frac{R_{X_i}}{\hat{\pi}} s_i(\theta)$

\item both MAR: biased and $\pi(y_i,z_i)$ can not be estimated due to missing $Y_i$
\end{itemize}

\paragraph{Missing Not at Random (MNAR)}

$$P(R_i|Y_i) \neq P(R_i|Y_{iO_i})$$

\noindent Can not be corrected to be unbiased.

\paragraph{EM Algorithm} replace $y_{iM}$ by $E(Y_{iM}|y_{iO})$ \\

\noindent Expectation Step:

$$Q(\theta,\theta_{(t)}) = \sum_{i=1}^n\int l_i(\theta)f(y_{iM}|y_{iO};\theta_{(t)})dy_{iM}$$

\noindent Maximization Step:

$$\frac{\partial Q(\theta,\theta_{(t)})}{\partial \theta} = s(\theta,\theta_{(t)}) \overset{!}{=} 0 $$


%\begin{Proof}
%$$l_i(\theta) = l_{iO_i} + \log f(y_{iM_i}|y_{iO_i};\theta)$$
%$$l_O(\theta) =  l(\theta) - \sum_{i=1}^n \log f(y_{iM_i}|y_{iO_i};\theta)$$
%$$l_O(\theta) = Q(\theta|\theta_{(t)}) - H(\theta,\theta_{(t)})$$
%$$l_O(\theta_{(t+1)} \geq l_O(\theta_{(t)}$$
%$$l_O(\theta) = \sum_{i=1}^n l_{iO}(\theta) = \sum_{i=1}^n \int f(y_i;\theta)dy_{iM}$$
%\end{Proof}

\noindent Iterate over E and M step until convergence

\paragraph{Louis' Formula for Variance Estimates in EM Settings} \ \\

$$J_O(\theta) = \sum_{i=1}^n \{\mathrm{E}(J_i(\theta)|y_{iO}) - \mathrm{E}(s_i(\theta)s_i(\theta)|y_{iO}) + s_{iO}(\theta)s_{iO}(\theta)\}$$


 \paragraph{Multiple Imputation} EM but considers estimation variability
 
 \begin{enumerate}
 \item Create $K$ complete datasets by simulating missing data $\sim f_{post}(y_{iM}|y_{iO})$
 \item Fit $K$ models $Y_i \sim f(y|\theta)$
 \item Rubin's Rule: $\hat{\theta}_{MI} = \frac{1}{K}\sum_{k=1}^K \hat{\theta}^*_{(k)}$;
  $\widehat{\text{Var}}(\hat{\theta}_{MI}) = \hat{V} {+} (1{+}\frac{1}{K})\bar{B}$ 
 with $\hat{V} = \frac{1}{K} \sum_{k=1}^K I^{-1}(\hat{\theta}_{(k)}^*)$ 
 and $\bar{B} = \frac{1}{K-1} \sum_{k=1}^K (\hat{\theta}_{(k)}^* - \hat{\theta}_{MI}) (\hat{\theta}_{(k)}^* - \hat{\theta}_{MI})^T$
 \end{enumerate}
 
 \paragraph{Estimate Accuracy}
 
 $$\hat{\mu}_g - \mu_g =\rho_{R_g} \times \sigma_g \times \sqrt{\frac{N-n}{n}}$$
 with $\rho_{R_g}$  data quality (correlation between $R_j$ and $g(Y_j)$), $\sigma_g$ variability, and $\sqrt{\frac{N-n}{n}}$ data quantity; $g$ some known function

\begin{itemize}
\item MCAR: $MSE(\hat{\mu}_g) = \frac{1}{N-1} \times \sigma^2_g \times \frac{N-n}{n}$
\item MNAR: $MSE(\hat{\mu}_g) = \mathrm{E}(\rho^2_{R_g}) \times \sigma^2_g \times \frac{N-n}{n}$ \\
$n_{eff} = \frac{\frac{n}{N}}{1-\frac{n}{N}}\frac{1}{\mathrm{E}(\rho^2_{R_g})}$
\end{itemize}

\paragraph{Measurement Error}

$$U = X - X_m \text{ with } \mathrm{E}(U) = \mu_U \text{ and }  \mathrm{Var}(U) = \sigma_U^2$$
with $\mu_U$ systematic error (bias/validity), $\mathrm{Var}(U)$ (reliability)

\noindent In Regression Settings:

\begin{itemize}
\item \textbf{error in $\mathbf{Y}$}: $Y_m = \beta_0 + \beta_1X +\epsilon+U$ and $\mathrm{E}(Y_m|X) = \beta_0 + \mu_U + \beta_1X$ leads to  biased $\hat{\beta}_0$
\item \textbf{ error in $\mathbf{X}$}: $Y=\beta_0 + \beta_1X +\epsilon$ and $X_m = X + U$ leads to biased $\hat{\beta}_0$ and $\hat{\beta}_1$, the latter is attenuated by the inverse of reliability ratio $rr = \frac{\sigma^2_X}{\sigma^2_X +\sigma^2_U} = \frac{\sigma^2_{X_m} {-}\sigma^2_U}{\sigma^2_{X_m}}$ \\
Getting information about $\sigma_U^2$:
\begin{itemize}
\item \textbf{Validation Data} with both $X$ and $X_m$ observed 
\item \textbf{Replication Data} repeated measures of $X_m$
\item \textbf{Assumptions} e.\,g.\ $\sigma_U^2=0$ (naive estimator)
\end{itemize}
\end{itemize}

\end{multicols}


\begin{multicols}{2}[\section{Experiment Design}][4cm]

\paragraph{Omitted Variables} \ \\

Regression setting ignoring omitted Variables:

$$\int f_{Y|X,Z,U}f_{Z,U}dzdu = \int \frac{f_{Y,X,Z,U}}{f_{X|Z,U}}dzdu \neq f_{Y|X}$$

\noindent with $Z$ observable and $U$ unobservable quantities influencing $Y$

Solutions:
\begin{itemize}
\item \textit{Randomization}: randomly assign $X$ and then observe $Y$
\item \textit{Balancing}: make $X$ independent of $Z$
\end{itemize}


\paragraph{Analysis of Variances (ANOVA)} of one categorical variable

\noindent Linear constraint: $\sum_{k=1}^K n_k \beta_k = 0$ (usually controlled over $\beta_K$)

\begin{align*}
\hat{\beta}_{0, ML} &= \hat{\mu}_{ML} = \sum_{k=1}^K\sum_{j=1}^{n_k} \frac{y_{kj}}{n} = \bar{y}_{..}\\
\hat{\beta}_{k,ML} &= \sum_{j=1}^{n_k} \frac{y_{kj}-\bar{y}_{..}}{n_k} = \bar{y}_{k.} - \bar{y}_{..}
\end{align*}

$$SS_{Total}=SS_{Explained}+SS_{Residual}$$

with
\begin{align*}
SS_{Total}  &=  \sum\limits_{i=1}^n(y_i-\bar{y}_{..})^2 \\
SS_{Explained} &= \sum\limits_{i=1}^n(\hat{y}_i-\bar{y}_{..})^2 \\
SS_{Residual} &= \sum\limits_{i=1}^n(y_i-\hat{y}_i)^2=\sum\limits_{i=1}^n e_i^2=S_{yy}-\hat{\beta}^2S_{xx} 
\end{align*}



\textbf{F-Test}

$$F=\frac{SS_{Explained}/(df_0 {-} df_X)}{SS_{Residual}/df_X} \sim \mathcal{F}_{df_0{-}df_X,df_X}$$

with $df_0 = n{-}1$ and $df_X=n{-}K$


\paragraph{Block Design} account for block effects

$$Y_{kbj} = \mu+ \beta_k +\alpha_k+\epsilon_{kbj}$$

\noindent Linear constraints: $\sum_{k=1}^K n_{k.} \beta_k = 0$ and $\sum_{b=1}^B n_{.b} \alpha_b = 0$ 

For the F-Test: $df_0 = df_Z = n{-}B$ and $df_{X+Z}=n{-}K{-}B{+}1$

\paragraph{Latin Squares} Sodoku pattern for more variables

\paragraph{Instrumental Variable}

$$Y = \beta_0 + X\beta_X + \overbrace{U\beta_U + \epsilon}^{\tilde{\epsilon}} \ \Rightarrow\  \frac{\partial Y}{\partial X} = \beta_X + \frac{\partial \tilde{\epsilon}}{\partial X}$$

\noindent Construct instrumental variable $Z$: $Cov(Z,\epsilon){=}0$, $Cov(Z,X){\neq}0$:

$$\frac{\partial Y}{\partial X}|(U{=}u) = \frac{\partial Y / \partial Z}{\partial X/ \partial Z}$$

i.\,e. fit two regressions $Y|Z$ and $X|Z$ and set $\hat{\beta}_X = \frac{\hat{\beta}_{YZ}}{\hat{\beta}_{XZ}}$

\paragraph{Propensity Score} 

$$\tau = \mathrm{E}(Y(1)|D{=}1) - \mathrm{E}(Y(0)|D{=}1)$$

\noindent with $\tau$ average treatment effect on the treated, $Y(1)$ response if treated, $Y(0)$ analogous; $D_i$ indicator if $i$ is influenced by the treatment 

$$\mathrm{E}(Y(1)|D{=}1) - \mathrm{E}(Y(0)|D{=}0) = \tau + \underbrace{\mathrm{E}(Y(0)|D{=}1) - \mathrm{E}(Y(0)|D{=}0)}_{\text{selection bias}}$$

If the selection bias is zero, $D$ and $X$ are unconfounded.

$$\hat{\tau} = \sum_{i=1}^n (Y_i(1) -Y_{j(i)}(0))$$

\noindent after matching treated individual $i$ with individual $j(i)$ from non-treatment group




\end{multicols}


\end{document}
